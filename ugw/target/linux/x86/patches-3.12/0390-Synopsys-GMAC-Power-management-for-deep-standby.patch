From 0670596681a74d89427fc4274854e10dc1b9e84f Mon Sep 17 00:00:00 2001
From: Marco A Vital Yep <marco.a.vital.yep@intel.com>
Date: Fri, 29 Apr 2016 12:49:10 -0700
Subject: [PATCH 390/441] Synopsys GMAC: Power management for deep standby

The GMAC registers cannot be accessed when the NetIP is in deep standby,
otherwise, the system hangs because the NetIP clock is gated. To prevent
this problem a callback was registered with the NetSS driver to turn off
the GMAC device power and implemented logic to enforce that no register
is accessed when the NetIP is moved to deep standby.

Additionally, a kernel crash was seen when trying to configure some
driver parameters (like MTU) at runtime, this was fixed checking the
PHY pointer before attempt to reconfigure the port.
---
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_desc.c   |   44 +-
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_dev.c    |  163 +-
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_drv.c    | 8626 ++++++++++----------
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_eee.c    |    8 +-
 .../net/ethernet/synopsys/DWC_ETH_QOS_ethtool.c    |   12 +-
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_mdio.c   |   12 +-
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_pci.c    |  277 +-
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_pci.h    |   52 -
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_pktgen.c |   10 +-
 drivers/net/ethernet/synopsys/DWC_ETH_QOS_ptp.c    |    8 +-
 .../net/ethernet/synopsys/DWC_ETH_QOS_yheader.h    |  169 +-
 .../net/ethernet/synopsys/DWC_ETH_QOS_yregacc.h    |    2 +-
 12 files changed, 4715 insertions(+), 4668 deletions(-)
 delete mode 100644 drivers/net/ethernet/synopsys/DWC_ETH_QOS_pci.h

--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_desc.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_desc.c
@@ -296,7 +296,7 @@ static void DWC_ETH_QOS_wrapper_tx_descr
 	struct DWC_ETH_QOS_tx_buffer *buffer = GET_TX_BUF_PTR(qInx, 0);
 	tx_descriptor_t *desc = GET_TX_DESC_PTR(qInx, 0);
 	dma_addr_t desc_dma = GET_TX_DESC_DMA_ADDR(qInx, 0);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 
 	DBGPR("-->DWC_ETH_QOS_wrapper_tx_descriptor_init_single_q: "\
 		"qInx = %u\n", qInx);
@@ -354,7 +354,7 @@ static void DWC_ETH_QOS_wrapper_rx_descr
 	struct DWC_ETH_QOS_rx_buffer *buffer = GET_RX_BUF_PTR(qInx, 0);
 	rx_descriptor_t *desc = GET_RX_DESC_PTR(qInx, 0);
 	dma_addr_t desc_dma = GET_RX_DESC_DMA_ADDR(qInx, 0);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 
 	DBGPR("-->DWC_ETH_QOS_wrapper_rx_descriptor_init_single_q: "\
 		"qInx = %u\n", qInx);
@@ -481,18 +481,14 @@ static void DWC_ETH_QOS_rx_free_mem(stru
 static void DWC_ETH_QOS_tx_free_mem(struct DWC_ETH_QOS_prv_data *pdata)
 {
 	DBGPR("-->DWC_ETH_QOS_tx_free_mem\n");
-
 	/* free TX descriptor */
 	DWC_ETH_QOS_tx_desc_free_mem(pdata, DWC_ETH_QOS_TX_QUEUE_CNT);
-
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
 	/* free TX skb's */
 	DWC_ETH_QOS_tx_skb_free_mem(pdata, DWC_ETH_QOS_TX_QUEUE_CNT);
 #endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
-
 	/* free TX buffer */
 	DWC_ETH_QOS_tx_buf_free_mem(pdata, DWC_ETH_QOS_TX_QUEUE_CNT);
-
 	DBGPR("<--DWC_ETH_QOS_tx_free_mem\n");
 }
 
@@ -504,17 +500,17 @@ static void DWC_ETH_QOS_tx_free_mem(stru
  *
  * \return void
  */
-
 static void DWC_ETH_QOS_tx_skb_free_mem_single_q(struct DWC_ETH_QOS_prv_data *pdata,
 							uint32_t qInx)
 {
 	uint32_t i;
-
+	struct DWC_ETH_QOS_tx_buffer *buffer = NULL;
 	DBGPR("-->DWC_ETH_QOS_tx_skb_free_mem_single_q: qInx = %u\n", qInx);
-
-	for (i = 0; i < TX_DESC_CNT; i++)
-		DWC_ETH_QOS_unmap_tx_skb(pdata, GET_TX_BUF_PTR(qInx, i));
-
+	for (i = 0; i < TX_DESC_CNT; i++) {
+		buffer = GET_TX_BUF_PTR(qInx, i);
+		if (buffer)
+		DWC_ETH_QOS_unmap_tx_skb(pdata, buffer);
+	}
 	DBGPR("<--DWC_ETH_QOS_tx_skb_free_mem_single_q\n");
 }
 
@@ -527,21 +523,16 @@ static void DWC_ETH_QOS_tx_skb_free_mem_
 *
 * \retval void.
 */
-
 static void DWC_ETH_QOS_tx_skb_free_mem(struct DWC_ETH_QOS_prv_data *pdata,
 					uint32_t tx_qCnt)
 {
 	uint32_t qInx;
-
 	DBGPR("-->DWC_ETH_QOS_tx_skb_free_mem: tx_qCnt = %d\n", tx_qCnt);
-
 	for (qInx = 0; qInx < tx_qCnt; qInx++)
 		DWC_ETH_QOS_tx_skb_free_mem_single_q(pdata, qInx);
-
 	DBGPR("<--DWC_ETH_QOS_tx_skb_free_mem\n");
 }
 
-
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
 /*!
  * \details This function is used to release Rx socket buffer.
@@ -580,30 +571,28 @@ static void DWC_ETH_QOS_unmap_rx_skb_pg(
  *
  * \return void
  */
-
 static void DWC_ETH_QOS_rx_skb_free_mem_single_q(struct DWC_ETH_QOS_prv_data *pdata,
 							uint32_t qInx)
 {
 	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
 	    GET_RX_WRAPPER_DESC(qInx);
+	struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
 	uint32_t i;
-
 	DBGPR("-->DWC_ETH_QOS_rx_skb_free_mem_single_q: qInx = %u\n", qInx);
-
 	for (i = 0; i < RX_DESC_CNT; i++) {
+		buffer = GET_RX_BUF_PTR(qInx, i);
+		if (buffer) {
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-		DWC_ETH_QOS_unmap_rx_skb_pg(pdata, GET_RX_BUF_PTR(qInx, i));
+			DWC_ETH_QOS_unmap_rx_skb_pg(pdata, );
 #else
 		DWC_ETH_QOS_unmap_rx_skb(pdata, GET_RX_BUF_PTR(qInx, i));
 #endif
+		}
 	}
-
 	/* there are also some cached data from a chained rx */
 	if (desc_data->skb_top)
 		dev_kfree_skb_any(desc_data->skb_top);
-
 	desc_data->skb_top = NULL;
-
 	DBGPR("<--DWC_ETH_QOS_rx_skb_free_mem_single_q\n");
 }
 
@@ -616,17 +605,13 @@ static void DWC_ETH_QOS_rx_skb_free_mem_
 *
 * \retval void.
 */
-
 static void DWC_ETH_QOS_rx_skb_free_mem(struct DWC_ETH_QOS_prv_data *pdata,
 					uint32_t rx_qCnt)
 {
 	uint32_t qInx;
-
 	DBGPR("-->DWC_ETH_QOS_rx_skb_free_mem: rx_qCnt = %d\n", rx_qCnt);
-
 	for (qInx = 0; qInx < rx_qCnt; qInx++)
 		DWC_ETH_QOS_rx_skb_free_mem_single_q(pdata, qInx);
-
 	DBGPR("<--DWC_ETH_QOS_rx_skb_free_mem\n");
 }
 
@@ -639,7 +624,6 @@ static void DWC_ETH_QOS_rx_skb_free_mem(
 *
 * \retval void.
 */
-
 static void DWC_ETH_QOS_tx_buf_free_mem(struct DWC_ETH_QOS_prv_data *pdata,
 					uint32_t tx_qCnt)
 {
@@ -1186,7 +1170,7 @@ static void DWC_ETH_QOS_re_alloc_skb(str
 	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
 	    GET_RX_WRAPPER_DESC(qInx);
 	struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
+	hw_interface_t *hw_if = &pdata->hw_if;
 	int tail_idx;
 
 	DBGPR("-->DWC_ETH_QOS_re_alloc_skb: desc_data->skb_realloc_idx = %d "\
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_dev.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_dev.c
@@ -1367,71 +1367,64 @@ static int disable_mmc_interrupts(void)
 static int config_mmc_counters(void)
 {
    uint32_t varMMC_CNTRL;
-   /* set COUNTER RESET */
-   /* set RESET ON READ */
-   /* set COUNTER PRESET */
-   /* set FULL_HALF PRESET */
    varMMC_CNTRL = DWC_REG_RD(MMC_CR);
    varMMC_CNTRL &= 0x10a;
+   /* set COUNTER RESET */
    VAR32_SET_BIT(varMMC_CNTRL, MMC_CR_CNTRST, 0x1);
+   /* set RESET ON READ */
    VAR32_SET_BIT(varMMC_CNTRL, MMC_CR_RSTONRD, 0x1);
+   /* set COUNTER PRESET */
    VAR32_SET_BIT(varMMC_CNTRL, MMC_CR_CNTPRST, 0x1);
+   /* set FULL_HALF PRESET */
    VAR32_SET_BIT(varMMC_CNTRL, MMC_CR_CNTPRSTLVL, 0x1);
    DWC_REG_WR(MMC_CR, varMMC_CNTRL);
    return Y_SUCCESS;
 }
 
-/*!
-* \brief This sequence is used to disable given DMA channel rx interrupts
-* \param[in] qInx
-*/
-static void disable_rx_interrupt(uint32_t qInx)
+/* Disable given DMA channel rx interrupts */
+static void disable_rx_interrupt(uint32_t qInx, hw_config_t *hw_cfg)
 {
    /* Disable Rx interrupts */
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_RBUE, 0x0);
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_RIE, 0x0);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RBUE, 0x0);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RIE, 0x0);
+   DWC_REG_WR(DMA_IER(qInx), hw_cfg->dma_ier);
+
    /* Clear any Rx pending interrupt */
    DWC_REG_WR_BIT(DMA_SR(qInx), DMA_SR_RBU, 0x1);
    DWC_REG_WR_BIT(DMA_SR(qInx), DMA_SR_RI, 0x1);
 }
 
-/*!
-* \brief This sequence is used to enable given DMA channel rx interrupts
-* \param[in] qInx
-*/
-static void enable_rx_interrupt(uint32_t qInx)
+/* Enable given DMA channel rx interrupts */
+static void enable_rx_interrupt(uint32_t qInx, hw_config_t *hw_cfg)
 {
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_RBUE, 0x1);
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_RIE, 0x1);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RBUE, 0x1);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RIE, 0x1);
+   DWC_REG_WR(DMA_IER(qInx), hw_cfg->dma_ier);
 }
 
-/*!
-* \brief This sequence is used to disable given DMA channel tx interrupts
-* \param[in] qInx
-*/
-static void disable_tx_interrupt(uint32_t qInx)
+/*Disable given DMA channel tx interrupts */
+static void disable_tx_interrupt(uint32_t qInx, hw_config_t *hw_cfg)
 {
    /* Disable and clear Tx interrupts */
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_TIE, 0x0);
-   DWC_REG_WR_BIT(DMA_SR(qInx), DMA_SR_TI, 0x1);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TIE, 0x0);
 #else
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_TBUE, 0x0);
-   DWC_REG_WR_BIT(DMA_SR(qInx), DMA_SR_TBU, 0x1);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TBUE, 0x0)
 #endif
+   DWC_REG_WR(DMA_IER(qInx), hw_cfg->dma_ier);
+   DWC_REG_WR_BIT(DMA_SR(qInx), DMA_SR_TI, 0x1);
+   DWC_REG_WR_BIT(DMA_SR(qInx), DMA_SR_TBU, 0x1);
 }
 
-/*!
-* \brief This sequence is used to enable given DMA channel tx interrupts
-* \param[in] qInx
-*/
-static void enable_tx_interrupt(uint32_t qInx)
+/* Enable given DMA channel tx interrupts */
+static void enable_tx_interrupt(uint32_t qInx, hw_config_t *hw_cfg)
 {
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_TIE, 0x1);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TIE, 0x1);
 #else
-   DWC_REG_WR_BIT(DMA_IER(qInx), DMA_IER_TBUE, 0x1);
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TBUE, 0x1)
 #endif
+   DWC_REG_WR(DMA_IER(qInx), hw_cfg->dma_ier);
 }
 
 static void configure_sa_via_reg(uint32_t cmd)
@@ -1828,7 +1821,6 @@ static int stop_dma_rx(uint32_t qInx)
    return ret;
 }
 
-
 /*!
 * \param[in] qInx
 * \return Success or Failure
@@ -1940,43 +1932,43 @@ static int start_mac_tx_rx(void)
    return Y_SUCCESS;
 }
 
-
 /*!
 * \brief This sequence is used to enable DMA interrupts
 * \return Success or Failure
 * \retval  0 Success
 * \retval -1 Failure
 */
-static int enable_dma_interrupts(uint32_t qInx, uint32_t version)
+static int enable_dma_interrupts(uint32_t qInx, uint32_t version,
+                                 hw_config_t *hw_cfg)
 {
    uint32_t varDMA_SR;
-   uint32_t varDMA_IER;
    /* Clear any current set interrupt */
    varDMA_SR = DWC_REG_RD(DMA_SR(qInx));
    DWC_REG_WR(DMA_SR(qInx), varDMA_SR);
-   /* Read current interrupt enable status register */
-   varDMA_IER = DWC_REG_RD(DMA_IER(qInx));
-   /* Reset all interrupts except CDEE and RWTE */
-   varDMA_IER &= ((1 << DMA_IER_CDEE_OFF) | (1 << DMA_IER_RWTE_OFF));
    /* Enable required DMA interrupts */
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_TXSE, 0x1); /* Transmit Stoppped */
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_RIE, 0x1);  /* Receive Interrupt */
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_RBUE, 0x1); /* Receive Buffer Unavailable */
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_RSE, 0x1);  /* Receive Stoppped */
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_FBEE, 0x1); /* Fatal Bus Error */
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RIE, 0x1);  /* Receive Interrupt */
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RBUE, 0x1); /* Receive Buffer Unavailable */
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_FBEE, 0x1); /* Fatal Bus Error */
+
+#if 0
+   // THESE ARE NOT NEEDED
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TSE, 0x1);  /* Transmit Stoppped */
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_RSE, 0x1);  /* Receive Stoppped */
+#endif
+
    if (version == MAC_VER_4_00) {
-      VAR32_SET_BIT(varDMA_IER, DMA_IER_AIE_4_00, 0x1);  /* Abnormal Interrupt Summary */
-      VAR32_SET_BIT(varDMA_IER, DMA_IER_NIE_4_00, 0x1);  /* Normal Interrupt Summary */
+      VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_AIE_4_00, 0x1);  /* Abnormal Interrupt Summary */
+      VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_NIE_4_00, 0x1);  /* Normal Interrupt Summary */
    } else {
-      VAR32_SET_BIT(varDMA_IER, DMA_IER_AIE_4_10, 0x1);  /* Abnormal Interrupt Summary */
-      VAR32_SET_BIT(varDMA_IER, DMA_IER_NIE_4_10, 0x1);  /* Normal Interrupt Summary */
+      VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_AIE_4_10, 0x1);  /* Abnormal Interrupt Summary */
+      VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_NIE_4_10, 0x1);  /* Normal Interrupt Summary */
    }
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_TIE, 0x1); /* Transmit Interrupt */
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TIE, 0x1); /* Transmit Interrupt */
 #else
-   VAR32_SET_BIT(varDMA_IER, DMA_IER_TBUE, 0x1); /* Transmit Buffer Unavailable */
+   VAR32_SET_BIT(hw_cfg->dma_ier, DMA_IER_TBUE, 0x1); /* Transmit Buffer Unavailable */
 #endif
-   DWC_REG_WR(DMA_IER(qInx), varDMA_IER);
+   DWC_REG_WR(DMA_IER(qInx), hw_cfg->dma_ier);
 
    return Y_SUCCESS;
 }
@@ -2883,30 +2875,22 @@ static int get_tx_descriptor_last(tx_des
    return VAR32_GET_BIT(txdesc->TDES3, NORMAL_WB_TDES3_LD);
 }
 
-/*!
-* \brief Exit routine
-* \details Exit function that unregisters the device, deallocates buffers,
-* unbinds the driver from controlling the device etc.
-*
-* \return Returns successful execution of the routine
-* \retval Y_SUCCESS Function executed successfully
-*/
-static int DWC_ETH_QOS_yexit(void)
+/* Sotware reset */
+static int DWC_ETH_QOS_sw_reset(void)
 {
    uint32_t retryCount = 1000;
-   DBGPR("-->DWC_ETH_QOS_yexit\n");
+   DBGPR("-->DWC_ETH_QOS_sw_reset\n");
    /* Issue a software reset */
    DWC_REG_WR_BIT(DMA_BMR, DMA_BMR_SWR, 0x1);
    udelay(10);
    /* Wait for software reset */
    while(DWC_REG_RD_BIT(DMA_BMR, DMA_BMR_SWR) && --retryCount)
       mdelay(1);
-   DBGPR("<--DWC_ETH_QOS_yexit\n");
+   DBGPR("<--DWC_ETH_QOS_sw_reset\n");
 
    return Y_SUCCESS;
 }
 
-
 /*!
 * \details This API will calculate per queue FIFO size.
 *
@@ -3099,7 +3083,6 @@ static int configure_mtl_queue(uint32_t
    return Y_SUCCESS;
 }
 
-
 static int configure_dma_channel(uint32_t qInx,
          struct DWC_ETH_QOS_prv_data *pdata)
 {
@@ -3131,7 +3114,7 @@ static int configure_dma_channel(uint32_
    CFG_PRINT("%s Rx watchdog timer\n",
       (rx_desc_data->use_riwt ? "Enabled" : "Disabled"));
 
-   enable_dma_interrupts(qInx, pdata->version);
+   enable_dma_interrupts(qInx, pdata->version, &pdata->hw_cfg);
    /* set PBLx8 */
    DWC_REG_WR_BIT(DMA_CR(qInx), DMA_CR_PBLx8, 0x1);
    /* set TX PBL = 256 */
@@ -3176,23 +3159,20 @@ static int configure_dma_channel(uint32_
 * \retval  0 Success
 * \retval -1 Failure
 */
-static int enable_mac_interrupts(void)
+static int enable_mac_interrupts(hw_config_t *hw_cfg)
 {
-   uint32_t varmac_ier = DWC_REG_RD(MAC_IER);
-   /* Enable following interrupts */
-   /* RGSMIIIM - RGMII/SMII interrupt Enable */
-   /* PCSLCHGIM -  PCS Link Status Interrupt Enable */
-   /* PCSANCIM - PCS AN Completion Interrupt Enable */
-   /* PMTIM - PMT Interrupt Enable */
-   /* LPIIM - LPI Interrupt Enable */
-   varmac_ier &= 0x1008;
-   VAR32_SET_BIT(varmac_ier, MAC_IER_RGMIIIE, 0x1);
-   VAR32_SET_BIT(varmac_ier, MAC_IER_PCSLCHGIE, 0x1);
-   VAR32_SET_BIT(varmac_ier, MAC_IER_PCSANCIE, 0x1);
-   VAR32_SET_BIT(varmac_ier, MAC_IER_PMTIE, 0x1);
-   VAR32_SET_BIT(varmac_ier, MAC_IER_LPIIE, 0x1);
-   DWC_REG_WR(MAC_IER, varmac_ier);
-   CFG_PRINT("[%s] MAC_IER = 0x%08x\n", __FUNCTION__, varmac_ier);
+   /* RGMII/SMII interrupt */
+   VAR32_SET_BIT(hw_cfg->mac_ier, MAC_IER_RGMIIIE, 0x1);
+   /* PCS Link Status Interrupt */
+   VAR32_SET_BIT(hw_cfg->mac_ier, MAC_IER_PCSLCHGIE, 0x1);
+   /* PCS AN Completion Interrupt */
+   VAR32_SET_BIT(hw_cfg->mac_ier, MAC_IER_PCSANCIE, 0x1);
+
+   /* LPIIM - LPI Interrupt Enable - THIS IS ONLY REQUIRED FOR EEE */
+   VAR32_SET_BIT(hw_cfg->mac_ier, MAC_IER_LPIIE, 0x1);
+
+   DWC_REG_WR(MAC_IER, hw_cfg->mac_ier);
+   CFG_PRINT("[%s] MAC_IER = 0x%08x\n", __FUNCTION__, hw_cfg->mac_ier);
    return Y_SUCCESS;
 }
 
@@ -3290,7 +3270,7 @@ static int configure_mac(struct DWC_ETH_
       config_mmc_counters();
    }
 
-   enable_mac_interrupts();
+   enable_mac_interrupts(&pdata->hw_cfg);
 
    DBGPR("<--configure_mac\n");
 
@@ -3439,7 +3419,6 @@ static void rx_descriptor_init_pg(struct
 
 #endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
 
-
 /*!
 * \brief API to initialize the function pointers.
 *
@@ -3452,7 +3431,7 @@ static void rx_descriptor_init_pg(struct
 * \return void.
 */
 
-void DWC_ETH_QOS_init_function_ptrs_dev(struct hw_if_struct *hw_if)
+void DWC_ETH_QOS_init_function_ptrs_dev(hw_interface_t *hw_if)
 {
 
    DBGPR("-->DWC_ETH_QOS_init_function_ptrs_dev\n");
@@ -3492,7 +3471,7 @@ void DWC_ETH_QOS_init_function_ptrs_dev(
    hw_if->pre_xmit = pre_transmit;
    hw_if->dev_read = device_read;
    hw_if->init = DWC_ETH_QOS_yinit;
-   hw_if->exit = DWC_ETH_QOS_yexit;
+   hw_if->sw_reset = DWC_ETH_QOS_sw_reset;
    /* Descriptor related Sequences have to be initialized here */
    hw_if->tx_desc_init = tx_descriptor_init;
    hw_if->rx_desc_init = rx_descriptor_init;
@@ -3515,11 +3494,9 @@ void DWC_ETH_QOS_init_function_ptrs_dev(
    hw_if->disable_remote_pmt = disable_remote_pmt_operation;
    hw_if->configure_rwk_filter = configure_rwk_filter_registers;
 
-    /* for TX vlan control */
-    hw_if->enable_vlan_reg_control = configure_reg_vlan_control;
-    hw_if->enable_vlan_desc_control = configure_desc_vlan_control;
-
-
+   /* for TX vlan control */
+   hw_if->enable_vlan_reg_control = configure_reg_vlan_control;
+   hw_if->enable_vlan_desc_control = configure_desc_vlan_control;
 
    /* for rx vlan stripping */
    hw_if->config_rx_outer_vlan_stripping =
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_drv.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_drv.c
@@ -39,6 +39,10 @@
 #include "DWC_ETH_QOS_drv.h"
 #include "DWC_ETH_QOS_yregacc.h"
 
+/* Priority constants for NAPI scheduling */
+#define HI_PRIORITY_INT  0x1
+#define LO_PRIORITY_INT  0x2
+
 /* SA(Source Address) operations on TX */
 unsigned char mac_addr0[6] = { 0x00, 0x11, 0x22, 0x33, 0x44, 0x55 };
 unsigned char mac_addr1[6] = { 0x00, 0x66, 0x77, 0x88, 0x99, 0xaa };
@@ -47,18 +51,18 @@ unsigned char mac_addr1[6] = { 0x00, 0x6
  * set default mode as GENERIC
  * */
 static int q_op_mode[DWC_ETH_QOS_MAX_TX_QUEUE_CNT] = {
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC,
-	DWC_ETH_QOS_Q_GENERIC
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC,
+   DWC_ETH_QOS_Q_GENERIC
 };
 module_param_array(q_op_mode, int, NULL, S_IRUGO | S_IWUSR);
 MODULE_PARM_DESC(q_op_mode,
-	"MTL queue operation mode [0-DISABLED, 1-AVB, 2-DCB, 3-GENERIC]");
+   "MTL queue operation mode [0-DISABLED, 1-AVB, 2-DCB, 3-GENERIC]");
 
 #ifdef GBE_DEBUG
 
@@ -89,7 +93,7 @@ static inline void print_skb(struct sk_b
    printk(KERN_ALERT "TYPE[%04x] LENGTH[%d]\n",
           (buffer[12] << 8 | buffer[13]) & 0xFFFF, length);
    printk("------------------------------------------------------\n");
-   for (i=0; i < length && i < 1600; i++)	{
+   for (i=0; i < length && i < 1600; i++)   {
       if ((i % 16) == 0) {
          printk("%06x ", i);
       }
@@ -128,183 +132,196 @@ static inline int replace_crc(struct sk_
 
 void DWC_ETH_QOS_stop_all_ch_tx_dma(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	DBGPR("-->DWC_ETH_QOS_stop_all_ch_tx_dma\n");
-	for(qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++)
-		hw_if->stop_dma_tx(qInx);
-	DBGPR("<--DWC_ETH_QOS_stop_all_ch_tx_dma\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t qInx;
+   DBGPR("-->DWC_ETH_QOS_stop_all_ch_tx_dma\n");
+   for(qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++)
+      hw_if->stop_dma_tx(qInx);
+   DBGPR("<--DWC_ETH_QOS_stop_all_ch_tx_dma\n");
 }
 
 static void DWC_ETH_QOS_stop_all_ch_rx_dma(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	DBGPR("-->DWC_ETH_QOS_stop_all_ch_rx_dma\n");
-	for(qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
-		hw_if->stop_dma_rx(qInx);
-	DBGPR("<--DWC_ETH_QOS_stop_all_ch_rx_dma\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t qInx;
+   DBGPR("-->DWC_ETH_QOS_stop_all_ch_rx_dma\n");
+   for(qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
+      hw_if->stop_dma_rx(qInx);
+   DBGPR("<--DWC_ETH_QOS_stop_all_ch_rx_dma\n");
 }
 
 static void DWC_ETH_QOS_start_all_ch_tx_dma(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t i;
-	DBGPR("-->DWC_ETH_QOS_start_all_ch_tx_dma\n");
-	for(i = 0; i < DWC_ETH_QOS_TX_QUEUE_CNT; i++)
-		hw_if->start_dma_tx(i);
-	DBGPR("<--DWC_ETH_QOS_start_all_ch_tx_dma\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t i;
+   DBGPR("-->DWC_ETH_QOS_start_all_ch_tx_dma\n");
+   for(i = 0; i < DWC_ETH_QOS_TX_QUEUE_CNT; i++)
+      hw_if->start_dma_tx(i);
+   DBGPR("<--DWC_ETH_QOS_start_all_ch_tx_dma\n");
 }
 
 static void DWC_ETH_QOS_start_all_ch_rx_dma(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t i;
-	DBGPR("-->DWC_ETH_QOS_start_all_ch_rx_dma\n");
-	for(i = 0; i < DWC_ETH_QOS_RX_QUEUE_CNT; i++)
-		hw_if->start_dma_rx(i);
-	DBGPR("<--DWC_ETH_QOS_start_all_ch_rx_dma\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t i;
+   DBGPR("-->DWC_ETH_QOS_start_all_ch_rx_dma\n");
+   for(i = 0; i < DWC_ETH_QOS_RX_QUEUE_CNT; i++)
+      hw_if->start_dma_rx(i);
+   DBGPR("<--DWC_ETH_QOS_start_all_ch_rx_dma\n");
 }
 
 static void DWC_ETH_QOS_napi_enable(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	DBGPR("-->DWC_ETH_QOS_napi_enable\n");
-	napi_enable(&pdata->rx_napi);
-	DBGPR("<--DWC_ETH_QOS_napi_enable\n");
+   DBGPR("-->DWC_ETH_QOS_napi_enable\n");
+   napi_enable(&pdata->rx_napi);
+   DBGPR("<--DWC_ETH_QOS_napi_enable\n");
 }
 
 static void DWC_ETH_QOS_napi_disable(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	DBGPR("-->DWC_ETH_QOS_napi_disable\n");
-	napi_disable(&pdata->rx_napi);
-	DBGPR("<--DWC_ETH_QOS_napi_disable\n");
-}
-
-/*!
- * \details This function is invoked to stop device operation
- * Following operations are performed in this function.
- * - Stop the queue.
- * - Stops DMA TX and RX.
- * - Free the TX and RX skb's.
- * - Issues soft reset to device.
- *
- * \param[in] pdata – pointer to private data structure.
- *
- * \return void
- */
-
-static void DWC_ETH_QOS_stop_dev(struct DWC_ETH_QOS_prv_data *pdata)
-{
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct desc_if_struct *desc_if = &(pdata->desc_if);
-
-	DBGPR("-->DWC_ETH_QOS_stop_dev\n");
-
-	netif_tx_disable(pdata->dev);
-
-	DWC_ETH_QOS_napi_disable(pdata);
-
-	/* stop DMA TX/RX */
-	DWC_ETH_QOS_stop_all_ch_tx_dma(pdata);
-	DWC_ETH_QOS_stop_all_ch_rx_dma(pdata);
-
-	/* issue software reset to device */
-	hw_if->exit();
-
-	/* free tx skb's */
-	desc_if->tx_skb_free_mem(pdata, DWC_ETH_QOS_TX_QUEUE_CNT);
-	/* free rx skb's */
-	desc_if->rx_skb_free_mem(pdata, DWC_ETH_QOS_RX_QUEUE_CNT);
-
-	DBGPR("<--DWC_ETH_QOS_stop_dev\n");
+   DBGPR("-->DWC_ETH_QOS_napi_disable\n");
+   napi_disable(&pdata->rx_napi);
+   DBGPR("<--DWC_ETH_QOS_napi_disable\n");
 }
 
 #ifdef YDEBUG
+
 static void DWC_ETH_QOS_tx_desc_mang_ds_dump(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data = NULL;
-	tx_descriptor_t *tx_desc = NULL;
-	int qInx, i;
-
-	printk(KERN_ALERT "/**** TX DESC MANAGEMENT DATA STRUCTURE DUMP ****/\n");
-
-	printk(KERN_ALERT "TX_DESC_QUEUE_CNT = %d\n", DWC_ETH_QOS_TX_QUEUE_CNT);
-	for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
-		tx_desc_data = GET_TX_WRAPPER_DESC(qInx);
-		printk(KERN_ALERT "DMA CHANNEL = %d\n", qInx);
-		printk(KERN_ALERT "\tcur_tx           = %d\n", tx_desc_data->cur_tx);
-		printk(KERN_ALERT "\tdirty_tx         = %d\n", tx_desc_data->dirty_tx);
-		printk(KERN_ALERT "\tfree_desc_cnt    = %d\n", tx_desc_data->free_desc_cnt);
-		printk(KERN_ALERT "\ttx_pkt_queued    = %d\n", tx_desc_data->tx_pkt_queued);
-		printk(KERN_ALERT "\tqueue_stopped    = %d\n", tx_desc_data->queue_stopped);
-		printk(KERN_ALERT "\tpacket_count     = %d\n", tx_desc_data->packet_count);
-		printk(KERN_ALERT "\ttx_threshold_val = %d\n", tx_desc_data->tx_threshold_val);
-		printk(KERN_ALERT "\ttsf_on           = %d\n", tx_desc_data->tsf_on);
-		printk(KERN_ALERT "\tosf_on           = %d\n", tx_desc_data->osf_on);
-		printk(KERN_ALERT "\ttx_pbl           = %d\n", tx_desc_data->tx_pbl);
-
-		printk(KERN_ALERT "\t[<desc_add> <index >] = <TDES0> : <TDES1> : <TDES2> : <TDES3>\n");
-		for (i = 0; i < TX_DESC_CNT; i++) {
-			tx_desc = GET_TX_DESC_PTR(qInx, i);
-			printk(KERN_ALERT "\t[%4p %03d] = %#x : %#x : %#x : %#x\n",
-				tx_desc, i, tx_desc->TDES0, tx_desc->TDES1,
-				tx_desc->TDES2, tx_desc->TDES3);
-		}
-	}
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data = NULL;
+   tx_descriptor_t *tx_desc = NULL;
+   int qInx, i;
+
+   printk(KERN_ALERT "/**** TX DESC MANAGEMENT DATA STRUCTURE DUMP ****/\n");
+
+   printk(KERN_ALERT "TX_DESC_QUEUE_CNT = %d\n", DWC_ETH_QOS_TX_QUEUE_CNT);
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
+      tx_desc_data = GET_TX_WRAPPER_DESC(qInx);
+      printk(KERN_ALERT "DMA CHANNEL = %d\n", qInx);
+      printk(KERN_ALERT "\tcur_tx           = %d\n", tx_desc_data->cur_tx);
+      printk(KERN_ALERT "\tdirty_tx         = %d\n", tx_desc_data->dirty_tx);
+      printk(KERN_ALERT "\tfree_desc_cnt    = %d\n", tx_desc_data->free_desc_cnt);
+      printk(KERN_ALERT "\ttx_pkt_queued    = %d\n", tx_desc_data->tx_pkt_queued);
+      printk(KERN_ALERT "\tqueue_stopped    = %d\n", tx_desc_data->queue_stopped);
+      printk(KERN_ALERT "\tpacket_count     = %d\n", tx_desc_data->packet_count);
+      printk(KERN_ALERT "\ttx_threshold_val = %d\n", tx_desc_data->tx_threshold_val);
+      printk(KERN_ALERT "\ttsf_on           = %d\n", tx_desc_data->tsf_on);
+      printk(KERN_ALERT "\tosf_on           = %d\n", tx_desc_data->osf_on);
+      printk(KERN_ALERT "\ttx_pbl           = %d\n", tx_desc_data->tx_pbl);
+
+      printk(KERN_ALERT "\t[<desc_add> <index >] = <TDES0> : <TDES1> : <TDES2> : <TDES3>\n");
+      for (i = 0; i < TX_DESC_CNT; i++) {
+         tx_desc = GET_TX_DESC_PTR(qInx, i);
+         printk(KERN_ALERT "\t[%4p %03d] = %#x : %#x : %#x : %#x\n",
+            tx_desc, i, tx_desc->TDES0, tx_desc->TDES1,
+            tx_desc->TDES2, tx_desc->TDES3);
+      }
+   }
 
-	printk(KERN_ALERT "/************************************************/\n");
+   printk(KERN_ALERT "/************************************************/\n");
 }
 
-
 static void DWC_ETH_QOS_rx_desc_mang_ds_dump(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data = NULL;
-	rx_descriptor_t *rx_desc = NULL;
-	int qInx, i;
-
-	printk(KERN_ALERT "/**** RX DESC MANAGEMENT DATA STRUCTURE DUMP ****/\n");
-	printk(KERN_ALERT "RX_DESC_QUEUE_CNT = %d\n", DWC_ETH_QOS_RX_QUEUE_CNT);
-
-	for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
-		rx_desc_data = GET_RX_WRAPPER_DESC(qInx);
-		printk(KERN_ALERT "DMA CHANNEL = %d\n", qInx);
-		printk(KERN_ALERT "\tcur_rx                = %d\n", rx_desc_data->cur_rx);
-		printk(KERN_ALERT "\tdirty_rx              = %d\n", rx_desc_data->dirty_rx);
-		printk(KERN_ALERT "\tpkt_received          = %d\n", rx_desc_data->pkt_received);
-		printk(KERN_ALERT "\tskb_realloc_idx       = %d\n", rx_desc_data->skb_realloc_idx);
-		printk(KERN_ALERT "\tskb_realloc_threshold = %d\n", rx_desc_data->skb_realloc_threshold);
-		printk(KERN_ALERT "\tuse_riwt              = %d\n", rx_desc_data->use_riwt);
-		printk(KERN_ALERT "\trx_riwt               = %d\n", rx_desc_data->rx_riwt);
-		printk(KERN_ALERT "\trx_coal_frames        = %d\n", rx_desc_data->rx_coal_frames);
-		printk(KERN_ALERT "\trx_threshold_val      = %d\n", rx_desc_data->rx_threshold_val);
-		printk(KERN_ALERT "\trsf_on                = %d\n", rx_desc_data->rsf_on);
-		printk(KERN_ALERT "\trx_pbl                = %d\n", rx_desc_data->rx_pbl);
-
-		printk(KERN_ALERT "\t[<desc_add> <index >] = <RDES0> : <RDES1> : <RDES2> : <RDES3>\n");
-		for (i = 0; i < RX_DESC_CNT; i++) {
-			rx_desc = GET_RX_DESC_PTR(qInx, i);
-			printk(KERN_ALERT "\t[%4p %03d] = %#x : %#x : %#x : %#x\n",
-				rx_desc, i, rx_desc->RDES0, rx_desc->RDES1,
-				rx_desc->RDES2, rx_desc->RDES3);
-		}
-	}
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data = NULL;
+   rx_descriptor_t *rx_desc = NULL;
+   int qInx, i;
+
+   printk(KERN_ALERT "/**** RX DESC MANAGEMENT DATA STRUCTURE DUMP ****/\n");
+   printk(KERN_ALERT "RX_DESC_QUEUE_CNT = %d\n", DWC_ETH_QOS_RX_QUEUE_CNT);
+
+   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
+      rx_desc_data = GET_RX_WRAPPER_DESC(qInx);
+      printk(KERN_ALERT "DMA CHANNEL = %d\n", qInx);
+      printk(KERN_ALERT "\tcur_rx                = %d\n", rx_desc_data->cur_rx);
+      printk(KERN_ALERT "\tdirty_rx              = %d\n", rx_desc_data->dirty_rx);
+      printk(KERN_ALERT "\tpkt_received          = %d\n", rx_desc_data->pkt_received);
+      printk(KERN_ALERT "\tskb_realloc_idx       = %d\n", rx_desc_data->skb_realloc_idx);
+      printk(KERN_ALERT "\tskb_realloc_threshold = %d\n", rx_desc_data->skb_realloc_threshold);
+      printk(KERN_ALERT "\tuse_riwt              = %d\n", rx_desc_data->use_riwt);
+      printk(KERN_ALERT "\trx_riwt               = %d\n", rx_desc_data->rx_riwt);
+      printk(KERN_ALERT "\trx_coal_frames        = %d\n", rx_desc_data->rx_coal_frames);
+      printk(KERN_ALERT "\trx_threshold_val      = %d\n", rx_desc_data->rx_threshold_val);
+      printk(KERN_ALERT "\trsf_on                = %d\n", rx_desc_data->rsf_on);
+      printk(KERN_ALERT "\trx_pbl                = %d\n", rx_desc_data->rx_pbl);
+
+      printk(KERN_ALERT "\t[<desc_add> <index >] = <RDES0> : <RDES1> : <RDES2> : <RDES3>\n");
+      for (i = 0; i < RX_DESC_CNT; i++) {
+         rx_desc = GET_RX_DESC_PTR(qInx, i);
+         printk(KERN_ALERT "\t[%4p %03d] = %#x : %#x : %#x : %#x\n",
+            rx_desc, i, rx_desc->RDES0, rx_desc->RDES1,
+            rx_desc->RDES2, rx_desc->RDES3);
+      }
+   }
 
-	printk(KERN_ALERT "/************************************************/\n");
+   printk(KERN_ALERT "/************************************************/\n");
 }
+
 #endif
 
 static void DWC_ETH_QOS_restart_phy(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	DBGPR("-->DWC_ETH_QOS_restart_phy\n");
+   DBGPR("-->DWC_ETH_QOS_restart_phy\n");
 
-	pdata->oldlink = 0;
-	pdata->speed = 0;
-	pdata->oldduplex = -1;
+   pdata->oldlink = 0;
+   pdata->speed = 0;
+   pdata->oldduplex = -1;
 
-	if (pdata->phydev)
-		phy_start_aneg(pdata->phydev);
+   if (pdata->phydev)
+      phy_start_aneg(pdata->phydev);
 
-	DBGPR("<--DWC_ETH_QOS_restart_phy\n");
+   DBGPR("<--DWC_ETH_QOS_restart_phy\n");
+}
+
+/*!
+ * \details This function is invoked to stop device operation
+ * Following operations are performed in this function.
+ * - Stop the queue.
+ * - Stops DMA TX and RX.
+ * - Free the TX and RX skb's.
+ * - Issues soft reset to device.
+ *
+ * \param[in] pdata – pointer to private data structure.
+ *
+ * \return controller power status (gbe_power_state_t)
+ */
+static gbe_power_state_t DWC_ETH_QOS_stop_dev(
+   struct DWC_ETH_QOS_prv_data *pdata)
+{
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct desc_if_struct *desc_if = &(pdata->desc_if);
+   gbe_power_state_t ret = GBE_RUN_STATE;
+
+   CFG_PRINT("-->DWC_ETH_QOS_stop_dev\n");
+
+   if (pdata->power_state & DWC_ETH_QOS_NETIP_WAKEUP) {
+      WRN_PRINT("Device is in StandBy\n");
+      ret = GBE_STANDBY_STATE;
+      goto stop_dev_exit;
+   }
+
+   if (pdata->power_state & DWC_ETH_QOS_NETIP_PWRUP) {
+      CFG_PRINT("Device waking up from StandBy\n");
+   } else if (netif_running(pdata->dev)) {
+      CFG_PRINT("Device is not running\n");
+      netif_tx_disable(pdata->dev);
+      DWC_ETH_QOS_napi_disable(pdata);
+      /* Stop DMA TX/RX */
+      DWC_ETH_QOS_stop_all_ch_tx_dma(pdata);
+      DWC_ETH_QOS_stop_all_ch_rx_dma(pdata);
+   } else {
+      ret = GBE_STOP_STATE;
+   }
+   /* Issue software reset to device */
+   hw_if->sw_reset();
+   /* Free tx skb's */
+   desc_if->tx_skb_free_mem(pdata, DWC_ETH_QOS_TX_QUEUE_CNT);
+   /* Free rx skb's */
+   desc_if->rx_skb_free_mem(pdata, DWC_ETH_QOS_RX_QUEUE_CNT);
+
+stop_dev_exit:
+   CFG_PRINT("<--DWC_ETH_QOS_stop_dev\n");
+   return ret;
 }
 
 /*!
@@ -321,44 +338,51 @@ static void DWC_ETH_QOS_restart_phy(stru
  */
 static void DWC_ETH_QOS_start_dev(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct desc_if_struct *desc_if = &(pdata->desc_if);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct desc_if_struct *desc_if = &(pdata->desc_if);
 
-	DBGPR("-->DWC_ETH_QOS_start_dev\n");
+   CFG_PRINT("-->DWC_ETH_QOS_start_dev\n");
 
-	/* reset all variables */
-	DWC_ETH_QOS_default_common_confs(pdata);
-	DWC_ETH_QOS_default_tx_confs(pdata);
-	DWC_ETH_QOS_default_rx_confs(pdata);
+   /* reset all variables */
+   DWC_ETH_QOS_default_common_confs(pdata);
+   DWC_ETH_QOS_default_tx_confs(pdata);
+   DWC_ETH_QOS_default_rx_confs(pdata);
 
-	DWC_ETH_QOS_configure_rx_fun_ptr(pdata);
+   DWC_ETH_QOS_configure_rx_fun_ptr(pdata);
 
-	DWC_ETH_QOS_napi_enable(pdata);
+   DWC_ETH_QOS_napi_enable(pdata);
 
-	/* reinit descriptor */
-	desc_if->wrapper_tx_desc_init(pdata);
-	desc_if->wrapper_rx_desc_init(pdata);
+   /* reinit descriptor */
+   desc_if->wrapper_tx_desc_init(pdata);
+   desc_if->wrapper_rx_desc_init(pdata);
 
 #ifdef YDEBUG
-	DWC_ETH_QOS_tx_desc_mang_ds_dump(pdata);
-	DWC_ETH_QOS_rx_desc_mang_ds_dump(pdata);
+   DWC_ETH_QOS_tx_desc_mang_ds_dump(pdata);
+   DWC_ETH_QOS_rx_desc_mang_ds_dump(pdata);
 #endif
 
-	/* initializes MAC and DMA */
-	hw_if->init(pdata);
-
-	if (pdata->vlan_hash_filtering)
-		hw_if->update_vlan_hash_table_reg(pdata->vlan_ht_or_id);
-	else
-		hw_if->update_vlan_id(pdata->vlan_ht_or_id);
+   /* initializes MAC and DMA */
+   hw_if->init(pdata);
 
-	DWC_ETH_QOS_restart_phy(pdata);
-
-	pdata->eee_enabled = DWC_ETH_QOS_eee_init(pdata);
+   if (pdata->vlan_hash_filtering)
+      hw_if->update_vlan_hash_table_reg(pdata->vlan_ht_or_id);
+   else
+      hw_if->update_vlan_id(pdata->vlan_ht_or_id);
+
+   if (pdata->phydev) {
+      DWC_ETH_QOS_restart_phy(pdata);
+      pdata->eee_enabled = DWC_ETH_QOS_eee_init(pdata);
+   } else
+      pdata->eee_enabled = false;
+
+   if (pdata->mux_cfg == GMCR_GMAC5_TO_GMAC4) {
+      hw_if->set_full_duplex();
+      hw_if->set_speed(pdata, gbe_config_to_speed(pdata->rate));
+   }
 
-	netif_tx_wake_all_queues(pdata->dev);
+   netif_tx_wake_all_queues(pdata->dev);
 
-	DBGPR("<--DWC_ETH_QOS_start_dev\n");
+   CFG_PRINT("<--DWC_ETH_QOS_start_dev\n");
 }
 
 /*!
@@ -372,98 +396,96 @@ static void DWC_ETH_QOS_start_dev(struct
  *
  * \return void
  */
-
 static void DWC_ETH_QOS_restart_dev(struct DWC_ETH_QOS_prv_data *pdata,
-					uint32_t qInx)
+               uint32_t qInx)
 {
-	struct desc_if_struct *desc_if = &(pdata->desc_if);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+   struct desc_if_struct *desc_if = &(pdata->desc_if);
+   hw_interface_t *hw_if = &(pdata->hw_if);
 
-	DBGPR("-->DWC_ETH_QOS_restart_dev\n");
+   DBGPR("-->DWC_ETH_QOS_restart_dev\n");
 
-	netif_stop_subqueue(pdata->dev, qInx);
-	DWC_ETH_QOS_napi_disable(pdata);
+   netif_stop_subqueue(pdata->dev, qInx);
+   DWC_ETH_QOS_napi_disable(pdata);
 
-	/* stop DMA TX/RX */
-	hw_if->stop_dma_tx(qInx);
-	hw_if->stop_dma_rx(qInx);
+   /* stop DMA TX/RX */
+   hw_if->stop_dma_tx(qInx);
+   hw_if->stop_dma_rx(qInx);
 
-	/* free tx skb's */
-	desc_if->tx_skb_free_mem_single_q(pdata, qInx);
-	/* free rx skb's */
-	desc_if->rx_skb_free_mem_single_q(pdata, qInx);
+   /* free tx skb's */
+   desc_if->tx_skb_free_mem_single_q(pdata, qInx);
+   /* free rx skb's */
+   desc_if->rx_skb_free_mem_single_q(pdata, qInx);
 
-	if ((DWC_ETH_QOS_TX_QUEUE_CNT == 0) &&
-		(DWC_ETH_QOS_RX_QUEUE_CNT == 0)) {
-		/* issue software reset to device */
-		hw_if->exit();
+   if ((DWC_ETH_QOS_TX_QUEUE_CNT == 0) &&
+      (DWC_ETH_QOS_RX_QUEUE_CNT == 0)) {
+      /* issue software reset to device */
+      hw_if->sw_reset();
 
-		DWC_ETH_QOS_configure_rx_fun_ptr(pdata);
-		DWC_ETH_QOS_default_common_confs(pdata);
-	}
-	/* reset all variables */
-	DWC_ETH_QOS_default_tx_confs_single_q(pdata, qInx);
-	DWC_ETH_QOS_default_rx_confs_single_q(pdata, qInx);
+      DWC_ETH_QOS_configure_rx_fun_ptr(pdata);
+      DWC_ETH_QOS_default_common_confs(pdata);
+   }
+   /* reset all variables */
+   DWC_ETH_QOS_default_tx_confs_single_q(pdata, qInx);
+   DWC_ETH_QOS_default_rx_confs_single_q(pdata, qInx);
 
-	/* reinit descriptor */
-	desc_if->wrapper_tx_desc_init_single_q(pdata, qInx);
-	desc_if->wrapper_rx_desc_init_single_q(pdata, qInx);
+   /* reinit descriptor */
+   desc_if->wrapper_tx_desc_init_single_q(pdata, qInx);
+   desc_if->wrapper_rx_desc_init_single_q(pdata, qInx);
 
-	DWC_ETH_QOS_napi_enable(pdata);
+   DWC_ETH_QOS_napi_enable(pdata);
 
-	/* initializes MAC and DMA
-	 * NOTE : Do we need to init only one channel
-	 * which generate FBE*/
-	hw_if->init(pdata);
+   /* initializes MAC and DMA
+    * NOTE : Do we need to init only one channel
+    * which generate FBE*/
+   hw_if->init(pdata);
 
-	DWC_ETH_QOS_restart_phy(pdata);
+   DWC_ETH_QOS_restart_phy(pdata);
 
-	netif_wake_subqueue(pdata->dev, qInx);
+   netif_wake_subqueue(pdata->dev, qInx);
 
-	DBGPR("<--DWC_ETH_QOS_restart_dev\n");
+   DBGPR("<--DWC_ETH_QOS_restart_dev\n");
 }
 
 void DWC_ETH_QOS_disable_rx_interrupts(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	DBGPR("-->DWC_ETH_QOS_disable_rx_interrupts\n");
-	for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
-		hw_if->disable_rx_interrupt(qInx);
-	DBGPR("<--DWC_ETH_QOS_disable_rx_interrupts\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t qInx;
+   DBGPR("-->DWC_ETH_QOS_disable_rx_interrupts\n");
+   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
+      hw_if->disable_rx_interrupt(qInx, &pdata->hw_cfg);
+   DBGPR("<--DWC_ETH_QOS_disable_rx_interrupts\n");
 }
 
 void DWC_ETH_QOS_enable_rx_interrupts(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	DBGPR("-->DWC_ETH_QOS_enable_rx_interrupts\n");
-	for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
-		hw_if->enable_rx_interrupt(qInx);
-	DBGPR("<--DWC_ETH_QOS_enable_rx_interrupts\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t qInx;
+   DBGPR("-->DWC_ETH_QOS_enable_rx_interrupts\n");
+   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
+      hw_if->enable_rx_interrupt(qInx, &pdata->hw_cfg);
+   DBGPR("<--DWC_ETH_QOS_enable_rx_interrupts\n");
 }
 
 void DWC_ETH_QOS_disable_tx_interrupts(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	DBGPR("-->DWC_ETH_QOS_disable_tx_interrupts\n");
-	for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++)
-		hw_if->disable_tx_interrupt(qInx);
-	DBGPR("<--DWC_ETH_QOS_disable_tx_interrupts\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t qInx;
+   DBGPR("-->DWC_ETH_QOS_disable_tx_interrupts\n");
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++)
+      hw_if->disable_tx_interrupt(qInx, &pdata->hw_cfg);
+   DBGPR("<--DWC_ETH_QOS_disable_tx_interrupts\n");
 }
 
 void DWC_ETH_QOS_enable_tx_interrupts(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	DBGPR("-->DWC_ETH_QOS_enable_tx_interrupts\n");
-	for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++)
-		hw_if->enable_tx_interrupt(qInx);
-	DBGPR("<--DWC_ETH_QOS_enable_tx_interrupts\n");
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t qInx;
+   DBGPR("-->DWC_ETH_QOS_enable_tx_interrupts\n");
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++)
+      hw_if->enable_tx_interrupt(qInx, &pdata->hw_cfg);
+   DBGPR("<--DWC_ETH_QOS_enable_tx_interrupts\n");
 }
 
-
 /*!
 * \brief Interrupt Service Routine
 * \details Interrupt Service Routine
@@ -473,196 +495,197 @@ void DWC_ETH_QOS_enable_tx_interrupts(st
 * \return returns positive integer
 * \retval IRQ_HANDLED
 */
+irqreturn_t DWC_ETH_QOS_ISR(int irq, void *device_id)
+{
+   uint32_t varDMA_ISR;
+   uint32_t varDMA_SR;
+   uint32_t varMAC_ISR;
+   uint32_t varMAC_PMTCSR;
+   struct DWC_ETH_QOS_prv_data *pdata =
+       (struct DWC_ETH_QOS_prv_data *)device_id;
+   struct net_device *dev = pdata->dev;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   hw_config_t *hw_cfg = &(pdata->hw_cfg);
+   uint32_t qInx;
+   uint32_t varMAC_ANS = 0, varMAC_PCS = 0;
+   unsigned int sched_rx_napi = 0, atom_ims = 0;
+   void __iomem *reg_base = pdata->gbe_base;
+
+   DBGPR("-->DWC_ETH_QOS_ISR\n");
+
+   /* Clock is gated when NetIP is powered down and any
+      register operation will hang the system. */
+   BUG_ON(pdata->power_state & DWC_ETH_QOS_NETIP_WAKEUP);
+
+   if (pdata->version == MAC_VER_4_00) {
+      // Read and clear interrupt in Atom Interrupt Controller
+      atom_ims = GBE_REG_RD(GBE_ATOM_IMS); //Interrupts are cleared when IMS is read
+      if (!VAR32_GET_BIT(atom_ims, GBE_ATOM_INTC))
+         printk(KERN_ALERT "GMAC5 interrupt with bit not set on IC!\n");
+   }
 
-#define HI_PRIORITY_INT  0x1
-#define LO_PRIORITY_INT  0x2
+   varDMA_ISR = DWC_REG_RD(DMA_ISR);
+   varMAC_ISR = DWC_REG_RD(MAC_ISR);
+   if (varDMA_ISR == 0x0 &&
+      !(pdata->power_state & DWC_ETH_QOS_NETIP_PWRDWN)) {
+      printk(KERN_ALERT "Unexpected interrupt - DMA(0x%08x) MAC(0x%08x)!\n",
+         varDMA_ISR, varMAC_ISR);
+      netss_interrupt_ack(NETSS_INTERUPT_GBE);
+      return IRQ_NONE;
+   }
 
-irqreturn_t DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS(int irq, void *device_id)
-{
-	uint32_t varDMA_ISR;
-	uint32_t varDMA_SR;
-	uint32_t varMAC_ISR;
-	uint32_t varMAC_PMTCSR;
-	uint32_t varDMA_IER;
-	struct DWC_ETH_QOS_prv_data *pdata =
-	    (struct DWC_ETH_QOS_prv_data *)device_id;
-	struct net_device *dev = pdata->dev;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t qInx;
-	uint32_t varMAC_ANS = 0, varMAC_PCS = 0;
-	unsigned int sched_rx_napi = 0, atom_ims = 0, gmac5_int = 0;
-	void __iomem *reg_base = pdata->gbe_base;
-
-	DBGPR("-->DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS\n");
-
-	if (pdata->version == MAC_VER_4_00) {
-		// Read and clear interrupt in Atom Interrupt Controller
-		atom_ims = GBE_REG_RD(GBE_ATOM_IMS); //Interrupts are cleared when IMS is read
-		gmac5_int = VAR32_GET_BIT(atom_ims, GBE_ATOM_INTC);
-		if (!gmac5_int)
-			printk(KERN_ALERT "GMAC5 interrupt with bit not set on IC!\n");
-	}
-
-	varDMA_ISR = DWC_REG_RD(DMA_ISR);
-	varMAC_ISR = DWC_REG_RD(MAC_ISR);
-	if (varDMA_ISR == 0x0) {
-		printk(KERN_ALERT "Unexpected GMAC5 interrupt - DMA(0x%08x) MAC(0x%08x)!\n", varDMA_ISR, varMAC_ISR);
-		netss_interrupt_ack(NETSS_INTERUPT_GBE);
-		return IRQ_NONE;
-	}
-
-	/* Handle DMA interrupts */
-	for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
-
-		varDMA_SR = DWC_REG_RD(DMA_SR(qInx));
-		/* clear interrupts */
-		DWC_REG_WR(DMA_SR(qInx), varDMA_SR);
-
-		varDMA_IER = DWC_REG_RD(DMA_IER(qInx));
-		/* handle only those DMA interrupts which are enabled */
-		varDMA_SR &= varDMA_IER;
-
-		DBGPR("DMA_SR[%d] = %#lx\n", qInx, varDMA_SR);
-
-		if (varDMA_SR == 0)
-			continue;
-
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RI)) {
-			pdata->xstats.rx_normal_irq_n[qInx]++;
-			sched_rx_napi |= LO_PRIORITY_INT;
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RBU)) {
-			pdata->xstats.rx_buf_unavailable_irq_n[qInx]++;
-			sched_rx_napi |= HI_PRIORITY_INT;
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_TI)) {
-			pdata->xstats.tx_normal_irq_n[qInx]++;
+   /* Handle DMA interrupts */
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
+
+      varDMA_SR = DWC_REG_RD(DMA_SR(qInx));
+      /* Clear interrupts */
+      DWC_REG_WR(DMA_SR(qInx), varDMA_SR);
+      /* Handle only enabled DMA interrupts */
+      varDMA_SR &= hw_cfg->dma_ier;
+
+      DBGPR("DMA_SR[%d] = 0x%08x\n", qInx, varDMA_SR);
+
+      if (varDMA_SR == 0)
+         continue;
+
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RI)) {
+         pdata->xstats.rx_normal_irq_n[qInx]++;
+         sched_rx_napi |= LO_PRIORITY_INT;
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RBU)) {
+         pdata->xstats.rx_buf_unavailable_irq_n[qInx]++;
+         sched_rx_napi |= HI_PRIORITY_INT;
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_TI)) {
+         pdata->xstats.tx_normal_irq_n[qInx]++;
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-			sched_rx_napi |= LO_PRIORITY_INT;
+         sched_rx_napi |= LO_PRIORITY_INT;
 #else
-			DWC_ETH_QOS_tx_interrupt(dev, pdata, qInx);
+         DWC_ETH_QOS_tx_interrupt(dev, pdata, qInx);
 #endif
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_TPS)) {
-			pdata->xstats.tx_process_stopped_irq_n[qInx]++;
-			printk(KERN_ALERT "Tx stopped interrupt!\n");
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_TBU)) {
-			pdata->xstats.tx_buf_unavailable_irq_n[qInx]++;
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_TPS)) {
+         pdata->xstats.tx_process_stopped_irq_n[qInx]++;
+         DBGPR("Tx stopped interrupt!\n");
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_TBU)) {
+         pdata->xstats.tx_buf_unavailable_irq_n[qInx]++;
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-			sched_rx_napi |= LO_PRIORITY_INT;
+         sched_rx_napi |= LO_PRIORITY_INT;
 #else
-			DWC_ETH_QOS_tx_interrupt(dev, pdata, qInx);
+         DWC_ETH_QOS_tx_interrupt(dev, pdata, qInx);
 #endif
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RPS)) {
-			pdata->xstats.rx_process_stopped_irq_n[qInx]++;
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RWT)) {
-			pdata->xstats.rx_watchdog_irq_n++;
-		}
-		if (VAR32_GET_BIT(varDMA_SR, DMA_SR_FBE)) {
-			pdata->xstats.fatal_bus_error_irq_n++;
-			DWC_ETH_QOS_restart_dev(pdata, qInx);
-		}
-	}
-
-	/* Schedule Rx NAPI, if required*/
-	if (sched_rx_napi) {
-		if (pdata->rx_napi_pending) {
-			printk(KERN_ALERT "DMA interrupt while in polling!\n");
-		} else {
-			spin_lock(&pdata->lock);
-			DWC_ETH_QOS_disable_rx_interrupts(pdata);
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RPS)) {
+         pdata->xstats.rx_process_stopped_irq_n[qInx]++;
+         DBGPR("Rx stopped interrupt!\n");
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_RWT)) {
+         pdata->xstats.rx_watchdog_irq_n++;
+      }
+      if (VAR32_GET_BIT(varDMA_SR, DMA_SR_FBE)) {
+         pdata->xstats.fatal_bus_error_irq_n++;
+         DWC_ETH_QOS_restart_dev(pdata, qInx);
+      }
+   }
+
+   /* Schedule Rx NAPI, if required*/
+   if (sched_rx_napi) {
+      if (pdata->rx_napi_pending) {
+         printk(KERN_ALERT "DMA interrupt while in polling!\n");
+      } else {
+         spin_lock(&pdata->lock);
+         DWC_ETH_QOS_disable_rx_interrupts(pdata);
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-			DWC_ETH_QOS_disable_tx_interrupts(pdata);
+         DWC_ETH_QOS_disable_tx_interrupts(pdata);
 #endif
-			pdata->rx_napi_pending = true;
-			spin_unlock(&pdata->lock);
-			if (sched_rx_napi & HI_PRIORITY_INT) // || itr_mode == NONE
-				/* Schedule NAPI now */
-				napi_schedule(&pdata->rx_napi);
-			else
-				/* Delay schedule of NAPI */
-				hrtimer_start(&pdata->rx_itr_timer,
-					ns_to_ktime(pdata->itr_latency), HRTIMER_MODE_REL);
-		}
-	}
-
-	/* Handle MAC interrupts */
-	if (VAR32_GET_BIT(varDMA_ISR, DMA_ISR_MACIS)) {
-		/* handle only those MAC interrupts which are enabled */
-		varMAC_ISR &= DWC_REG_RD(MAC_IER);
-
-		/* PMT interrupt */
-		if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_PMTIS)) {
-			pdata->xstats.pmt_irq_n++;
-			varMAC_PMTCSR = DWC_REG_RD(MAC_PMT_CSR);
-			DBGPR("PMT interrupt: varMAC_PMTCSR = 0x%08x\n", varMAC_PMTCSR);
-			if (pdata->power_down)
-				DWC_ETH_QOS_powerup(pdata->dev, DWC_ETH_QOS_IOCTL_CONTEXT);
-		}
-
-		/* RGMII/SMII interrupt */
-		if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_RGMIIIS)) {
-			varMAC_PCS = DWC_REG_RD(MAC_PHY_CSR);
-			printk(KERN_ALERT "RGMII/SMII interrupt: MAC_PCS = 0x%08x\n", varMAC_PCS);
-			if ((varMAC_PCS & 0x80000) == 0x80000) {
-				pdata->pcs_link = 1;
-				netif_carrier_on(dev);
-				if ((varMAC_PCS & 0x10000) == 0x10000) {
-					pdata->pcs_duplex = 1;
-					hw_if->set_full_duplex();
-				} else {
-					pdata->pcs_duplex = 0;
-					hw_if->set_half_duplex();
-				}
-
-				if ((varMAC_PCS & 0x60000) == 0x0) {
-					hw_if->set_speed(pdata, 10);
-				} else if ((varMAC_PCS & 0x60000) == 0x20000) {
-					hw_if->set_speed(pdata, 100);
-				} else if ((varMAC_PCS & 0x60000) == 0x40000) {
-					hw_if->set_speed(pdata, 1000);
-				}
-				printk(KERN_ALERT "Link is UP:%dMbps & %s duplex\n",
-					pdata->pcs_speed, pdata->pcs_duplex ? "Full" : "Half");
-			} else {
-				printk(KERN_ALERT "Link is Down\n");
-				pdata->pcs_link = 0;
-				netif_carrier_off(dev);
-			}
-		}
-
-		/* PCS Link Status interrupt */
-		if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_PCSLCHGIS)) {
-			printk(KERN_ALERT "PCS Link Status interrupt\n");
-			if (DWC_REG_RD_BIT(MAC_ANSR, MAC_ANSR_LS)) {
-				printk(KERN_ALERT "Link: Up\n");
-				netif_carrier_on(dev);
-				pdata->pcs_link = 1;
-			} else {
-				printk(KERN_ALERT "Link: Down\n");
-				netif_carrier_off(dev);
-				pdata->pcs_link = 0;
-			}
-		}
-
-		/* PCS Auto-Negotiation Complete interrupt */
-		if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_PCSANCIS)) {
-			printk(KERN_ALERT "PCS Auto-Negotiation Complete interrupt\n");
-			varMAC_ANS = DWC_REG_RD(MAC_ANSR);
-		}
-
-		/* EEE interrupts */
-		if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_LPIIS)) {
-			DWC_ETH_QOS_handle_eee_interrupt(pdata);
-		}
-	}
-	netss_interrupt_ack(NETSS_INTERUPT_GBE);
-	DBGPR("<--DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS\n");
+         pdata->rx_napi_pending = true;
+         spin_unlock(&pdata->lock);
+         if (sched_rx_napi & HI_PRIORITY_INT) // || itr_mode == NONE
+            /* Schedule NAPI now */
+            napi_schedule(&pdata->rx_napi);
+         else
+            /* Delay schedule of NAPI */
+            hrtimer_start(&pdata->rx_itr_timer,
+               ns_to_ktime(pdata->itr_latency), HRTIMER_MODE_REL);
+      }
+   }
+
+   /* Handle MAC interrupts */
+   if (VAR32_GET_BIT(varDMA_ISR, DMA_ISR_MACIS)) {
+      /* handle only those MAC interrupts which are enabled */
+      varMAC_ISR &= hw_cfg->mac_ier;
+
+      /* PMT interrupt */
+      if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_PMTIS)) {
+         pdata->xstats.pmt_irq_n++;
+         varMAC_PMTCSR = DWC_REG_RD(MAC_PMT_CSR);
+         DBGPR("PMT interrupt: varMAC_PMTCSR = 0x%08x\n", varMAC_PMTCSR);
+         if (pdata->power_state &
+             (DWC_ETH_QOS_MAGIC_WAKEUP | DWC_ETH_QOS_REMOTE_WAKEUP))
+            DWC_ETH_QOS_powerup(pdata->dev);
+      }
+
+      /* RGMII/SMII interrupt */
+      if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_RGMIIIS)) {
+         varMAC_PCS = DWC_REG_RD(MAC_PHY_CSR);
+         printk(KERN_ALERT "RGMII/SMII interrupt: MAC_PCS = 0x%08x\n", varMAC_PCS);
+         if ((varMAC_PCS & 0x80000) == 0x80000) {
+            pdata->pcs_link = 1;
+            netif_carrier_on(dev);
+            if ((varMAC_PCS & 0x10000) == 0x10000) {
+               pdata->pcs_duplex = 1;
+               hw_if->set_full_duplex();
+            } else {
+               pdata->pcs_duplex = 0;
+               hw_if->set_half_duplex();
+            }
+
+            if ((varMAC_PCS & 0x60000) == 0x0) {
+               hw_if->set_speed(pdata, 10);
+            } else if ((varMAC_PCS & 0x60000) == 0x20000) {
+               hw_if->set_speed(pdata, 100);
+            } else if ((varMAC_PCS & 0x60000) == 0x40000) {
+               hw_if->set_speed(pdata, 1000);
+            }
+            printk(KERN_ALERT "Link is UP:%dMbps & %s duplex\n",
+               pdata->pcs_speed, pdata->pcs_duplex ? "Full" : "Half");
+         } else {
+            printk(KERN_ALERT "Link is Down\n");
+            pdata->pcs_link = 0;
+            netif_carrier_off(dev);
+         }
+      }
+
+      /* PCS Link Status interrupt */
+      if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_PCSLCHGIS)) {
+         printk(KERN_ALERT "PCS Link Status interrupt\n");
+         if (DWC_REG_RD_BIT(MAC_ANSR, MAC_ANSR_LS)) {
+            printk(KERN_ALERT "Link: Up\n");
+            netif_carrier_on(dev);
+            pdata->pcs_link = 1;
+         } else {
+            printk(KERN_ALERT "Link: Down\n");
+            netif_carrier_off(dev);
+            pdata->pcs_link = 0;
+         }
+      }
+
+      /* PCS Auto-Negotiation Complete interrupt */
+      if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_PCSANCIS)) {
+         printk(KERN_ALERT "PCS Auto-Negotiation Complete interrupt\n");
+         varMAC_ANS = DWC_REG_RD(MAC_ANSR);
+      }
 
-	return IRQ_HANDLED;
+      /* EEE interrupts */
+      if (VAR32_GET_BIT(varMAC_ISR, MAC_ISR_LPIIS)) {
+         DWC_ETH_QOS_handle_eee_interrupt(pdata);
+      }
+   }
+   netss_interrupt_ack(NETSS_INTERUPT_GBE);
+   DBGPR("<--DWC_ETH_QOS_ISR\n");
+
+   return IRQ_HANDLED;
 }
 
 /*!
@@ -677,57 +700,56 @@ irqreturn_t DWC_ETH_QOS_ISR_SW_DWC_ETH_Q
 */
 void DWC_ETH_QOS_get_all_hw_features(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	uint32_t varMAC_HF0R = DWC_REG_RD(MAC_HF0R);
-	uint32_t varMAC_HF1R = DWC_REG_RD(MAC_HF1R);
-	uint32_t varMAC_HF2R = DWC_REG_RD(MAC_HF2R);
-
-	DBGPR("-->DWC_ETH_QOS_get_all_hw_features\n");
-
-	memset(&pdata->hw_feat, 0, sizeof(pdata->hw_feat));
-	pdata->hw_feat.mii_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MIISEL);
-	pdata->hw_feat.gmii_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_GMIISEL);
-	pdata->hw_feat.hd_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_HDSEL);
-	pdata->hw_feat.pcs_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_PCSSEL);
-	pdata->hw_feat.vlan_hash_en = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_VLHASH);
-	pdata->hw_feat.sma_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_SMASEL);
-	pdata->hw_feat.rwk_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_RWKSEL);
-	pdata->hw_feat.mgk_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MGKSEL);
-	pdata->hw_feat.mmc_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MMCSEL);
-	pdata->hw_feat.arp_offld_en = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_ARPOFFSEL);
-	pdata->hw_feat.ts_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_TSSEL);
-	pdata->hw_feat.eee_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_EEESEL);
-	pdata->hw_feat.tx_coe_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_TXCOESEL);
-	pdata->hw_feat.rx_coe_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_RXCOESEL);
-	pdata->hw_feat.mac_addr16_sel = VAR32_GET_FIELD(varMAC_HF0R, MAC_HF0R_ADDMACADRSEL);
-	pdata->hw_feat.mac_addr32_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MACADR32SEL);
-	pdata->hw_feat.mac_addr64_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MACADR64SEL);
-	pdata->hw_feat.tsstssel = VAR32_GET_FIELD(varMAC_HF0R, MAC_HF0R_TSSTSSEL);
-	pdata->hw_feat.sa_vlan_ins = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_SAVLANINS);
-	pdata->hw_feat.act_phy_sel = VAR32_GET_FIELD(varMAC_HF0R, MAC_HF0R_ACTPHYSEL);
-
-	pdata->hw_feat.rx_fifo_size = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_RXFIFOSIZE);
-	pdata->hw_feat.tx_fifo_size = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_TXFIFOSIZE);
-	pdata->hw_feat.adv_ts_hword = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_ADVTHWORD);
-	pdata->hw_feat.dcb_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_DCBEN);
-	pdata->hw_feat.sph_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_SPHEN);
-	pdata->hw_feat.tso_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_TSOEN);
-	pdata->hw_feat.dma_debug_gen = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_DBGMEMA);
-	pdata->hw_feat.av_sel = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_AVSEL);
-	pdata->hw_feat.lp_mode_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_LPMODEEN);
-	pdata->hw_feat.hash_tbl_sz = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_HASHTBLSZ);
-	pdata->hw_feat.l3l4_filter_num = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_L3L4FNUM);
-
-	pdata->hw_feat.rx_q_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_RXQCNT);
-	pdata->hw_feat.tx_q_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_TXQCNT);
-	pdata->hw_feat.rx_ch_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_RXCHCNT);
-	pdata->hw_feat.tx_ch_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_TXCHCNT);
-	pdata->hw_feat.pps_out_num = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_PPSOUTNUM);
-	pdata->hw_feat.aux_snap_num = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_AUXSNAPNUM);
+   uint32_t varMAC_HF0R = DWC_REG_RD(MAC_HF0R);
+   uint32_t varMAC_HF1R = DWC_REG_RD(MAC_HF1R);
+   uint32_t varMAC_HF2R = DWC_REG_RD(MAC_HF2R);
+
+   DBGPR("-->DWC_ETH_QOS_get_all_hw_features\n");
+
+   memset(&pdata->hw_feat, 0, sizeof(pdata->hw_feat));
+   pdata->hw_feat.mii_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MIISEL);
+   pdata->hw_feat.gmii_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_GMIISEL);
+   pdata->hw_feat.hd_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_HDSEL);
+   pdata->hw_feat.pcs_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_PCSSEL);
+   pdata->hw_feat.vlan_hash_en = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_VLHASH);
+   pdata->hw_feat.sma_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_SMASEL);
+   pdata->hw_feat.rwk_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_RWKSEL);
+   pdata->hw_feat.mgk_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MGKSEL);
+   pdata->hw_feat.mmc_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MMCSEL);
+   pdata->hw_feat.arp_offld_en = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_ARPOFFSEL);
+   pdata->hw_feat.ts_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_TSSEL);
+   pdata->hw_feat.eee_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_EEESEL);
+   pdata->hw_feat.tx_coe_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_TXCOESEL);
+   pdata->hw_feat.rx_coe_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_RXCOESEL);
+   pdata->hw_feat.mac_addr16_sel = VAR32_GET_FIELD(varMAC_HF0R, MAC_HF0R_ADDMACADRSEL);
+   pdata->hw_feat.mac_addr32_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MACADR32SEL);
+   pdata->hw_feat.mac_addr64_sel = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_MACADR64SEL);
+   pdata->hw_feat.tsstssel = VAR32_GET_FIELD(varMAC_HF0R, MAC_HF0R_TSSTSSEL);
+   pdata->hw_feat.sa_vlan_ins = VAR32_GET_BIT(varMAC_HF0R, MAC_HF0R_SAVLANINS);
+   pdata->hw_feat.act_phy_sel = VAR32_GET_FIELD(varMAC_HF0R, MAC_HF0R_ACTPHYSEL);
+
+   pdata->hw_feat.rx_fifo_size = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_RXFIFOSIZE);
+   pdata->hw_feat.tx_fifo_size = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_TXFIFOSIZE);
+   pdata->hw_feat.adv_ts_hword = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_ADVTHWORD);
+   pdata->hw_feat.dcb_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_DCBEN);
+   pdata->hw_feat.sph_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_SPHEN);
+   pdata->hw_feat.tso_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_TSOEN);
+   pdata->hw_feat.dma_debug_gen = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_DBGMEMA);
+   pdata->hw_feat.av_sel = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_AVSEL);
+   pdata->hw_feat.lp_mode_en = VAR32_GET_BIT(varMAC_HF1R, MAC_HF1R_LPMODEEN);
+   pdata->hw_feat.hash_tbl_sz = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_HASHTBLSZ);
+   pdata->hw_feat.l3l4_filter_num = VAR32_GET_FIELD(varMAC_HF1R, MAC_HF1R_L3L4FNUM);
+
+   pdata->hw_feat.rx_q_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_RXQCNT);
+   pdata->hw_feat.tx_q_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_TXQCNT);
+   pdata->hw_feat.rx_ch_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_RXCHCNT);
+   pdata->hw_feat.tx_ch_cnt = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_TXCHCNT);
+   pdata->hw_feat.pps_out_num = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_PPSOUTNUM);
+   pdata->hw_feat.aux_snap_num = VAR32_GET_FIELD(varMAC_HF2R, MAC_HF2R_AUXSNAPNUM);
 
-	DBGPR("<--DWC_ETH_QOS_get_all_hw_features\n");
+   DBGPR("<--DWC_ETH_QOS_get_all_hw_features\n");
 }
 
-
 /*!
 * \brief API to print all hw features.
 *
@@ -739,306 +761,305 @@ void DWC_ETH_QOS_get_all_hw_features(str
 */
 void DWC_ETH_QOS_print_all_hw_features(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	char *str = NULL;
+   char *str = NULL;
 
-	DBGPR("-->DWC_ETH_QOS_print_all_hw_features\n");
+   DBGPR("-->DWC_ETH_QOS_print_all_hw_features\n");
 
-	CFG_PRINT("\n");
-	CFG_PRINT("=====================================================/\n");
-	CFG_PRINT("\n");
-	CFG_PRINT("10/100 Mbps Support                         : %s\n",
-		pdata->hw_feat.mii_sel ? "YES" : "NO");
-	CFG_PRINT("1000 Mbps Support                           : %s\n",
-		pdata->hw_feat.gmii_sel ? "YES" : "NO");
-	CFG_PRINT("Half-duplex Support                         : %s\n",
-		pdata->hw_feat.hd_sel ? "YES" : "NO");
-	CFG_PRINT("PCS Registers(TBI/SGMII/RTBI PHY interface) : %s\n",
-		pdata->hw_feat.pcs_sel ? "YES" : "NO");
-	CFG_PRINT("VLAN Hash Filter Selected                   : %s\n",
-		pdata->hw_feat.vlan_hash_en ? "YES" : "NO");
-	pdata->vlan_hash_filtering = pdata->hw_feat.vlan_hash_en;
-	CFG_PRINT("SMA (MDIO) Interface                        : %s\n",
-		pdata->hw_feat.sma_sel ? "YES" : "NO");
-	CFG_PRINT("PMT Remote Wake-up Packet Enable            : %s\n",
-		pdata->hw_feat.rwk_sel ? "YES" : "NO");
-	CFG_PRINT("PMT Magic Packet Enable                     : %s\n",
-		pdata->hw_feat.mgk_sel ? "YES" : "NO");
-	CFG_PRINT("RMON/MMC Module Enable                      : %s\n",
-		pdata->hw_feat.mmc_sel ? "YES" : "NO");
-	CFG_PRINT("ARP Offload Enabled                         : %s\n",
-		pdata->hw_feat.arp_offld_en ? "YES" : "NO");
-	CFG_PRINT("IEEE 1588-2008 Timestamp Enabled            : %s\n",
-		pdata->hw_feat.ts_sel ? "YES" : "NO");
-	CFG_PRINT("Energy Efficient Ethernet Enabled           : %s\n",
-		pdata->hw_feat.eee_sel ? "YES" : "NO");
-	CFG_PRINT("Transmit Checksum Offload Enabled           : %s\n",
-		pdata->hw_feat.tx_coe_sel ? "YES" : "NO");
-	CFG_PRINT("Receive Checksum Offload Enabled            : %s\n",
-		pdata->hw_feat.rx_coe_sel ? "YES" : "NO");
-	CFG_PRINT("MAC Addresses 16–31 Selected                : %s\n",
-		pdata->hw_feat.mac_addr16_sel ? "YES" : "NO");
-	CFG_PRINT("MAC Addresses 32–63 Selected                : %s\n",
-		pdata->hw_feat.mac_addr32_sel ? "YES" : "NO");
-	CFG_PRINT("MAC Addresses 64–127 Selected               : %s\n",
-		pdata->hw_feat.mac_addr64_sel ? "YES" : "NO");
-
-	if (pdata->hw_feat.mac_addr64_sel)
-		pdata->max_addr_reg_cnt = 128;
-	else if (pdata->hw_feat.mac_addr32_sel)
-		pdata->max_addr_reg_cnt = 64;
-	else if (pdata->hw_feat.mac_addr16_sel)
-		pdata->max_addr_reg_cnt = 32;
-	else
-		pdata->max_addr_reg_cnt = 1;
-
-	switch(pdata->hw_feat.tsstssel) {
-	case 0:
-		str = "RESERVED";
-		break;
-	case 1:
-		str = "INTERNAL";
-		break;
-	case 2:
-		str = "EXTERNAL";
-		break;
-	case 3:
-		str = "BOTH";
-		break;
-	}
-	CFG_PRINT("Timestamp System Time Source                : %s\n",
-		str);
-	CFG_PRINT("Source Address or VLAN Insertion Enable     : %s\n",
-		pdata->hw_feat.sa_vlan_ins ? "YES" : "NO");
-
-	switch (pdata->hw_feat.act_phy_sel) {
-	case 0:
-		str = "GMII/MII";
-		break;
-	case 1:
-		str = "RGMII";
-		break;
-	case 2:
-		str = "SGMII";
-		break;
-	case 3:
-		str = "TBI";
-		break;
-	case 4:
-		str = "RMII";
-		break;
-	case 5:
-		str = "RTBI";
-		break;
-	case 6:
-		str = "SMII";
-		break;
-	case 7:
-		str = "RevMII";
-		break;
-	default:
-		str = "RESERVED";
-	}
-	CFG_PRINT("Active PHY Selected                         : %s\n", str);
-
-	switch(pdata->hw_feat.rx_fifo_size) {
-	case 0:
-		str = "128 bytes";
-		break;
-	case 1:
-		str = "256 bytes";
-		break;
-	case 2:
-		str = "512 bytes";
-		break;
-	case 3:
-		str = "1 KBytes";
-		break;
-	case 4:
-		str = "2 KBytes";
-		break;
-	case 5:
-		str = "4 KBytes";
-		break;
-	case 6:
-		str = "8 KBytes";
-		break;
-	case 7:
-		str = "16 KBytes";
-		break;
-	case 8:
-		str = "32 kBytes";
-		break;
-	case 9:
-		str = "64 KBytes";
-		break;
-	case 10:
-		str = "128 KBytes";
-		break;
-	case 11:
-		str = "256 KBytes";
-		break;
-	default:
-		str = "RESERVED";
-	}
-	CFG_PRINT("MTL Receive FIFO Size                       : %s\n", str);
-
-	switch(pdata->hw_feat.tx_fifo_size) {
-	case 0:
-		str = "128 bytes";
-		break;
-	case 1:
-		str = "256 bytes";
-		break;
-	case 2:
-		str = "512 bytes";
-		break;
-	case 3:
-		str = "1 KBytes";
-		break;
-	case 4:
-		str = "2 KBytes";
-		break;
-	case 5:
-		str = "4 KBytes";
-		break;
-	case 6:
-		str = "8 KBytes";
-		break;
-	case 7:
-		str = "16 KBytes";
-		break;
-	case 8:
-		str = "32 kBytes";
-		break;
-	case 9:
-		str = "64 KBytes";
-		break;
-	case 10:
-		str = "128 KBytes";
-		break;
-	case 11:
-		str = "256 KBytes";
-		break;
-	default:
-		str = "RESERVED";
-	}
-	CFG_PRINT("MTL Transmit FIFO Size                       : %s\n", str);
-	CFG_PRINT("IEEE 1588 High Word Register Enable          : %s\n",
-		pdata->hw_feat.adv_ts_hword ? "YES" : "NO");
-	CFG_PRINT("DCB Feature Enable                           : %s\n",
-		pdata->hw_feat.dcb_en ? "YES" : "NO");
-	CFG_PRINT("Split Header Feature Enable                  : %s\n",
-		pdata->hw_feat.sph_en ? "YES" : "NO");
-	CFG_PRINT("TCP Segmentation Offload Enable              : %s\n",
-		pdata->hw_feat.tso_en ? "YES" : "NO");
-	CFG_PRINT("DMA Debug Registers Enabled                  : %s\n",
-		pdata->hw_feat.dma_debug_gen ? "YES" : "NO");
-	CFG_PRINT("AV Feature Enabled                           : %s\n",
-		pdata->hw_feat.av_sel ? "YES" : "NO");
-	CFG_PRINT("Low Power Mode Enabled                       : %s\n",
-		pdata->hw_feat.lp_mode_en ? "YES" : "NO");
-
-	switch(pdata->hw_feat.hash_tbl_sz) {
-	case 0:
-		str = "No hash table selected";
-		pdata->max_hash_table_size = 0;
-		break;
-	case 1:
-		str = "64";
-		pdata->max_hash_table_size = 64;
-		break;
-	case 2:
-		str = "128";
-		pdata->max_hash_table_size = 128;
-		break;
-	case 3:
-		str = "256";
-		pdata->max_hash_table_size = 256;
-		break;
-	}
-	CFG_PRINT("Hash Table Size                              : %s\n", str);
-	CFG_PRINT("Total number of L3 or L4 Filters             : %d L3/L4 Filter\n",
-		pdata->hw_feat.l3l4_filter_num);
-	CFG_PRINT("Number of MTL Receive Queues                 : %d\n",
-		(pdata->hw_feat.rx_q_cnt + 1));
-	CFG_PRINT("Number of MTL Transmit Queues                : %d\n",
-		(pdata->hw_feat.tx_q_cnt + 1));
-	CFG_PRINT("Number of DMA Receive Channels               : %d\n",
-		(pdata->hw_feat.rx_ch_cnt + 1));
-	CFG_PRINT("Number of DMA Transmit Channels              : %d\n",
-		(pdata->hw_feat.tx_ch_cnt + 1));
-
-	switch(pdata->hw_feat.pps_out_num) {
-	case 0:
-		str = "No PPS output";
-		break;
-	case 1:
-		str = "1 PPS output";
-		break;
-	case 2:
-		str = "2 PPS output";
-		break;
-	case 3:
-		str = "3 PPS output";
-		break;
-	case 4:
-		str = "4 PPS output";
-		break;
-	default:
-		str = "RESERVED";
-	}
-	CFG_PRINT("Number of PPS Outputs                        : %s\n", str);
-
-	switch(pdata->hw_feat.aux_snap_num) {
-	case 0:
-		str = "No auxillary input";
-		break;
-	case 1:
-		str = "1 auxillary input";
-		break;
-	case 2:
-		str = "2 auxillary input";
-		break;
-	case 3:
-		str = "3 auxillary input";
-		break;
-	case 4:
-		str = "4 auxillary input";
-		break;
-	default:
-		str = "RESERVED";
-	}
-	CFG_PRINT("Number of Auxiliary Snapshot Inputs          : %s\n", str);
+   CFG_PRINT("\n");
+   CFG_PRINT("=====================================================/\n");
+   CFG_PRINT("\n");
+   CFG_PRINT("10/100 Mbps Support                         : %s\n",
+      pdata->hw_feat.mii_sel ? "YES" : "NO");
+   CFG_PRINT("1000 Mbps Support                           : %s\n",
+      pdata->hw_feat.gmii_sel ? "YES" : "NO");
+   CFG_PRINT("Half-duplex Support                         : %s\n",
+      pdata->hw_feat.hd_sel ? "YES" : "NO");
+   CFG_PRINT("PCS Registers(TBI/SGMII/RTBI PHY interface) : %s\n",
+      pdata->hw_feat.pcs_sel ? "YES" : "NO");
+   CFG_PRINT("VLAN Hash Filter Selected                   : %s\n",
+      pdata->hw_feat.vlan_hash_en ? "YES" : "NO");
+   pdata->vlan_hash_filtering = pdata->hw_feat.vlan_hash_en;
+   CFG_PRINT("SMA (MDIO) Interface                        : %s\n",
+      pdata->hw_feat.sma_sel ? "YES" : "NO");
+   CFG_PRINT("PMT Remote Wake-up Packet Enable            : %s\n",
+      pdata->hw_feat.rwk_sel ? "YES" : "NO");
+   CFG_PRINT("PMT Magic Packet Enable                     : %s\n",
+      pdata->hw_feat.mgk_sel ? "YES" : "NO");
+   CFG_PRINT("RMON/MMC Module Enable                      : %s\n",
+      pdata->hw_feat.mmc_sel ? "YES" : "NO");
+   CFG_PRINT("ARP Offload Enabled                         : %s\n",
+      pdata->hw_feat.arp_offld_en ? "YES" : "NO");
+   CFG_PRINT("IEEE 1588-2008 Timestamp Enabled            : %s\n",
+      pdata->hw_feat.ts_sel ? "YES" : "NO");
+   CFG_PRINT("Energy Efficient Ethernet Enabled           : %s\n",
+      pdata->hw_feat.eee_sel ? "YES" : "NO");
+   CFG_PRINT("Transmit Checksum Offload Enabled           : %s\n",
+      pdata->hw_feat.tx_coe_sel ? "YES" : "NO");
+   CFG_PRINT("Receive Checksum Offload Enabled            : %s\n",
+      pdata->hw_feat.rx_coe_sel ? "YES" : "NO");
+   CFG_PRINT("MAC Addresses 16–31 Selected                : %s\n",
+      pdata->hw_feat.mac_addr16_sel ? "YES" : "NO");
+   CFG_PRINT("MAC Addresses 32–63 Selected                : %s\n",
+      pdata->hw_feat.mac_addr32_sel ? "YES" : "NO");
+   CFG_PRINT("MAC Addresses 64–127 Selected               : %s\n",
+      pdata->hw_feat.mac_addr64_sel ? "YES" : "NO");
+
+   if (pdata->hw_feat.mac_addr64_sel)
+      pdata->max_addr_reg_cnt = 128;
+   else if (pdata->hw_feat.mac_addr32_sel)
+      pdata->max_addr_reg_cnt = 64;
+   else if (pdata->hw_feat.mac_addr16_sel)
+      pdata->max_addr_reg_cnt = 32;
+   else
+      pdata->max_addr_reg_cnt = 1;
+
+   switch(pdata->hw_feat.tsstssel) {
+   case 0:
+      str = "RESERVED";
+      break;
+   case 1:
+      str = "INTERNAL";
+      break;
+   case 2:
+      str = "EXTERNAL";
+      break;
+   case 3:
+      str = "BOTH";
+      break;
+   }
+   CFG_PRINT("Timestamp System Time Source                : %s\n",
+      str);
+   CFG_PRINT("Source Address or VLAN Insertion Enable     : %s\n",
+      pdata->hw_feat.sa_vlan_ins ? "YES" : "NO");
+
+   switch (pdata->hw_feat.act_phy_sel) {
+   case 0:
+      str = "GMII/MII";
+      break;
+   case 1:
+      str = "RGMII";
+      break;
+   case 2:
+      str = "SGMII";
+      break;
+   case 3:
+      str = "TBI";
+      break;
+   case 4:
+      str = "RMII";
+      break;
+   case 5:
+      str = "RTBI";
+      break;
+   case 6:
+      str = "SMII";
+      break;
+   case 7:
+      str = "RevMII";
+      break;
+   default:
+      str = "RESERVED";
+   }
+   CFG_PRINT("Active PHY Selected                         : %s\n", str);
 
-	CFG_PRINT("=====================================================/\n");
+   switch(pdata->hw_feat.rx_fifo_size) {
+   case 0:
+      str = "128 bytes";
+      break;
+   case 1:
+      str = "256 bytes";
+      break;
+   case 2:
+      str = "512 bytes";
+      break;
+   case 3:
+      str = "1 KBytes";
+      break;
+   case 4:
+      str = "2 KBytes";
+      break;
+   case 5:
+      str = "4 KBytes";
+      break;
+   case 6:
+      str = "8 KBytes";
+      break;
+   case 7:
+      str = "16 KBytes";
+      break;
+   case 8:
+      str = "32 kBytes";
+      break;
+   case 9:
+      str = "64 KBytes";
+      break;
+   case 10:
+      str = "128 KBytes";
+      break;
+   case 11:
+      str = "256 KBytes";
+      break;
+   default:
+      str = "RESERVED";
+   }
+   CFG_PRINT("MTL Receive FIFO Size                       : %s\n", str);
 
-	DBGPR("<--DWC_ETH_QOS_print_all_hw_features\n");
-}
+   switch(pdata->hw_feat.tx_fifo_size) {
+   case 0:
+      str = "128 bytes";
+      break;
+   case 1:
+      str = "256 bytes";
+      break;
+   case 2:
+      str = "512 bytes";
+      break;
+   case 3:
+      str = "1 KBytes";
+      break;
+   case 4:
+      str = "2 KBytes";
+      break;
+   case 5:
+      str = "4 KBytes";
+      break;
+   case 6:
+      str = "8 KBytes";
+      break;
+   case 7:
+      str = "16 KBytes";
+      break;
+   case 8:
+      str = "32 kBytes";
+      break;
+   case 9:
+      str = "64 KBytes";
+      break;
+   case 10:
+      str = "128 KBytes";
+      break;
+   case 11:
+      str = "256 KBytes";
+      break;
+   default:
+      str = "RESERVED";
+   }
+   CFG_PRINT("MTL Transmit FIFO Size                       : %s\n", str);
+   CFG_PRINT("IEEE 1588 High Word Register Enable          : %s\n",
+      pdata->hw_feat.adv_ts_hword ? "YES" : "NO");
+   CFG_PRINT("DCB Feature Enable                           : %s\n",
+      pdata->hw_feat.dcb_en ? "YES" : "NO");
+   CFG_PRINT("Split Header Feature Enable                  : %s\n",
+      pdata->hw_feat.sph_en ? "YES" : "NO");
+   CFG_PRINT("TCP Segmentation Offload Enable              : %s\n",
+      pdata->hw_feat.tso_en ? "YES" : "NO");
+   CFG_PRINT("DMA Debug Registers Enabled                  : %s\n",
+      pdata->hw_feat.dma_debug_gen ? "YES" : "NO");
+   CFG_PRINT("AV Feature Enabled                           : %s\n",
+      pdata->hw_feat.av_sel ? "YES" : "NO");
+   CFG_PRINT("Low Power Mode Enabled                       : %s\n",
+      pdata->hw_feat.lp_mode_en ? "YES" : "NO");
+
+   switch(pdata->hw_feat.hash_tbl_sz) {
+   case 0:
+      str = "No hash table selected";
+      pdata->max_hash_table_size = 0;
+      break;
+   case 1:
+      str = "64";
+      pdata->max_hash_table_size = 64;
+      break;
+   case 2:
+      str = "128";
+      pdata->max_hash_table_size = 128;
+      break;
+   case 3:
+      str = "256";
+      pdata->max_hash_table_size = 256;
+      break;
+   }
+   CFG_PRINT("Hash Table Size                              : %s\n", str);
+   CFG_PRINT("Total number of L3 or L4 Filters             : %d L3/L4 Filter\n",
+      pdata->hw_feat.l3l4_filter_num);
+   CFG_PRINT("Number of MTL Receive Queues                 : %d\n",
+      (pdata->hw_feat.rx_q_cnt + 1));
+   CFG_PRINT("Number of MTL Transmit Queues                : %d\n",
+      (pdata->hw_feat.tx_q_cnt + 1));
+   CFG_PRINT("Number of DMA Receive Channels               : %d\n",
+      (pdata->hw_feat.rx_ch_cnt + 1));
+   CFG_PRINT("Number of DMA Transmit Channels              : %d\n",
+      (pdata->hw_feat.tx_ch_cnt + 1));
+
+   switch(pdata->hw_feat.pps_out_num) {
+   case 0:
+      str = "No PPS output";
+      break;
+   case 1:
+      str = "1 PPS output";
+      break;
+   case 2:
+      str = "2 PPS output";
+      break;
+   case 3:
+      str = "3 PPS output";
+      break;
+   case 4:
+      str = "4 PPS output";
+      break;
+   default:
+      str = "RESERVED";
+   }
+   CFG_PRINT("Number of PPS Outputs                        : %s\n", str);
 
+   switch(pdata->hw_feat.aux_snap_num) {
+   case 0:
+      str = "No auxillary input";
+      break;
+   case 1:
+      str = "1 auxillary input";
+      break;
+   case 2:
+      str = "2 auxillary input";
+      break;
+   case 3:
+      str = "3 auxillary input";
+      break;
+   case 4:
+      str = "4 auxillary input";
+      break;
+   default:
+      str = "RESERVED";
+   }
+   CFG_PRINT("Number of Auxiliary Snapshot Inputs          : %s\n", str);
+
+   CFG_PRINT("=====================================================/\n");
+
+   DBGPR("<--DWC_ETH_QOS_print_all_hw_features\n");
+}
 
 static const struct net_device_ops DWC_ETH_QOS_netdev_ops = {
-	.ndo_open = DWC_ETH_QOS_open,
-	.ndo_stop = DWC_ETH_QOS_close,
-	.ndo_start_xmit = DWC_ETH_QOS_start_xmit,
-	.ndo_get_stats = DWC_ETH_QOS_get_stats,
-	.ndo_set_rx_mode = DWC_ETH_QOS_set_rx_mode,
+   .ndo_open = DWC_ETH_QOS_open,
+   .ndo_stop = DWC_ETH_QOS_close,
+   .ndo_start_xmit = DWC_ETH_QOS_start_xmit,
+   .ndo_get_stats = DWC_ETH_QOS_get_stats,
+   .ndo_set_rx_mode = DWC_ETH_QOS_set_rx_mode,
 #ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller = DWC_ETH_QOS_poll_controller,
-#endif				/*end of CONFIG_NET_POLL_CONTROLLER */
-	.ndo_set_features = DWC_ETH_QOS_set_features,
-	.ndo_fix_features = DWC_ETH_QOS_fix_features,
-	.ndo_do_ioctl = DWC_ETH_QOS_ioctl,
-	.ndo_change_mtu = DWC_ETH_QOS_change_mtu,
+   .ndo_poll_controller = DWC_ETH_QOS_poll_controller,
+#endif            /*end of CONFIG_NET_POLL_CONTROLLER */
+   .ndo_set_features = DWC_ETH_QOS_set_features,
+   .ndo_fix_features = DWC_ETH_QOS_fix_features,
+   .ndo_do_ioctl = DWC_ETH_QOS_ioctl,
+   .ndo_change_mtu = DWC_ETH_QOS_change_mtu,
 #ifdef DWC_ETH_QOS_QUEUE_SELECT_ALGO
-	.ndo_select_queue = DWC_ETH_QOS_select_queue,
+   .ndo_select_queue = DWC_ETH_QOS_select_queue,
 #endif
-	.ndo_vlan_rx_add_vid = DWC_ETH_QOS_vlan_rx_add_vid,
-	.ndo_vlan_rx_kill_vid = DWC_ETH_QOS_vlan_rx_kill_vid,
+   .ndo_vlan_rx_add_vid = DWC_ETH_QOS_vlan_rx_add_vid,
+   .ndo_vlan_rx_kill_vid = DWC_ETH_QOS_vlan_rx_kill_vid,
 };
 
 struct net_device_ops *DWC_ETH_QOS_get_netdev_ops(void)
 {
-	return (struct net_device_ops *)&DWC_ETH_QOS_netdev_ops;
+   return (struct net_device_ops *)&DWC_ETH_QOS_netdev_ops;
 }
 
 /*!
@@ -1055,60 +1076,58 @@ struct net_device_ops *DWC_ETH_QOS_get_n
  *
  * \retval 0 on success and -ve number on failure.
  */
-
 static int DWC_ETH_QOS_alloc_split_hdr_rx_buf(
-		struct DWC_ETH_QOS_prv_data *pdata,
-		struct DWC_ETH_QOS_rx_buffer *buffer,
-		gfp_t gfp)
-{
-	struct sk_buff *skb = buffer->skb;
-
-	DBGPR("-->DWC_ETH_QOS_alloc_split_hdr_rx_buf\n");
-
-	if (skb) {
-		skb_trim(skb, 0);
-		goto check_page;
-	}
-
-	buffer->rx_hdr_size = DWC_ETH_QOS_MAX_HDR_SIZE;
-	/* allocate twice the maximum header size */
-	skb = __netdev_alloc_skb_ip_align(pdata->dev,
-			(2*buffer->rx_hdr_size),
-			gfp);
-	if (skb == NULL) {
-		printk(KERN_ALERT "Failed to allocate skb\n");
-		return -ENOMEM;
-	}
-	buffer->skb = skb;
-	DBGPR("Maximum header buffer size allocated = %d\n",
-		buffer->rx_hdr_size);
- check_page:
-	if (!buffer->dma)
-		buffer->dma = dma_map_single(&pdata->pdev->dev,
-					buffer->skb->data,
-					(2*buffer->rx_hdr_size),
-					DMA_FROM_DEVICE);
-	buffer->len = buffer->rx_hdr_size;
-
-	/* allocate a new page if necessary */
-	if (buffer->page2 == NULL) {
-		buffer->page2 = alloc_page(gfp);
-		if (unlikely(!buffer->page2)) {
-			printk(KERN_ALERT
-			"Failed to allocate page for second buffer\n");
-			return -ENOMEM;
-		}
-	}
-	if (!buffer->dma2)
-		buffer->dma2 = dma_map_page(&pdata->pdev->dev,
-				    buffer->page2, 0,
-				    PAGE_SIZE, DMA_FROM_DEVICE);
-	buffer->len2 = PAGE_SIZE;
-	buffer->mapped_as_page = Y_TRUE;
+      struct DWC_ETH_QOS_prv_data *pdata,
+      struct DWC_ETH_QOS_rx_buffer *buffer,
+      gfp_t gfp)
+{
+   struct sk_buff *skb = buffer->skb;
+
+   DBGPR("-->DWC_ETH_QOS_alloc_split_hdr_rx_buf\n");
+
+   if (skb) {
+      skb_trim(skb, 0);
+      goto check_page;
+   }
+
+   buffer->rx_hdr_size = DWC_ETH_QOS_MAX_HDR_SIZE;
+   /* allocate twice the maximum header size */
+   skb = __netdev_alloc_skb_ip_align(pdata->dev,
+         (2*buffer->rx_hdr_size),
+         gfp);
+   if (skb == NULL) {
+      printk(KERN_ALERT "Failed to allocate skb\n");
+      return -ENOMEM;
+   }
+   buffer->skb = skb;
+   DBGPR("Maximum header buffer size allocated = %d\n", buffer->rx_hdr_size);
+check_page:
+   if (!buffer->dma)
+      buffer->dma = dma_map_single(&pdata->pdev->dev,
+               buffer->skb->data,
+               (2*buffer->rx_hdr_size),
+               DMA_FROM_DEVICE);
+   buffer->len = buffer->rx_hdr_size;
+
+   /* allocate a new page if necessary */
+   if (buffer->page2 == NULL) {
+      buffer->page2 = alloc_page(gfp);
+      if (unlikely(!buffer->page2)) {
+         printk(KERN_ALERT
+         "Failed to allocate page for second buffer\n");
+         return -ENOMEM;
+      }
+   }
+   if (!buffer->dma2)
+      buffer->dma2 = dma_map_page(&pdata->pdev->dev,
+                buffer->page2, 0,
+                PAGE_SIZE, DMA_FROM_DEVICE);
+   buffer->len2 = PAGE_SIZE;
+   buffer->mapped_as_page = Y_TRUE;
 
-	DBGPR("<--DWC_ETH_QOS_alloc_split_hdr_rx_buf\n");
+   DBGPR("<--DWC_ETH_QOS_alloc_split_hdr_rx_buf\n");
 
-	return 0;
+   return 0;
 }
 
 /*!
@@ -1125,61 +1144,60 @@ static int DWC_ETH_QOS_alloc_split_hdr_r
  *
  * \retval 0 on success and -ve number on failure.
  */
-
 static int DWC_ETH_QOS_alloc_jumbo_rx_buf(struct DWC_ETH_QOS_prv_data *pdata,
-					  struct DWC_ETH_QOS_rx_buffer *buffer,
-					  gfp_t gfp)
+                 struct DWC_ETH_QOS_rx_buffer *buffer,
+                 gfp_t gfp)
 {
-	struct sk_buff *skb = buffer->skb;
-	unsigned int bufsz = (256 - 16);	/* for skb_reserve */
+   struct sk_buff *skb = buffer->skb;
+   unsigned int bufsz = (256 - 16);   /* for skb_reserve */
 
-	DBGPR("-->DWC_ETH_QOS_alloc_jumbo_rx_buf\n");
+   DBGPR("-->DWC_ETH_QOS_alloc_jumbo_rx_buf\n");
 
-	if (skb) {
-		skb_trim(skb, 0);
-		goto check_page;
-	}
-
-	skb = __netdev_alloc_skb_ip_align(pdata->dev, bufsz, gfp);
-	if (skb == NULL) {
-		printk(KERN_ALERT "Failed to allocate skb\n");
-		return -ENOMEM;
-	}
-	buffer->skb = skb;
- check_page:
-	/* allocate a new page if necessary */
-	if (buffer->page == NULL) {
-		buffer->page = alloc_page(gfp);
-		if (unlikely(!buffer->page)) {
-			printk(KERN_ALERT "Failed to allocate page\n");
-			return -ENOMEM;
-		}
-	}
-	if (!buffer->dma)
-		buffer->dma = dma_map_page(&pdata->pdev->dev,
-					   buffer->page, 0,
-					   PAGE_SIZE, DMA_FROM_DEVICE);
-	buffer->len = PAGE_SIZE;
-
-	if (buffer->page2 == NULL) {
-		buffer->page2 = alloc_page(gfp);
-		if (unlikely(!buffer->page2)) {
-			printk(KERN_ALERT
-			       "Failed to allocate page for second buffer\n");
-			return -ENOMEM;
-		}
-	}
-	if (!buffer->dma2)
-		buffer->dma2 = dma_map_page(&pdata->pdev->dev,
-					    buffer->page2, 0,
-					    PAGE_SIZE, DMA_FROM_DEVICE);
-	buffer->len2 = PAGE_SIZE;
+   if (skb) {
+      skb_trim(skb, 0);
+      goto check_page;
+   }
+
+   skb = __netdev_alloc_skb_ip_align(pdata->dev, bufsz, gfp);
+   if (skb == NULL) {
+      printk(KERN_ALERT "Failed to allocate skb\n");
+      return -ENOMEM;
+   }
+   buffer->skb = skb;
+check_page:
+   /* allocate a new page if necessary */
+   if (buffer->page == NULL) {
+      buffer->page = alloc_page(gfp);
+      if (unlikely(!buffer->page)) {
+         printk(KERN_ALERT "Failed to allocate page\n");
+         return -ENOMEM;
+      }
+   }
+   if (!buffer->dma)
+      buffer->dma = dma_map_page(&pdata->pdev->dev,
+                  buffer->page, 0,
+                  PAGE_SIZE, DMA_FROM_DEVICE);
+   buffer->len = PAGE_SIZE;
+
+   if (buffer->page2 == NULL) {
+      buffer->page2 = alloc_page(gfp);
+      if (unlikely(!buffer->page2)) {
+         printk(KERN_ALERT
+                "Failed to allocate page for second buffer\n");
+         return -ENOMEM;
+      }
+   }
+   if (!buffer->dma2)
+      buffer->dma2 = dma_map_page(&pdata->pdev->dev,
+                   buffer->page2, 0,
+                   PAGE_SIZE, DMA_FROM_DEVICE);
+   buffer->len2 = PAGE_SIZE;
 
-	buffer->mapped_as_page = Y_TRUE;
+   buffer->mapped_as_page = Y_TRUE;
 
-	DBGPR("<--DWC_ETH_QOS_alloc_jumbo_rx_buf\n");
+   DBGPR("<--DWC_ETH_QOS_alloc_jumbo_rx_buf\n");
 
-	return 0;
+   return 0;
 }
 
 /*!
@@ -1197,41 +1215,39 @@ static int DWC_ETH_QOS_alloc_jumbo_rx_bu
  *
  * \retval 0 on success and -ve number on failure.
  */
-
 static int DWC_ETH_QOS_alloc_rx_buf(struct DWC_ETH_QOS_prv_data *pdata,
-				    struct DWC_ETH_QOS_rx_buffer *buffer,
-				    gfp_t gfp)
+                struct DWC_ETH_QOS_rx_buffer *buffer,
+                gfp_t gfp)
 {
-	struct sk_buff *skb = buffer->skb;
+   struct sk_buff *skb = buffer->skb;
 
-	DBGPR("-->DWC_ETH_QOS_alloc_rx_buf\n");
+   DBGPR("-->DWC_ETH_QOS_alloc_rx_buf\n");
 
-	if (skb) {
-		skb_trim(skb, 0);
-		goto map_skb;
-	}
-
-	skb = __netdev_alloc_skb_ip_align(pdata->dev, pdata->rx_buffer_len, gfp);
-	if (skb == NULL) {
-		printk(KERN_ALERT "Failed to allocate skb\n");
-		return -ENOMEM;
-	}
-	buffer->skb = skb;
-	buffer->len = pdata->rx_buffer_len;
+   if (skb) {
+      skb_trim(skb, 0);
+      goto map_skb;
+   }
+
+   skb = __netdev_alloc_skb_ip_align(pdata->dev, pdata->rx_buffer_len, gfp);
+   if (skb == NULL) {
+      printk(KERN_ALERT "Failed to allocate skb\n");
+      return -ENOMEM;
+   }
+   buffer->skb = skb;
+   buffer->len = pdata->rx_buffer_len;
  map_skb:
-	buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
-				     pdata->rx_buffer_len, DMA_FROM_DEVICE);
-	if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
-		printk(KERN_ALERT "failed to do the RX dma map\n");
+   buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
+                 pdata->rx_buffer_len, DMA_FROM_DEVICE);
+   if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
+      printk(KERN_ALERT "failed to do the RX dma map\n");
 
-	buffer->mapped_as_page = Y_FALSE;
+   buffer->mapped_as_page = Y_FALSE;
 
-	DBGPR("<--DWC_ETH_QOS_alloc_rx_buf\n");
+   DBGPR("<--DWC_ETH_QOS_alloc_rx_buf\n");
 
-	return 0;
+   return 0;
 }
 
-
 /*!
  * \brief api to configure Rx function pointer after reset.
  *
@@ -1243,28 +1259,26 @@ static int DWC_ETH_QOS_alloc_rx_buf(stru
  *
  * \return void
  */
-
 static void DWC_ETH_QOS_configure_rx_fun_ptr(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	DBGPR("-->DWC_ETH_QOS_configure_rx_fun_ptr\n");
+   DBGPR("-->DWC_ETH_QOS_configure_rx_fun_ptr\n");
 
-	if (pdata->rx_split_hdr) {
-		pdata->clean_rx = DWC_ETH_QOS_clean_split_hdr_rx_irq;
-		pdata->alloc_rx_buf = DWC_ETH_QOS_alloc_split_hdr_rx_buf;
-	}
-	else if (pdata->dev->mtu > DWC_ETH_QOS_ETH_FRAME_LEN) {
-		pdata->clean_rx = DWC_ETH_QOS_clean_jumbo_rx_irq;
-		pdata->alloc_rx_buf = DWC_ETH_QOS_alloc_jumbo_rx_buf;
-	} else {
-		pdata->rx_buffer_len = DWC_ETH_QOS_ETH_FRAME_LEN;
-		pdata->clean_rx = DWC_ETH_QOS_clean_rx_irq;
-		pdata->alloc_rx_buf = DWC_ETH_QOS_alloc_rx_buf;
-	}
+   if (pdata->rx_split_hdr) {
+      pdata->clean_rx = DWC_ETH_QOS_clean_split_hdr_rx_irq;
+      pdata->alloc_rx_buf = DWC_ETH_QOS_alloc_split_hdr_rx_buf;
+   }
+   else if (pdata->dev->mtu > DWC_ETH_QOS_ETH_FRAME_LEN) {
+      pdata->clean_rx = DWC_ETH_QOS_clean_jumbo_rx_irq;
+      pdata->alloc_rx_buf = DWC_ETH_QOS_alloc_jumbo_rx_buf;
+   } else {
+      pdata->rx_buffer_len = DWC_ETH_QOS_ETH_FRAME_LEN;
+      pdata->clean_rx = DWC_ETH_QOS_clean_rx_irq;
+      pdata->alloc_rx_buf = DWC_ETH_QOS_alloc_rx_buf;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_configure_rx_fun_ptr\n");
+   DBGPR("<--DWC_ETH_QOS_configure_rx_fun_ptr\n");
 }
 
-
 /*!
  * \brief api to initialize default values.
  *
@@ -1278,30 +1292,29 @@ static void DWC_ETH_QOS_configure_rx_fun
 
 static void DWC_ETH_QOS_default_common_confs(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	DBGPR("-->DWC_ETH_QOS_default_common_confs\n");
+   DBGPR("-->DWC_ETH_QOS_default_common_confs\n");
 
-	pdata->drop_tx_pktburstcnt = 1;
-	pdata->mac_enable_count = 0;
-	pdata->incr_incrx = DWC_ETH_QOS_INCR_ENABLE;
-	pdata->flow_ctrl = DWC_ETH_QOS_FLOW_CTRL_TX_RX;
-	pdata->oldflow_ctrl = DWC_ETH_QOS_FLOW_CTRL_TX_RX;
-	pdata->power_down = 0;
-	pdata->tx_sa_ctrl_via_desc = DWC_ETH_QOS_SA0_NONE;
-	pdata->tx_sa_ctrl_via_reg = DWC_ETH_QOS_SA0_NONE;
-	pdata->hwts_tx_en = 0;
-	pdata->hwts_rx_en = 0;
-	pdata->l3_l4_filter = 0;
-	pdata->l2_filtering_mode = !!pdata->hw_feat.hash_tbl_sz;
-	pdata->tx_path_in_lpi_mode = 0;
-	pdata->use_lpi_tx_automate = true;
-	pdata->eee_active = 0;
-	pdata->one_nsec_accuracy = 1;
-	pdata->rx_napi_pending = false;
+   pdata->drop_tx_pktburstcnt = 1;
+   pdata->mac_enable_count = 0;
+   pdata->incr_incrx = DWC_ETH_QOS_INCR_ENABLE;
+   pdata->flow_ctrl = DWC_ETH_QOS_FLOW_CTRL_TX_RX;
+   pdata->oldflow_ctrl = DWC_ETH_QOS_FLOW_CTRL_TX_RX;
+   pdata->power_state = DWC_ETH_QOS_POWER_ON;
+   pdata->tx_sa_ctrl_via_desc = DWC_ETH_QOS_SA0_NONE;
+   pdata->tx_sa_ctrl_via_reg = DWC_ETH_QOS_SA0_NONE;
+   pdata->hwts_tx_en = 0;
+   pdata->hwts_rx_en = 0;
+   pdata->l3_l4_filter = 0;
+   pdata->l2_filtering_mode = !!pdata->hw_feat.hash_tbl_sz;
+   pdata->tx_path_in_lpi_mode = 0;
+   pdata->use_lpi_tx_automate = true;
+   pdata->eee_active = 0;
+   pdata->one_nsec_accuracy = 1;
+   pdata->rx_napi_pending = false;
 
-	DBGPR("<--DWC_ETH_QOS_default_common_confs\n");
+   DBGPR("<--DWC_ETH_QOS_default_common_confs\n");
 }
 
-
 /*!
  * \brief api to initialize Tx parameters.
  *
@@ -1313,33 +1326,31 @@ static void DWC_ETH_QOS_default_common_c
  *
  * \return void
  */
-
 static void DWC_ETH_QOS_default_tx_confs_single_q(
-		struct DWC_ETH_QOS_prv_data *pdata,
-		uint32_t qInx)
+      struct DWC_ETH_QOS_prv_data *pdata,
+      uint32_t qInx)
 {
-	struct DWC_ETH_QOS_tx_queue *queue_data = GET_TX_QUEUE_PTR(qInx);
-	struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
-		GET_TX_WRAPPER_DESC(qInx);
-
-	DBGPR("-->DWC_ETH_QOS_default_tx_confs_single_q\n");
-
-	queue_data->q_op_mode = q_op_mode[qInx];
-
-	desc_data->tx_threshold_val = DWC_ETH_QOS_TX_THRESHOLD_32;
-	desc_data->tsf_on = DWC_ETH_QOS_TSF_ENABLE;
-	desc_data->osf_on = DWC_ETH_QOS_OSF_ENABLE;
-	desc_data->tx_pbl = DWC_ETH_QOS_PBL_16;
-	desc_data->tx_vlan_tag_via_reg = Y_FALSE;
-	desc_data->tx_vlan_tag_ctrl = DWC_ETH_QOS_TX_VLAN_TAG_INSERT;
-	desc_data->vlan_tag_present = 0;
-	desc_data->context_setup = 0;
-	desc_data->default_mss = 0;
+   struct DWC_ETH_QOS_tx_queue *queue_data = GET_TX_QUEUE_PTR(qInx);
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
+      GET_TX_WRAPPER_DESC(qInx);
+
+   DBGPR("-->DWC_ETH_QOS_default_tx_confs_single_q\n");
+
+   queue_data->q_op_mode = q_op_mode[qInx];
+
+   desc_data->tx_threshold_val = DWC_ETH_QOS_TX_THRESHOLD_32;
+   desc_data->tsf_on = DWC_ETH_QOS_TSF_ENABLE;
+   desc_data->osf_on = DWC_ETH_QOS_OSF_ENABLE;
+   desc_data->tx_pbl = DWC_ETH_QOS_PBL_16;
+   desc_data->tx_vlan_tag_via_reg = Y_FALSE;
+   desc_data->tx_vlan_tag_ctrl = DWC_ETH_QOS_TX_VLAN_TAG_INSERT;
+   desc_data->vlan_tag_present = 0;
+   desc_data->context_setup = 0;
+   desc_data->default_mss = 0;
 
-	DBGPR("<--DWC_ETH_QOS_default_tx_confs_single_q\n");
+   DBGPR("<--DWC_ETH_QOS_default_tx_confs_single_q\n");
 }
 
-
 /*!
  * \brief api to initialize Rx parameters.
  *
@@ -1351,76 +1362,75 @@ static void DWC_ETH_QOS_default_tx_confs
  *
  * \return void
  */
-
 static void DWC_ETH_QOS_default_rx_confs_single_q(
-		struct DWC_ETH_QOS_prv_data *pdata,
-		uint32_t qInx)
+      struct DWC_ETH_QOS_prv_data *pdata,
+      uint32_t qInx)
 {
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
-		GET_RX_WRAPPER_DESC(qInx);
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
+      GET_RX_WRAPPER_DESC(qInx);
 
-	DBGPR("-->DWC_ETH_QOS_default_rx_confs_single_q\n");
+   DBGPR("-->DWC_ETH_QOS_default_rx_confs_single_q\n");
 
-	desc_data->rx_threshold_val = DWC_ETH_QOS_RX_THRESHOLD_64;
-	desc_data->rsf_on = DWC_ETH_QOS_RSF_DISABLE;
-	desc_data->rx_pbl = DWC_ETH_QOS_PBL_16;
-	desc_data->rx_outer_vlan_strip = DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS;
-	desc_data->rx_inner_vlan_strip = DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS;
+   desc_data->rx_threshold_val = DWC_ETH_QOS_RX_THRESHOLD_64;
+   desc_data->rsf_on = DWC_ETH_QOS_RSF_DISABLE;
+   desc_data->rx_pbl = DWC_ETH_QOS_PBL_16;
+   desc_data->rx_outer_vlan_strip = DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS;
+   desc_data->rx_inner_vlan_strip = DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS;
 
-	DBGPR("<--DWC_ETH_QOS_default_rx_confs_single_q\n");
+   DBGPR("<--DWC_ETH_QOS_default_rx_confs_single_q\n");
 }
 
 static void DWC_ETH_QOS_default_tx_confs(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	uint32_t qInx;
+   uint32_t qInx;
 
-	DBGPR("-->DWC_ETH_QOS_default_tx_confs\n");
+   DBGPR("-->DWC_ETH_QOS_default_tx_confs\n");
 
-	for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
-		DWC_ETH_QOS_default_tx_confs_single_q(pdata, qInx);
-	}
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
+      DWC_ETH_QOS_default_tx_confs_single_q(pdata, qInx);
+   }
 
-	DBGPR("<--DWC_ETH_QOS_default_tx_confs\n");
+   DBGPR("<--DWC_ETH_QOS_default_tx_confs\n");
 }
 
 static void DWC_ETH_QOS_default_rx_confs(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	uint32_t qInx;
+   uint32_t qInx;
 
-	DBGPR("-->DWC_ETH_QOS_default_rx_confs\n");
+   DBGPR("-->DWC_ETH_QOS_default_rx_confs\n");
 
-	for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
-		DWC_ETH_QOS_default_rx_confs_single_q(pdata, qInx);
-	}
+   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
+      DWC_ETH_QOS_default_rx_confs_single_q(pdata, qInx);
+   }
 
-	DBGPR("<--DWC_ETH_QOS_default_rx_confs\n");
+   DBGPR("<--DWC_ETH_QOS_default_rx_confs\n");
 }
 
 enum hrtimer_restart rx_itr_timeout(struct hrtimer *timer)
 {
-	struct DWC_ETH_QOS_prv_data *pdata;
-	pdata = container_of(timer, struct DWC_ETH_QOS_prv_data, rx_itr_timer);
-	DBGPR("-->rx_itr_timeout\n");
-	/* Schedule TX NAPI to start polling */
-	if (likely(napi_schedule_prep(&pdata->rx_napi))) {
-		__napi_schedule(&pdata->rx_napi);
-	}
-	DBGPR("<--rx_itr_timeout\n");
-	return HRTIMER_NORESTART;
+   struct DWC_ETH_QOS_prv_data *pdata;
+   pdata = container_of(timer, struct DWC_ETH_QOS_prv_data, rx_itr_timer);
+   DBGPR("-->rx_itr_timeout\n");
+   /* Schedule TX NAPI to start polling */
+   if (likely(napi_schedule_prep(&pdata->rx_napi))) {
+      __napi_schedule(&pdata->rx_napi);
+   }
+   DBGPR("<--rx_itr_timeout\n");
+   return HRTIMER_NORESTART;
 }
 
 #ifdef GBE_POLLING
 enum hrtimer_restart gbe_timeout(struct hrtimer *timer)
 {
-	struct DWC_ETH_QOS_prv_data *pdata;
-	pdata = container_of(timer, struct DWC_ETH_QOS_prv_data, gbe_timer);
-	DBGPR("-->gbe_timeout irq(%d)\n", pdata->irq_number);
-	disable_irq(pdata->irq_number);
-	DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS(pdata->irq_number, pdata);
-	hrtimer_start(&pdata->gbe_timer, ktime_set(0, 100000), HRTIMER_MODE_REL);
-	enable_irq(pdata->irq_number);
-	DBGPR("<--gbe_timeout\n");
-	return HRTIMER_NORESTART;
+   struct DWC_ETH_QOS_prv_data *pdata;
+   pdata = container_of(timer, struct DWC_ETH_QOS_prv_data, gbe_timer);
+   DBGPR("-->gbe_timeout irq(%d)\n", pdata->irq_number);
+   disable_irq(pdata->irq_number);
+   DWC_ETH_QOS_ISR(pdata->irq_number, pdata);
+   hrtimer_start(&pdata->gbe_timer, ktime_set(0, 100000), HRTIMER_MODE_REL);
+   enable_irq(pdata->irq_number);
+   DBGPR("<--gbe_timeout\n");
+   return HRTIMER_NORESTART;
 }
 #endif
 
@@ -1438,110 +1448,116 @@ enum hrtimer_restart gbe_timeout(struct
 *
 * \retval 0 on success & negative number on failure.
 */
-
 static int DWC_ETH_QOS_open(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	int ret = Y_SUCCESS;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
-	struct desc_if_struct *desc_if = &pdata->desc_if;
-
-	DBGPR("-->DWC_ETH_QOS_open (%d)\n", dev->irq);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   int ret = Y_SUCCESS;
+   hw_interface_t *hw_if = &pdata->hw_if;
+   struct desc_if_struct *desc_if = &pdata->desc_if;
+
+   CFG_PRINT("-->DWC_ETH_QOS_open (%d)\n", dev->irq);
+
+   if (pdata->power_state & DWC_ETH_QOS_NETIP_WAKEUP) {
+      WRN_PRINT("Atom GMAC is powered down!\n");
+      ret = -EBUSY;
+      goto err_irq_0;
+   }
 
-	pdata->irq_number = dev->irq;
+   pdata->irq_number = dev->irq;
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	ret = request_irq(pdata->irq_number, DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS_pg,
-			  IRQF_SHARED, DEV_NAME, pdata);
+   ret = request_irq(pdata->irq_number, DWC_ETH_QOS_ISR_pg,
+           IRQF_SHARED, DEV_NAME, pdata);
 #else
-	ret = request_irq(pdata->irq_number, DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS,
-			  0, DEV_NAME, pdata);
+   ret = request_irq(pdata->irq_number, DWC_ETH_QOS_ISR,
+           0, DEV_NAME, pdata);
 #endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
-	if (ret != 0) {
-		printk(KERN_ALERT "Unable to register IRQ %d\n",
-		       pdata->irq_number);
-		ret = -EBUSY;
-		goto err_irq_0;
-	}
-
-	ret = desc_if->alloc_buff_and_desc(pdata);
-	if (ret < 0) {
-		printk(KERN_ALERT
-		       "failed to allocate buffer/descriptor memory\n");
-		ret = -ENOMEM;
-		goto err_out_desc_buf_alloc_failed;
-	}
+   if (ret != 0) {
+      printk(KERN_ALERT "Unable to register IRQ %d\n", pdata->irq_number);
+      ret = -EBUSY;
+      goto err_irq_0;
+   }
 
-	/* default configuration */
+   ret = desc_if->alloc_buff_and_desc(pdata);
+   if (ret < 0) {
+      printk(KERN_ALERT
+             "failed to allocate buffer/descriptor memory\n");
+      ret = -ENOMEM;
+      goto err_out_desc_buf_alloc_failed;
+   }
+
+   /* default configuration */
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	DWC_ETH_QOS_default_confs(pdata);
+   DWC_ETH_QOS_default_confs(pdata);
 #else
-	DWC_ETH_QOS_default_common_confs(pdata);
-	DWC_ETH_QOS_default_tx_confs(pdata);
-	DWC_ETH_QOS_default_rx_confs(pdata);
-	DWC_ETH_QOS_configure_rx_fun_ptr(pdata);
+   DWC_ETH_QOS_default_common_confs(pdata);
+   DWC_ETH_QOS_default_tx_confs(pdata);
+   DWC_ETH_QOS_default_rx_confs(pdata);
+   DWC_ETH_QOS_configure_rx_fun_ptr(pdata);
 
-	DWC_ETH_QOS_napi_enable(pdata);
+   DWC_ETH_QOS_napi_enable(pdata);
 #endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
 
-	DWC_ETH_QOS_set_rx_mode(dev);
-	desc_if->wrapper_tx_desc_init(pdata);
-	desc_if->wrapper_rx_desc_init(pdata);
+   DWC_ETH_QOS_set_rx_mode(dev);
+   desc_if->wrapper_tx_desc_init(pdata);
+   desc_if->wrapper_rx_desc_init(pdata);
 
 #ifdef YDEBUG
-	DWC_ETH_QOS_tx_desc_mang_ds_dump(pdata);
-	DWC_ETH_QOS_rx_desc_mang_ds_dump(pdata);
+   DWC_ETH_QOS_tx_desc_mang_ds_dump(pdata);
+   DWC_ETH_QOS_rx_desc_mang_ds_dump(pdata);
 #endif
 
-	DWC_ETH_QOS_mmc_setup(pdata);
+   DWC_ETH_QOS_mmc_setup(pdata);
 
-	/* initializes MAC and DMA */
-	hw_if->init(pdata);
+   /* initializes MAC and DMA */
+   hw_if->init(pdata);
 
-	if (pdata->hw_feat.pcs_sel)
-		hw_if->control_an(1, 0);
+   if (pdata->hw_feat.pcs_sel)
+      hw_if->control_an(1, 0);
 
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	hw_if->prepare_dev_pktgen(pdata);
+   hw_if->prepare_dev_pktgen(pdata);
 #endif
 
-	if (pdata->phydev) {
-		phy_start(pdata->phydev);
-		pdata->eee_enabled = DWC_ETH_QOS_eee_init(pdata);
-	} else
-		pdata->eee_enabled = false;
+   if (pdata->phydev) {
+      phy_start(pdata->phydev);
+      pdata->eee_enabled = DWC_ETH_QOS_eee_init(pdata);
+   } else
+      pdata->eee_enabled = false;
 
 #ifndef DWC_ETH_QOS_CONFIG_PGTEST
-	if (pdata->mux_cfg == GMCR_GMAC5_TO_GMAC4) {
-		hw_if->set_full_duplex();
-		hw_if->set_speed(pdata, 5000);
-		netif_tx_start_all_queues(dev);
-	} else if (pdata->phydev) {
-		netif_tx_start_all_queues(dev);
-	}
-	hrtimer_init(&pdata->rx_itr_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-	pdata->rx_itr_timer.function = rx_itr_timeout;
+   if (pdata->mux_cfg == GMCR_GMAC5_TO_GMAC4) {
+      hw_if->set_full_duplex();
+      hw_if->set_speed(pdata, 5000);
+      netif_tx_start_all_queues(dev);
+   } else if (pdata->phydev) {
+      netif_tx_start_all_queues(dev);
+   }
+   hrtimer_init(&pdata->rx_itr_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+   pdata->rx_itr_timer.function = rx_itr_timeout;
+
 #ifdef GBE_POLLING
-	hrtimer_init(&pdata->gbe_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-	pdata->gbe_timer.function = gbe_timeout;
-	hrtimer_start(&pdata->gbe_timer, ktime_set(0, 10000), HRTIMER_MODE_REL);
+   hrtimer_init(&pdata->gbe_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+   pdata->gbe_timer.function = gbe_timeout;
+   hrtimer_start(&pdata->gbe_timer, ktime_set(0, 10000), HRTIMER_MODE_REL);
 #else
-	netss_interrupt_enable(NETSS_INTERUPT_GBE);
+   netss_interrupt_enable(NETSS_INTERUPT_GBE);
 #endif
+
 #else
-	netif_tx_disable(dev);
+   netif_tx_disable(dev);
 #endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
 
-	DBGPR("<--DWC_ETH_QOS_open\n");
+   CFG_PRINT("<--DWC_ETH_QOS_open\n");
 
-	return ret;
+   return ret;
 
  err_out_desc_buf_alloc_failed:
-	free_irq(pdata->irq_number, pdata);
-	pdata->irq_number = 0;
+   free_irq(pdata->irq_number, pdata);
+   pdata->irq_number = 0;
 
  err_irq_0:
-	DBGPR("<--DWC_ETH_QOS_open\n");
-	return ret;
+   CFG_PRINT("<--DWC_ETH_QOS_open\n");
+   return ret;
 }
 
 /*!
@@ -1556,48 +1572,60 @@ static int DWC_ETH_QOS_open(struct net_d
 *
 * \retval 0 on success & negative number on failure.
 */
-
 static int DWC_ETH_QOS_close(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &pdata->hw_if;
-	struct desc_if_struct *desc_if = &pdata->desc_if;
-
-	DBGPR("-->DWC_ETH_QOS_close\n");
-	netss_interrupt_disable(NETSS_INTERUPT_GBE);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &pdata->hw_if;
+   struct desc_if_struct *desc_if = &pdata->desc_if;
+
+   CFG_PRINT("-->DWC_ETH_QOS_close\n");
+
+   if (pdata->power_state & DWC_ETH_QOS_NETIP_WAKEUP) {
+      WRN_PRINT("Atom GMAC is powered down!\n");
+      /* Attach net device before stop so it can be
+         restarted when power is up */
+      netif_device_attach(dev);
+   } else {
+      netss_interrupt_disable(NETSS_INTERUPT_GBE);
 #ifdef GBE_POLLING
-	hrtimer_cancel(&pdata->gbe_timer);
+      hrtimer_cancel(&pdata->gbe_timer);
 #endif
 
-	if (pdata->eee_enabled)
-		del_timer_sync(&pdata->eee_ctrl_timer);
-
-	if (pdata->phydev)
-		phy_stop(pdata->phydev);
+      if (pdata->phydev)
+         phy_stop(pdata->phydev);
 
 #ifndef DWC_ETH_QOS_CONFIG_PGTEST
-	netif_tx_disable(dev);
-	DWC_ETH_QOS_napi_disable(pdata);
-#endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
+      netif_tx_disable(dev);
+      DWC_ETH_QOS_napi_disable(pdata);
+#endif
+
+      /* Stop DMA TX/RX */
+      DWC_ETH_QOS_stop_all_ch_tx_dma(pdata);
+      DWC_ETH_QOS_stop_all_ch_rx_dma(pdata);
+
+      /* issue software reset to device */
+      hw_if->sw_reset();
+   }
+
+   if (pdata->eee_enabled)
+      del_timer_sync(&pdata->eee_ctrl_timer);
+
+   desc_if->tx_free_mem(pdata);
+   desc_if->rx_free_mem(pdata);
+   if (pdata->irq_number != 0) {
+      free_irq(pdata->irq_number, pdata);
+      pdata->irq_number = 0;
+   }
 
-	/* issue software reset to device */
-	hw_if->exit();
-	desc_if->tx_free_mem(pdata);
-	desc_if->rx_free_mem(pdata);
-	if (pdata->irq_number != 0) {
-		free_irq(pdata->irq_number, pdata);
-		pdata->irq_number = 0;
-	}
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	del_timer(&pdata->pg_timer);
-#endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
+   del_timer(&pdata->pg_timer);
+#endif
 
-	DBGPR("<--DWC_ETH_QOS_close\n");
+   CFG_PRINT("<--DWC_ETH_QOS_close\n");
 
-	return Y_SUCCESS;
+   return Y_SUCCESS;
 }
 
-
 /*!
 * \brief API to configure the multicast address in device.
 *
@@ -1611,98 +1639,98 @@ static int DWC_ETH_QOS_close(struct net_
 */
 static int DWC_ETH_QOS_prepare_mc_list(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t mc_filter[DWC_ETH_QOS_HTR_CNT];
-	struct netdev_hw_addr *ha = NULL;
-	int crc32_val = 0;
-	int ret = 0, i = 1;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_prepare_mc_list\n");
-
-	if (pdata->l2_filtering_mode) {
-		DBGPR_FILTER("select HASH FILTERING for mc addresses: mc_count = %d\n",
-				netdev_mc_count(dev));
-		ret = 1;
-		memset(mc_filter, 0, sizeof(mc_filter));
-
-		if (pdata->max_hash_table_size == 64) {
-			netdev_for_each_mc_addr(ha, dev) {
-				DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
-						ha->addr[0], ha->addr[1], ha->addr[2],
-						ha->addr[3], ha->addr[4], ha->addr[5]);
-				/* The upper 6 bits of the calculated CRC are used to
-				 * index the content of the Hash Table Reg 0 and 1.
-				 * */
-				crc32_val =
-					(bitrev32(~crc32_le(~0, ha->addr, 6)) >> 26);
-				/* The most significant bit determines the register
-				 * to use (Hash Table Reg X, X = 0 and 1) while the
-				 * other 5(0x1F) bits determines the bit within the
-				 * selected register
-				 * */
-				mc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-			}
-		} else if (pdata->max_hash_table_size == 128) {
-			netdev_for_each_mc_addr(ha, dev) {
-				DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
-						ha->addr[0], ha->addr[1], ha->addr[2],
-						ha->addr[3], ha->addr[4], ha->addr[5]);
-				/* The upper 7 bits of the calculated CRC are used to
-				 * index the content of the Hash Table Reg 0,1,2 and 3.
-				 * */
-				crc32_val =
-					(bitrev32(~crc32_le(~0, ha->addr, 6)) >> 25);
-
-				printk(KERN_ALERT "crc_le = %#x, crc_be = %#x\n",
-						bitrev32(~crc32_le(~0, ha->addr, 6)),
-						bitrev32(~crc32_be(~0, ha->addr, 6)));
-
-				/* The most significant 2 bits determines the register
-				 * to use (Hash Table Reg X, X = 0,1,2 and 3) while the
-				 * other 5(0x1F) bits determines the bit within the
-				 * selected register
-				 * */
-				mc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-			}
-		} else if (pdata->max_hash_table_size == 256) {
-			netdev_for_each_mc_addr(ha, dev) {
-				DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
-						ha->addr[0], ha->addr[1], ha->addr[2],
-						ha->addr[3], ha->addr[4], ha->addr[5]);
-				/* The upper 8 bits of the calculated CRC are used to
-				 * index the content of the Hash Table Reg 0,1,2,3,4,
-				 * 5,6, and 7.
-				 * */
-				crc32_val =
-					(bitrev32(~crc32_le(~0, ha->addr, 6)) >> 24);
-				/* The most significant 3 bits determines the register
-				 * to use (Hash Table Reg X, X = 0,1,2,3,4,5,6 and 7) while
-				 * the other 5(0x1F) bits determines the bit within the
-				 * selected register
-				 * */
-				mc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-			}
-		}
-
-		for (i = 0; i < DWC_ETH_QOS_HTR_CNT; i++)
-			hw_if->update_hash_table_reg(i, mc_filter[i]);
-
-	} else {
-		DBGPR_FILTER("select PERFECT FILTERING for mc addresses, mc_count = %d, max_addr_reg_cnt = %d\n",
-				netdev_mc_count(dev), pdata->max_addr_reg_cnt);
-		i  = 0;
-		netdev_for_each_mc_addr(ha, dev) {
-			DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n", i + 1,
-					ha->addr[0], ha->addr[1], ha->addr[2],
-					ha->addr[3], ha->addr[4], ha->addr[5]);
-			hw_if->update_mac_addr(i++, ha->addr);
-		}
-	}
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t mc_filter[DWC_ETH_QOS_HTR_CNT];
+   struct netdev_hw_addr *ha = NULL;
+   int crc32_val = 0;
+   int ret = 0, i = 1;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_prepare_mc_list\n");
+
+   if (pdata->l2_filtering_mode) {
+      DBGPR_FILTER("select HASH FILTERING for mc addresses: mc_count = %d\n",
+            netdev_mc_count(dev));
+      ret = 1;
+      memset(mc_filter, 0, sizeof(mc_filter));
+
+      if (pdata->max_hash_table_size == 64) {
+         netdev_for_each_mc_addr(ha, dev) {
+            DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
+                  ha->addr[0], ha->addr[1], ha->addr[2],
+                  ha->addr[3], ha->addr[4], ha->addr[5]);
+            /* The upper 6 bits of the calculated CRC are used to
+             * index the content of the Hash Table Reg 0 and 1.
+             * */
+            crc32_val =
+               (bitrev32(~crc32_le(~0, ha->addr, 6)) >> 26);
+            /* The most significant bit determines the register
+             * to use (Hash Table Reg X, X = 0 and 1) while the
+             * other 5(0x1F) bits determines the bit within the
+             * selected register
+             * */
+            mc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+         }
+      } else if (pdata->max_hash_table_size == 128) {
+         netdev_for_each_mc_addr(ha, dev) {
+            DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
+                  ha->addr[0], ha->addr[1], ha->addr[2],
+                  ha->addr[3], ha->addr[4], ha->addr[5]);
+            /* The upper 7 bits of the calculated CRC are used to
+             * index the content of the Hash Table Reg 0,1,2 and 3.
+             * */
+            crc32_val =
+               (bitrev32(~crc32_le(~0, ha->addr, 6)) >> 25);
+
+            printk(KERN_ALERT "crc_le = %#x, crc_be = %#x\n",
+                  bitrev32(~crc32_le(~0, ha->addr, 6)),
+                  bitrev32(~crc32_be(~0, ha->addr, 6)));
+
+            /* The most significant 2 bits determines the register
+             * to use (Hash Table Reg X, X = 0,1,2 and 3) while the
+             * other 5(0x1F) bits determines the bit within the
+             * selected register
+             * */
+            mc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+         }
+      } else if (pdata->max_hash_table_size == 256) {
+         netdev_for_each_mc_addr(ha, dev) {
+            DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
+                  ha->addr[0], ha->addr[1], ha->addr[2],
+                  ha->addr[3], ha->addr[4], ha->addr[5]);
+            /* The upper 8 bits of the calculated CRC are used to
+             * index the content of the Hash Table Reg 0,1,2,3,4,
+             * 5,6, and 7.
+             * */
+            crc32_val =
+               (bitrev32(~crc32_le(~0, ha->addr, 6)) >> 24);
+            /* The most significant 3 bits determines the register
+             * to use (Hash Table Reg X, X = 0,1,2,3,4,5,6 and 7) while
+             * the other 5(0x1F) bits determines the bit within the
+             * selected register
+             * */
+            mc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+         }
+      }
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_prepare_mc_list\n");
+      for (i = 0; i < DWC_ETH_QOS_HTR_CNT; i++)
+         hw_if->update_hash_table_reg(i, mc_filter[i]);
 
-	return ret;
+   } else {
+      DBGPR_FILTER("select PERFECT FILTERING for mc addresses, mc_count = %d, max_addr_reg_cnt = %d\n",
+            netdev_mc_count(dev), pdata->max_addr_reg_cnt);
+      i  = 0;
+      netdev_for_each_mc_addr(ha, dev) {
+         DBGPR_FILTER("mc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n", i + 1,
+               ha->addr[0], ha->addr[1], ha->addr[2],
+               ha->addr[3], ha->addr[4], ha->addr[5]);
+         hw_if->update_mac_addr(i++, ha->addr);
+      }
+   }
+
+   DBGPR_FILTER("<--DWC_ETH_QOS_prepare_mc_list\n");
+
+   return ret;
 }
 
 /*!
@@ -1718,88 +1746,88 @@ static int DWC_ETH_QOS_prepare_mc_list(s
 */
 static int DWC_ETH_QOS_prepare_uc_list(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t uc_filter[DWC_ETH_QOS_HTR_CNT];
-	struct netdev_hw_addr *ha = NULL;
-	int crc32_val = 0;
-	int ret = 0, i = 1;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_prepare_uc_list\n");
-
-	if (pdata->l2_filtering_mode) {
-		DBGPR_FILTER("select HASH FILTERING for uc addresses: uc_count = %d\n",
-				netdev_uc_count(dev));
-		ret = 1;
-		memset(uc_filter, 0, sizeof(uc_filter));
-
-		if (pdata->max_hash_table_size == 64) {
-			netdev_for_each_uc_addr(ha, dev) {
-				DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
-						ha->addr[0], ha->addr[1], ha->addr[2],
-						ha->addr[3], ha->addr[4], ha->addr[5]);
-				crc32_val =
-					(bitrev32(~crc32_le(~0, ha->addr, 6)) >> 26);
-				uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-			}
-		} else if (pdata->max_hash_table_size == 128) {
-			netdev_for_each_uc_addr(ha, dev) {
-				DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
-						ha->addr[0], ha->addr[1], ha->addr[2],
-						ha->addr[3], ha->addr[4], ha->addr[5]);
-				crc32_val =
-					(bitrev32(~crc32_le(~0, ha->addr, 6)) >> 25);
-				uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-			}
-		} else if (pdata->max_hash_table_size == 256) {
-			netdev_for_each_uc_addr(ha, dev) {
-				DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
-						ha->addr[0], ha->addr[1], ha->addr[2],
-						ha->addr[3], ha->addr[4], ha->addr[5]);
-				crc32_val =
-					(bitrev32(~crc32_le(~0, ha->addr, 6)) >> 24);
-				uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-			}
-		}
-
-		/* configure hash value of real/default interface also */
-		DBGPR_FILTER("real/default dev_addr = %#x:%#x:%#x:%#x:%#x:%#x\n",
-				dev->dev_addr[0], dev->dev_addr[1], dev->dev_addr[2],
-				dev->dev_addr[3], dev->dev_addr[4], dev->dev_addr[5]);
-
-		if (pdata->max_hash_table_size == 64) {
-			crc32_val =
-				(bitrev32(~crc32_le(~0, dev->dev_addr, 6)) >> 26);
-			uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-		} else if (pdata->max_hash_table_size == 128) {
-			crc32_val =
-				(bitrev32(~crc32_le(~0, dev->dev_addr, 6)) >> 25);
-			uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-
-		} else if (pdata->max_hash_table_size == 256) {
-			crc32_val =
-				(bitrev32(~crc32_le(~0, dev->dev_addr, 6)) >> 24);
-			uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
-		}
-
-		for (i = 0; i < DWC_ETH_QOS_HTR_CNT; i++)
-			hw_if->update_hash_table_reg(i, uc_filter[i]);
-
-	} else {
-		DBGPR_FILTER("select PERFECT FILTERING for uc addresses: uc_count = %d\n",
-				netdev_uc_count(dev));
-		i = 0;
-		netdev_for_each_uc_addr(ha, dev) {
-			DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n", i,
-					ha->addr[0], ha->addr[1], ha->addr[2],
-					ha->addr[3], ha->addr[4], ha->addr[5]);
-			hw_if->update_mac_addr(i++, ha->addr);
-		}
-	}
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t uc_filter[DWC_ETH_QOS_HTR_CNT];
+   struct netdev_hw_addr *ha = NULL;
+   int crc32_val = 0;
+   int ret = 0, i = 1;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_prepare_uc_list\n");
+
+   if (pdata->l2_filtering_mode) {
+      DBGPR_FILTER("select HASH FILTERING for uc addresses: uc_count = %d\n",
+            netdev_uc_count(dev));
+      ret = 1;
+      memset(uc_filter, 0, sizeof(uc_filter));
+
+      if (pdata->max_hash_table_size == 64) {
+         netdev_for_each_uc_addr(ha, dev) {
+            DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
+                  ha->addr[0], ha->addr[1], ha->addr[2],
+                  ha->addr[3], ha->addr[4], ha->addr[5]);
+            crc32_val =
+               (bitrev32(~crc32_le(~0, ha->addr, 6)) >> 26);
+            uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+         }
+      } else if (pdata->max_hash_table_size == 128) {
+         netdev_for_each_uc_addr(ha, dev) {
+            DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
+                  ha->addr[0], ha->addr[1], ha->addr[2],
+                  ha->addr[3], ha->addr[4], ha->addr[5]);
+            crc32_val =
+               (bitrev32(~crc32_le(~0, ha->addr, 6)) >> 25);
+            uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+         }
+      } else if (pdata->max_hash_table_size == 256) {
+         netdev_for_each_uc_addr(ha, dev) {
+            DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n",i++,
+                  ha->addr[0], ha->addr[1], ha->addr[2],
+                  ha->addr[3], ha->addr[4], ha->addr[5]);
+            crc32_val =
+               (bitrev32(~crc32_le(~0, ha->addr, 6)) >> 24);
+            uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+         }
+      }
+
+      /* configure hash value of real/default interface also */
+      DBGPR_FILTER("real/default dev_addr = %#x:%#x:%#x:%#x:%#x:%#x\n",
+            dev->dev_addr[0], dev->dev_addr[1], dev->dev_addr[2],
+            dev->dev_addr[3], dev->dev_addr[4], dev->dev_addr[5]);
+
+      if (pdata->max_hash_table_size == 64) {
+         crc32_val =
+            (bitrev32(~crc32_le(~0, dev->dev_addr, 6)) >> 26);
+         uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+      } else if (pdata->max_hash_table_size == 128) {
+         crc32_val =
+            (bitrev32(~crc32_le(~0, dev->dev_addr, 6)) >> 25);
+         uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+
+      } else if (pdata->max_hash_table_size == 256) {
+         crc32_val =
+            (bitrev32(~crc32_le(~0, dev->dev_addr, 6)) >> 24);
+         uc_filter[crc32_val >> 5] |= (1 << (crc32_val & 0x1F));
+      }
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_prepare_uc_list\n");
+      for (i = 0; i < DWC_ETH_QOS_HTR_CNT; i++)
+         hw_if->update_hash_table_reg(i, uc_filter[i]);
 
-	return ret;
+   } else {
+      DBGPR_FILTER("select PERFECT FILTERING for uc addresses: uc_count = %d\n",
+            netdev_uc_count(dev));
+      i = 0;
+      netdev_for_each_uc_addr(ha, dev) {
+         DBGPR_FILTER("uc addr[%d] = %#x:%#x:%#x:%#x:%#x:%#x\n", i,
+               ha->addr[0], ha->addr[1], ha->addr[2],
+               ha->addr[3], ha->addr[4], ha->addr[5]);
+         hw_if->update_mac_addr(i++, ha->addr);
+      }
+   }
+
+   DBGPR_FILTER("<--DWC_ETH_QOS_prepare_uc_list\n");
+
+   return ret;
 }
 
 /*!
@@ -1814,79 +1842,79 @@ static int DWC_ETH_QOS_prepare_uc_list(s
 */
 static void DWC_ETH_QOS_set_rx_mode(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	unsigned long flags;
-	unsigned char pr_mode = 0;
-	unsigned char huc_mode = 0;
-	unsigned char hmc_mode = 0;
-	unsigned char pm_mode = 0;
-	unsigned char hpf_mode = 0;
-	int mode, i;
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   unsigned long flags;
+   unsigned char pr_mode = 0;
+   unsigned char huc_mode = 0;
+   unsigned char hmc_mode = 0;
+   unsigned char pm_mode = 0;
+   unsigned char hpf_mode = 0;
+   int mode, i;
 
-	DBGPR_FILTER("-->DWC_ETH_QOS_set_rx_mode\n");
+   DBGPR_FILTER("-->DWC_ETH_QOS_set_rx_mode\n");
 
-	spin_lock_irqsave(&pdata->lock, flags);
+   spin_lock_irqsave(&pdata->lock, flags);
 
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	DBGPR("PG Test running, no parameters will be changed\n");
-	spin_unlock_irqrestore(&pdata->lock, flags);
-	return;
-#endif
-
-	if (dev->flags & IFF_PROMISC) {
-		DBGPR_FILTER("PROMISCUOUS MODE (Accept all packets irrespective of DA)\n");
-		pr_mode = 1;
-	} else if ((dev->flags & IFF_ALLMULTI) ||
-			(netdev_mc_count(dev) > (pdata->max_hash_table_size))) {
-		DBGPR_FILTER("pass all multicast pkt\n");
-		pm_mode = 1;
-		if (pdata->max_hash_table_size) {
-			for (i = 0; i < DWC_ETH_QOS_HTR_CNT; i++)
-				hw_if->update_hash_table_reg(i, 0xffffffff);
-		}
-	} else if (!netdev_mc_empty(dev)) {
-		DBGPR_FILTER("pass list of multicast pkt\n");
-		if ((netdev_mc_count(dev) > (pdata->max_addr_reg_cnt - 1)) &&
-			(!pdata->max_hash_table_size)) {
-			/* switch to PROMISCUOUS mode */
-			pr_mode = 1;
-		} else {
-			mode = DWC_ETH_QOS_prepare_mc_list(dev);
-			if (mode) {
-				/* Hash filtering for multicast */
-				hmc_mode = 1;
-			} else {
-				/* Perfect filtering for multicast */
-				hmc_mode = 0;
-				hpf_mode = 1;
-			}
-		}
-	}
-
-	/* Handle multiple unicast addresses */
-	if ((netdev_uc_count(dev) > (pdata->max_addr_reg_cnt - 1)) &&
-			(!pdata->max_hash_table_size)) {
-		/* switch to PROMISCUOUS mode */
-		pr_mode = 1;
-	} else if (!netdev_uc_empty(dev)) {
-		mode = DWC_ETH_QOS_prepare_uc_list(dev);
-		if (mode) {
-			/* Hash filtering for unicast */
-			huc_mode = 1;
-		} else {
-			/* Perfect filtering for unicast */
-			huc_mode = 0;
-			hpf_mode = 1;
-		}
-	}
+   DBGPR("PG Test running, no parameters will be changed\n");
+   spin_unlock_irqrestore(&pdata->lock, flags);
+   return;
+#endif
 
-	hw_if->config_mac_pkt_filter_reg(pr_mode, huc_mode,
-		hmc_mode, pm_mode, hpf_mode);
+   if (dev->flags & IFF_PROMISC) {
+      DBGPR_FILTER("PROMISCUOUS MODE (Accept all packets irrespective of DA)\n");
+      pr_mode = 1;
+   } else if ((dev->flags & IFF_ALLMULTI) ||
+         (netdev_mc_count(dev) > (pdata->max_hash_table_size))) {
+      DBGPR_FILTER("pass all multicast pkt\n");
+      pm_mode = 1;
+      if (pdata->max_hash_table_size) {
+         for (i = 0; i < DWC_ETH_QOS_HTR_CNT; i++)
+            hw_if->update_hash_table_reg(i, 0xffffffff);
+      }
+   } else if (!netdev_mc_empty(dev)) {
+      DBGPR_FILTER("pass list of multicast pkt\n");
+      if ((netdev_mc_count(dev) > (pdata->max_addr_reg_cnt - 1)) &&
+         (!pdata->max_hash_table_size)) {
+         /* switch to PROMISCUOUS mode */
+         pr_mode = 1;
+      } else {
+         mode = DWC_ETH_QOS_prepare_mc_list(dev);
+         if (mode) {
+            /* Hash filtering for multicast */
+            hmc_mode = 1;
+         } else {
+            /* Perfect filtering for multicast */
+            hmc_mode = 0;
+            hpf_mode = 1;
+         }
+      }
+   }
+
+   /* Handle multiple unicast addresses */
+   if ((netdev_uc_count(dev) > (pdata->max_addr_reg_cnt - 1)) &&
+         (!pdata->max_hash_table_size)) {
+      /* switch to PROMISCUOUS mode */
+      pr_mode = 1;
+   } else if (!netdev_uc_empty(dev)) {
+      mode = DWC_ETH_QOS_prepare_uc_list(dev);
+      if (mode) {
+         /* Hash filtering for unicast */
+         huc_mode = 1;
+      } else {
+         /* Perfect filtering for unicast */
+         huc_mode = 0;
+         hpf_mode = 1;
+      }
+   }
+
+   hw_if->config_mac_pkt_filter_reg(pr_mode, huc_mode,
+      hmc_mode, pm_mode, hpf_mode);
 
-	spin_unlock_irqrestore(&pdata->lock, flags);
+   spin_unlock_irqrestore(&pdata->lock, flags);
 
-	DBGPR("<--DWC_ETH_QOS_set_rx_mode\n");
+   DBGPR("<--DWC_ETH_QOS_set_rx_mode\n");
 }
 
 /*!
@@ -1903,64 +1931,63 @@ static void DWC_ETH_QOS_set_rx_mode(stru
 *
 * \retval number of descriptor required.
 */
-
 uint32_t DWC_ETH_QOS_get_total_desc_cnt(struct DWC_ETH_QOS_prv_data *pdata,
-		struct sk_buff *skb, uint32_t qInx)
+      struct sk_buff *skb, uint32_t qInx)
 {
-	uint32_t count = 0, size = 0;
-	int length = 0;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
-	tx_pkt_features_t *tx_pkt_features = &pdata->tx_pkt_features;
-	struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
-	    GET_TX_WRAPPER_DESC(qInx);
-
-	/* SG fragment count */
-	count += skb_shinfo(skb)->nr_frags;
-
-	/* descriptors required based on data limit per descriptor */
-	length = (skb->len - skb->data_len);
-	while (length) {
-		size = min(length, DWC_ETH_QOS_MAX_DATA_PER_TXD);
-		count++;
-		length = length - size;
-	}
-
-	/* we need one context descriptor to carry tso details */
-	if (skb_shinfo(skb)->gso_size != 0)
-		count++;
+   uint32_t count = 0, size = 0;
+   int length = 0;
+   hw_interface_t *hw_if = &pdata->hw_if;
+   tx_pkt_features_t *tx_pkt_features = &pdata->tx_pkt_features;
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
+       GET_TX_WRAPPER_DESC(qInx);
+
+   /* SG fragment count */
+   count += skb_shinfo(skb)->nr_frags;
+
+   /* descriptors required based on data limit per descriptor */
+   length = (skb->len - skb->data_len);
+   while (length) {
+      size = min(length, DWC_ETH_QOS_MAX_DATA_PER_TXD);
+      count++;
+      length = length - size;
+   }
+
+   /* we need one context descriptor to carry tso details */
+   if (skb_shinfo(skb)->gso_size != 0)
+      count++;
 
 #ifdef DWC_ETH_QOS_ENABLE_VLAN_TAG
-	desc_data->vlan_tag_present = 0;
-	if (vlan_tx_tag_present(skb)) {
-		uint16_t vlan_tag = vlan_tx_tag_get(skb);
-		vlan_tag |= (qInx << 13);
-		desc_data->vlan_tag_present = 1;
-		if (vlan_tag != desc_data->vlan_tag_id ||
-				desc_data->context_setup == 1) {
-			desc_data->vlan_tag_id = vlan_tag;
-			if (Y_TRUE == desc_data->tx_vlan_tag_via_reg) {
-				printk(KERN_ALERT "VLAN control info update via register\n\n");
-				hw_if->enable_vlan_reg_control(desc_data);
-			} else {
-				hw_if->enable_vlan_desc_control(pdata);
-				VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
-					TX_PKT_FEATURES_ATTR_VLAN_PKT, 1);
-				tx_pkt_features->vlan_tag = vlan_tag;
-				/* we need one context descriptor to carry vlan tag info */
-				count++;
-			}
-		}
-		pdata->xstats.tx_vlan_pkt_n++;
-	}
+   desc_data->vlan_tag_present = 0;
+   if (vlan_tx_tag_present(skb)) {
+      uint16_t vlan_tag = vlan_tx_tag_get(skb);
+      vlan_tag |= (qInx << 13);
+      desc_data->vlan_tag_present = 1;
+      if (vlan_tag != desc_data->vlan_tag_id ||
+            desc_data->context_setup == 1) {
+         desc_data->vlan_tag_id = vlan_tag;
+         if (Y_TRUE == desc_data->tx_vlan_tag_via_reg) {
+            printk(KERN_ALERT "VLAN control info update via register\n\n");
+            hw_if->enable_vlan_reg_control(desc_data);
+         } else {
+            hw_if->enable_vlan_desc_control(pdata);
+            VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
+               TX_PKT_FEATURES_ATTR_VLAN_PKT, 1);
+            tx_pkt_features->vlan_tag = vlan_tag;
+            /* we need one context descriptor to carry vlan tag info */
+            count++;
+         }
+      }
+      pdata->xstats.tx_vlan_pkt_n++;
+   }
 #endif
 #ifdef DWC_ETH_QOS_ENABLE_DVLAN
-	if (pdata->via_reg_or_desc == DWC_ETH_QOS_VIA_DESC) {
-		/* we need one context descriptor to carry vlan tag info */
-		count++;
-	}
+   if (pdata->via_reg_or_desc == DWC_ETH_QOS_VIA_DESC) {
+      /* we need one context descriptor to carry vlan tag info */
+      count++;
+   }
 #endif /* End of DWC_ETH_QOS_ENABLE_DVLAN */
 
-	return count;
+   return count;
 }
 
 /*!
@@ -1979,201 +2006,201 @@ uint32_t DWC_ETH_QOS_get_total_desc_cnt(
 */
 static int DWC_ETH_QOS_start_xmit(struct sk_buff *skb, struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	uint32_t qInx = skb_get_queue_mapping(skb);
-	struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
-	    GET_TX_WRAPPER_DESC(qInx);
-	tx_pkt_features_t *tx_pkt_features = &pdata->tx_pkt_features;
-	unsigned long flags;
-	unsigned int desc_count = 0;
-	unsigned int count = 0;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
-	struct desc_if_struct *desc_if = &pdata->desc_if;
-	int retval = NETDEV_TX_OK;
-	int tso;
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   uint32_t qInx = skb_get_queue_mapping(skb);
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
+       GET_TX_WRAPPER_DESC(qInx);
+   tx_pkt_features_t *tx_pkt_features = &pdata->tx_pkt_features;
+   unsigned long flags;
+   unsigned int desc_count = 0;
+   unsigned int count = 0;
+   hw_interface_t *hw_if = &pdata->hw_if;
+   struct desc_if_struct *desc_if = &pdata->desc_if;
+   int retval = NETDEV_TX_OK;
+   int tso;
 
-	DBGPR("-->DWC_ETH_QOS_start_xmit: skb->len = %d, qInx = %u, tx_pkt_queued(%d)\n",
-		skb->len, qInx, desc_data->tx_pkt_queued);
+   DBGPR("-->DWC_ETH_QOS_start_xmit: skb->len = %d, qInx = %u, tx_pkt_queued(%d)\n",
+      skb->len, qInx, desc_data->tx_pkt_queued);
 
-	spin_lock_irqsave(&pdata->tx_lock, flags);
+   spin_lock_irqsave(&pdata->tx_lock, flags);
 
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	retval = NETDEV_TX_BUSY;
-	goto tx_netdev_return;
+   retval = NETDEV_TX_BUSY;
+   goto tx_netdev_return;
 #endif
 
-	if (skb->len <= 0) {
-		dev_kfree_skb_any(skb);
-		printk(KERN_ERR "%s : Empty skb received from stack\n",
-			dev->name);
-		goto tx_netdev_return;
-	} else if ((skb_shinfo(skb)->gso_size == 0) &&
-		(skb->len > DWC_ETH_QOS_MAX_SUPPORTED_MTU)) {
-		printk(KERN_ERR "%s : big packet = %d\n", dev->name, skb->len);
-		dev_kfree_skb_any(skb);
-		dev->stats.tx_dropped++;
-		goto tx_netdev_return;
-	}
+   if (skb->len <= 0) {
+      dev_kfree_skb_any(skb);
+      printk(KERN_ERR "%s : Empty skb received from stack\n",
+         dev->name);
+      goto tx_netdev_return;
+   } else if ((skb_shinfo(skb)->gso_size == 0) &&
+      (skb->len > DWC_ETH_QOS_MAX_SUPPORTED_MTU)) {
+      printk(KERN_ERR "%s : big packet = %d\n", dev->name, skb->len);
+      dev_kfree_skb_any(skb);
+      dev->stats.tx_dropped++;
+      goto tx_netdev_return;
+   }
 #ifdef GBE_DEBUG
-	if (metadata_on_crc) {
-		if (replace_crc(skb)) {
-			printk(KERN_ALERT "Failed replacing CRC on SKB!\n");
-			goto tx_netdev_return;
-		}
-	}
-	if (print_tx_pkts)
-		print_skb(skb, false);
-#endif
-
-	if ((pdata->eee_enabled) && (pdata->tx_path_in_lpi_mode) &&
-		(!pdata->use_lpi_tx_automate))
-		DWC_ETH_QOS_disable_eee_mode(pdata);
-
-	memset(&pdata->tx_pkt_features, 0, sizeof(pdata->tx_pkt_features));
-
-	/* check total number of desc required for current xfer */
-	desc_count = DWC_ETH_QOS_get_total_desc_cnt(pdata, skb, qInx);
-	if (desc_data->free_desc_cnt < desc_count) {
-		desc_data->queue_stopped = 1;
-		netif_stop_subqueue(dev, qInx);
-		DBGPR("Stopped TX queue(%d) no enough space in queue\n", qInx);
-		retval = NETDEV_TX_BUSY;
-		goto tx_netdev_return;
-	}
-
-	/* check for hw tstamping */
-	if (pdata->hw_feat.tsstssel && pdata->hwts_tx_en) {
-		if(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
-			/* declare that device is doing timestamping */
-			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
-			VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
-				TX_PKT_FEATURES_ATTR_PTP_ENABLE, 1);
-			DBGPR_PTP("Got PTP pkt to transmit [qInx = %d, cur_tx = %d]\n",
-				qInx, desc_data->cur_tx);
-		}
-	}
-
-	tso = desc_if->handle_tso(dev, skb);
-	if (tso < 0) {
-		printk(KERN_ALERT "Unable to handle TSO\n");
-		dev_kfree_skb_any(skb);
-		retval = NETDEV_TX_OK;
-		goto tx_netdev_return;
-	} else if (tso) {
-		pdata->xstats.tx_tso_pkt_n++;
-		VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
-			TX_PKT_FEATURES_ATTR_TSO_ENABLE, 1);
-	} else if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
-			TX_PKT_FEATURES_ATTR_CSUM_ENABLE, 1);
-	}
-
-	count = desc_if->map_tx_skb(dev, skb);
-	if (count == 0) {
-		dev_kfree_skb_any(skb);
-		retval = NETDEV_TX_OK;
-		goto tx_netdev_return;
-	}
-
-	dev->trans_start = jiffies;
-	desc_data->free_desc_cnt -= count;
-	desc_data->tx_pkt_queued += count;
-
-	/* fallback to software time stamping if core doesn't
-	 * support hardware time stamping */
-	if ((pdata->hw_feat.tsstssel == 0) || (pdata->hwts_tx_en == 0))
-		skb_tx_timestamp(skb);
+   if (metadata_on_crc) {
+      if (replace_crc(skb)) {
+         printk(KERN_ALERT "Failed replacing CRC on SKB!\n");
+         goto tx_netdev_return;
+      }
+   }
+   if (print_tx_pkts)
+      print_skb(skb, false);
+#endif
+
+   if ((pdata->eee_enabled) && (pdata->tx_path_in_lpi_mode) &&
+      (!pdata->use_lpi_tx_automate))
+      DWC_ETH_QOS_disable_eee_mode(pdata);
+
+   memset(&pdata->tx_pkt_features, 0, sizeof(pdata->tx_pkt_features));
+
+   /* check total number of desc required for current xfer */
+   desc_count = DWC_ETH_QOS_get_total_desc_cnt(pdata, skb, qInx);
+   if (desc_data->free_desc_cnt < desc_count) {
+      desc_data->queue_stopped = 1;
+      netif_stop_subqueue(dev, qInx);
+      DBGPR("Stopped TX queue(%d) no enough space in queue\n", qInx);
+      retval = NETDEV_TX_BUSY;
+      goto tx_netdev_return;
+   }
+
+   /* check for hw tstamping */
+   if (pdata->hw_feat.tsstssel && pdata->hwts_tx_en) {
+      if(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
+         /* declare that device is doing timestamping */
+         skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+         VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
+            TX_PKT_FEATURES_ATTR_PTP_ENABLE, 1);
+         DBGPR_PTP("Got PTP pkt to transmit [qInx = %d, cur_tx = %d]\n",
+            qInx, desc_data->cur_tx);
+      }
+   }
+
+   tso = desc_if->handle_tso(dev, skb);
+   if (tso < 0) {
+      printk(KERN_ALERT "Unable to handle TSO\n");
+      dev_kfree_skb_any(skb);
+      retval = NETDEV_TX_OK;
+      goto tx_netdev_return;
+   } else if (tso) {
+      pdata->xstats.tx_tso_pkt_n++;
+      VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
+         TX_PKT_FEATURES_ATTR_TSO_ENABLE, 1);
+   } else if (skb->ip_summed == CHECKSUM_PARTIAL) {
+      VAR32_SET_BIT(tx_pkt_features->pkt_attributes,
+         TX_PKT_FEATURES_ATTR_CSUM_ENABLE, 1);
+   }
+
+   count = desc_if->map_tx_skb(dev, skb);
+   if (count == 0) {
+      dev_kfree_skb_any(skb);
+      retval = NETDEV_TX_OK;
+      goto tx_netdev_return;
+   }
+
+   dev->trans_start = jiffies;
+   desc_data->free_desc_cnt -= count;
+   desc_data->tx_pkt_queued += count;
+
+   /* fallback to software time stamping if core doesn't
+    * support hardware time stamping */
+   if ((pdata->hw_feat.tsstssel == 0) || (pdata->hwts_tx_en == 0))
+      skb_tx_timestamp(skb);
 
-	/* configure required descriptor fields for transmission */
-	hw_if->pre_xmit(pdata, qInx);
+   /* configure required descriptor fields for transmission */
+   hw_if->pre_xmit(pdata, qInx);
 
 tx_netdev_return:
-	spin_unlock_irqrestore(&pdata->tx_lock, flags);
+   spin_unlock_irqrestore(&pdata->tx_lock, flags);
 
-	DBGPR("<--DWC_ETH_QOS_start_xmit\n");
+   DBGPR("<--DWC_ETH_QOS_start_xmit\n");
 
-	return retval;
+   return retval;
 }
 
 static void DWC_ETH_QOS_print_rx_tstamp_info(rx_descriptor_t *rxdesc,
-	unsigned int qInx)
+   unsigned int qInx)
 {
-	uint32_t ptp_status = 0;
-	uint32_t pkt_type = 0;
-	char *tstamp_dropped = NULL;
-	char *tstamp_available = NULL;
-	char *ptp_version = NULL;
-	char *ptp_pkt_type = NULL;
-	char *ptp_msg_type = NULL;
-
-	DBGPR_PTP("-->DWC_ETH_QOS_print_rx_tstamp_info\n");
-
-	/* status in RDES1 is not valid */
-	if (!(rxdesc->RDES3 & DWC_ETH_QOS_RDESC3_RS1V))
-		return;
-
-	ptp_status = rxdesc->RDES1;
-	tstamp_dropped = ((ptp_status & 0x8000) ? "YES" : "NO");
-	tstamp_available = ((ptp_status & 0x4000) ? "YES" : "NO");
-	ptp_version = ((ptp_status & 0x2000) ? "v2 (1588-2008)" : "v1 (1588-2002)");
-	ptp_pkt_type = ((ptp_status & 0x1000) ? "ptp over Eth" : "ptp over IPv4/6");
-
-	pkt_type = ((ptp_status & 0xF00) > 8);
-	switch (pkt_type) {
-	case 0:
-		ptp_msg_type = "NO PTP msg received";
-		break;
-	case 1:
-		ptp_msg_type = "SYNC";
-		break;
-	case 2:
-		ptp_msg_type = "Follow_Up";
-		break;
-	case 3:
-		ptp_msg_type = "Delay_Req";
-		break;
-	case 4:
-		ptp_msg_type = "Delay_Resp";
-		break;
-	case 5:
-		ptp_msg_type = "Pdelay_Req";
-		break;
-	case 6:
-		ptp_msg_type = "Pdelay_Resp";
-		break;
-	case 7:
-		ptp_msg_type = "Pdelay_Resp_Follow_up";
-		break;
-	case 8:
-		ptp_msg_type = "Announce";
-		break;
-	case 9:
-		ptp_msg_type = "Management";
-		break;
-	case 10:
-		ptp_msg_type = "Signaling";
-		break;
-	case 11:
-	case 12:
-	case 13:
-	case 14:
-		ptp_msg_type = "Reserved";
-		break;
-	case 15:
-		ptp_msg_type = "PTP pkr with Reserved Msg Type";
-		break;
-	}
-
-	DBGPR_PTP("Rx timestamp detail for queue %d\n"
-			"tstamp dropped    = %s\n"
-			"tstamp available  = %s\n"
-			"PTP version       = %s\n"
-			"PTP Pkt Type      = %s\n"
-			"PTP Msg Type      = %s\n",
-			qInx, tstamp_dropped, tstamp_available,
-			ptp_version, ptp_pkt_type, ptp_msg_type);
+   uint32_t ptp_status = 0;
+   uint32_t pkt_type = 0;
+   char *tstamp_dropped = NULL;
+   char *tstamp_available = NULL;
+   char *ptp_version = NULL;
+   char *ptp_pkt_type = NULL;
+   char *ptp_msg_type = NULL;
+
+   DBGPR_PTP("-->DWC_ETH_QOS_print_rx_tstamp_info\n");
+
+   /* status in RDES1 is not valid */
+   if (!(rxdesc->RDES3 & DWC_ETH_QOS_RDESC3_RS1V))
+      return;
+
+   ptp_status = rxdesc->RDES1;
+   tstamp_dropped = ((ptp_status & 0x8000) ? "YES" : "NO");
+   tstamp_available = ((ptp_status & 0x4000) ? "YES" : "NO");
+   ptp_version = ((ptp_status & 0x2000) ? "v2 (1588-2008)" : "v1 (1588-2002)");
+   ptp_pkt_type = ((ptp_status & 0x1000) ? "ptp over Eth" : "ptp over IPv4/6");
+
+   pkt_type = ((ptp_status & 0xF00) > 8);
+   switch (pkt_type) {
+   case 0:
+      ptp_msg_type = "NO PTP msg received";
+      break;
+   case 1:
+      ptp_msg_type = "SYNC";
+      break;
+   case 2:
+      ptp_msg_type = "Follow_Up";
+      break;
+   case 3:
+      ptp_msg_type = "Delay_Req";
+      break;
+   case 4:
+      ptp_msg_type = "Delay_Resp";
+      break;
+   case 5:
+      ptp_msg_type = "Pdelay_Req";
+      break;
+   case 6:
+      ptp_msg_type = "Pdelay_Resp";
+      break;
+   case 7:
+      ptp_msg_type = "Pdelay_Resp_Follow_up";
+      break;
+   case 8:
+      ptp_msg_type = "Announce";
+      break;
+   case 9:
+      ptp_msg_type = "Management";
+      break;
+   case 10:
+      ptp_msg_type = "Signaling";
+      break;
+   case 11:
+   case 12:
+   case 13:
+   case 14:
+      ptp_msg_type = "Reserved";
+      break;
+   case 15:
+      ptp_msg_type = "PTP pkr with Reserved Msg Type";
+      break;
+   }
+
+   DBGPR_PTP("Rx timestamp detail for queue %d\n"
+         "tstamp dropped    = %s\n"
+         "tstamp available  = %s\n"
+         "PTP version       = %s\n"
+         "PTP Pkt Type      = %s\n"
+         "PTP Msg Type      = %s\n",
+         qInx, tstamp_dropped, tstamp_available,
+         ptp_version, ptp_pkt_type, ptp_msg_type);
 
-	DBGPR_PTP("<--DWC_ETH_QOS_print_rx_tstamp_info\n");
+   DBGPR_PTP("<--DWC_ETH_QOS_print_rx_tstamp_info\n");
 }
 
 /*!
@@ -2194,69 +2221,68 @@ static void DWC_ETH_QOS_print_rx_tstamp_
 * \retval 2 if time stamp is corrupted
 */
 static unsigned char DWC_ETH_QOS_get_rx_hwtstamp(
-	struct DWC_ETH_QOS_prv_data *pdata,
-	struct sk_buff *skb,
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data,
-	unsigned int qInx)
-{
-	rx_descriptor_t *rx_normal_desc =
-		GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
-	rx_descriptor_t *rx_context_desc = NULL;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct skb_shared_hwtstamps *shhwtstamp = NULL;
-	uint64_t ns;
-	int retry, ret;
-
-	DBGPR_PTP("-->DWC_ETH_QOS_get_rx_hwtstamp\n");
-
-	DWC_ETH_QOS_print_rx_tstamp_info(rx_normal_desc, qInx);
-
-	desc_data->dirty_rx++;
-	INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
-	rx_context_desc = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
-
-	DBGPR_PTP("\nRX_CONTEX_DESC[%d %4p %d RECEIVED FROM DEVICE]"\
-			" = %#x:%#x:%#x:%#x",
-			qInx, rx_context_desc, desc_data->cur_rx, rx_context_desc->RDES0,
-			rx_context_desc->RDES1,
-			rx_context_desc->RDES2, rx_context_desc->RDES3);
-
-	/* check rx tsatmp */
-	for (retry = 0; retry < 10; retry++) {
-		ret = hw_if->get_rx_tstamp_status(rx_context_desc);
-		if (ret == 1) {
-			/* time stamp is valid */
-			break;
-		} else if (ret == 0) {
-			printk(KERN_ALERT "Device has not yet updated the context "
-				"desc to hold Rx time stamp(retry = %d)\n", retry);
-		} else {
-			printk(KERN_ALERT "Error: Rx time stamp is corrupted(retry = %d)\n", retry);
-			return 2;
-		}
-	}
-
-	if (retry == 10) {
-			printk(KERN_ALERT "Device has not yet updated the context "
-				"desc to hold Rx time stamp(retry = %d)\n", retry);
-			desc_data->dirty_rx--;
-			DECR_RX_DESC_INDEX(desc_data->cur_rx);
-			return 0;
-	}
-
-	pdata->xstats.rx_timestamp_captured_n++;
-	/* get valid tstamp */
-	ns = hw_if->get_rx_tstamp(rx_context_desc);
-
-	shhwtstamp = skb_hwtstamps(skb);
-	memset(shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));
-	shhwtstamp->hwtstamp = ns_to_ktime(ns);
+   struct DWC_ETH_QOS_prv_data *pdata,
+   struct sk_buff *skb,
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data,
+   unsigned int qInx)
+{
+   rx_descriptor_t *rx_normal_desc =
+      GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
+   rx_descriptor_t *rx_context_desc = NULL;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct skb_shared_hwtstamps *shhwtstamp = NULL;
+   uint64_t ns;
+   int retry, ret;
+
+   DBGPR_PTP("-->DWC_ETH_QOS_get_rx_hwtstamp\n");
+
+   DWC_ETH_QOS_print_rx_tstamp_info(rx_normal_desc, qInx);
+
+   desc_data->dirty_rx++;
+   INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
+   rx_context_desc = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
+
+   DBGPR_PTP("\nRX_CONTEX_DESC[%d %4p %d RECEIVED FROM DEVICE]"\
+         " = %#x:%#x:%#x:%#x",
+         qInx, rx_context_desc, desc_data->cur_rx, rx_context_desc->RDES0,
+         rx_context_desc->RDES1,
+         rx_context_desc->RDES2, rx_context_desc->RDES3);
+
+   /* check rx tsatmp */
+   for (retry = 0; retry < 10; retry++) {
+      ret = hw_if->get_rx_tstamp_status(rx_context_desc);
+      if (ret == 1) {
+         /* time stamp is valid */
+         break;
+      } else if (ret == 0) {
+         printk(KERN_ALERT "Device has not yet updated the context "
+            "desc to hold Rx time stamp(retry = %d)\n", retry);
+      } else {
+         printk(KERN_ALERT "Error: Rx time stamp is corrupted(retry = %d)\n", retry);
+         return 2;
+      }
+   }
 
-	DBGPR_PTP("<--DWC_ETH_QOS_get_rx_hwtstamp\n");
+   if (retry == 10) {
+         printk(KERN_ALERT "Device has not yet updated the context "
+            "desc to hold Rx time stamp(retry = %d)\n", retry);
+         desc_data->dirty_rx--;
+         DECR_RX_DESC_INDEX(desc_data->cur_rx);
+         return 0;
+   }
 
-	return 1;
-}
+   pdata->xstats.rx_timestamp_captured_n++;
+   /* get valid tstamp */
+   ns = hw_if->get_rx_tstamp(rx_context_desc);
 
+   shhwtstamp = skb_hwtstamps(skb);
+   memset(shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));
+   shhwtstamp->hwtstamp = ns_to_ktime(ns);
+
+   DBGPR_PTP("<--DWC_ETH_QOS_get_rx_hwtstamp\n");
+
+   return 1;
+}
 
 /*!
 * \brief API to get tx time stamp value.
@@ -2274,48 +2300,48 @@ static unsigned char DWC_ETH_QOS_get_rx_
 * \retval 0 if time stamp in not taken/valid
 */
 static unsigned int DWC_ETH_QOS_get_tx_hwtstamp(
-	struct DWC_ETH_QOS_prv_data *pdata,
-	tx_descriptor_t *txdesc,
-	struct sk_buff *skb)
-{
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct skb_shared_hwtstamps shhwtstamp;
-	uint64_t ns;
-
-	DBGPR_PTP("-->DWC_ETH_QOS_get_tx_hwtstamp\n");
-
-	if (hw_if->drop_tx_status_enabled() == 0) {
-		/* check tx tstamp status */
-		if (!hw_if->get_tx_tstamp_status(txdesc)) {
-			printk(KERN_ALERT "tx timestamp is not captured for this packet\n");
-			return 0;
-		}
-
-		/* get the valid tstamp */
-		ns = hw_if->get_tx_tstamp(txdesc);
-	} else {
-		/* drop tx status mode is enabled, hence read time
-		 * stamp from register instead of descriptor */
-
-		/* check tx tstamp status */
-		if (!hw_if->get_tx_tstamp_status_via_reg()) {
-			printk(KERN_ALERT "tx timestamp is not captured for this packet\n");
-			return 0;
-		}
-
-		/* get the valid tstamp */
-		ns = hw_if->get_tx_tstamp_via_reg();
-	}
-
-	pdata->xstats.tx_timestamp_captured_n++;
-	memset(&shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));
-	shhwtstamp.hwtstamp = ns_to_ktime(ns);
-	/* pass tstamp to stack */
-	skb_tstamp_tx(skb, &shhwtstamp);
+   struct DWC_ETH_QOS_prv_data *pdata,
+   tx_descriptor_t *txdesc,
+   struct sk_buff *skb)
+{
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct skb_shared_hwtstamps shhwtstamp;
+   uint64_t ns;
+
+   DBGPR_PTP("-->DWC_ETH_QOS_get_tx_hwtstamp\n");
+
+   if (hw_if->drop_tx_status_enabled() == 0) {
+      /* check tx tstamp status */
+      if (!hw_if->get_tx_tstamp_status(txdesc)) {
+         printk(KERN_ALERT "tx timestamp is not captured for this packet\n");
+         return 0;
+      }
 
-	DBGPR_PTP("<--DWC_ETH_QOS_get_tx_hwtstamp\n");
+      /* get the valid tstamp */
+      ns = hw_if->get_tx_tstamp(txdesc);
+   } else {
+      /* drop tx status mode is enabled, hence read time
+       * stamp from register instead of descriptor */
+
+      /* check tx tstamp status */
+      if (!hw_if->get_tx_tstamp_status_via_reg()) {
+         printk(KERN_ALERT "tx timestamp is not captured for this packet\n");
+         return 0;
+      }
 
-	return 1;
+      /* get the valid tstamp */
+      ns = hw_if->get_tx_tstamp_via_reg();
+   }
+
+   pdata->xstats.tx_timestamp_captured_n++;
+   memset(&shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));
+   shhwtstamp.hwtstamp = ns_to_ktime(ns);
+   /* pass tstamp to stack */
+   skb_tstamp_tx(skb, &shhwtstamp);
+
+   DBGPR_PTP("<--DWC_ETH_QOS_get_tx_hwtstamp\n");
+
+   return 1;
 }
 
 /*!
@@ -2332,256 +2358,256 @@ static unsigned int DWC_ETH_QOS_get_tx_h
 * \return void
 */
 static void DWC_ETH_QOS_tx_interrupt(struct net_device *dev,
-				     struct DWC_ETH_QOS_prv_data *pdata,
-				     uint32_t qInx)
+                 struct DWC_ETH_QOS_prv_data *pdata,
+                 uint32_t qInx)
 {
-	struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
-	    GET_TX_WRAPPER_DESC(qInx);
-	tx_descriptor_t *txptr = NULL;
-	struct DWC_ETH_QOS_tx_buffer *buffer = NULL;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct desc_if_struct *desc_if = &(pdata->desc_if);
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data =
+       GET_TX_WRAPPER_DESC(qInx);
+   tx_descriptor_t *txptr = NULL;
+   struct DWC_ETH_QOS_tx_buffer *buffer = NULL;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct desc_if_struct *desc_if = &(pdata->desc_if);
 #ifndef DWC_ETH_QOS_CERTIFICATION_PKTBURSTCNT
-	int err_incremented;
+   int err_incremented;
 #endif
-	unsigned int tstamp_taken = 0;
-	unsigned long flags;
-
-	DBGPR("-->DWC_ETH_QOS_tx_interrupt: desc_data->tx_pkt_queued = %d"
-		" dirty_tx = %d, qInx = %u\n",
-		desc_data->tx_pkt_queued, desc_data->dirty_tx, qInx);
+   unsigned int tstamp_taken = 0;
+   unsigned long flags;
 
-	spin_lock_irqsave(&pdata->tx_lock, flags);
+   DBGPR("-->DWC_ETH_QOS_tx_interrupt: desc_data->tx_pkt_queued = %d"
+      " dirty_tx = %d, qInx = %u\n",
+      desc_data->tx_pkt_queued, desc_data->dirty_tx, qInx);
+
+   spin_lock_irqsave(&pdata->tx_lock, flags);
+
+   pdata->xstats.tx_clean_n[qInx]++;
+   while (desc_data->tx_pkt_queued > 0) {
+      txptr = GET_TX_DESC_PTR(qInx, desc_data->dirty_tx);
+      buffer = GET_TX_BUF_PTR(qInx, desc_data->dirty_tx);
+      tstamp_taken = 0;
 
-	pdata->xstats.tx_clean_n[qInx]++;
-	while (desc_data->tx_pkt_queued > 0) {
-		txptr = GET_TX_DESC_PTR(qInx, desc_data->dirty_tx);
-		buffer = GET_TX_BUF_PTR(qInx, desc_data->dirty_tx);
-		tstamp_taken = 0;
-
-		if (!hw_if->tx_complete(txptr))
-			break;
+      if (!hw_if->tx_complete(txptr))
+         break;
 
 #ifdef GBE_DEBUG
-		PRINT_TX_DESC(txptr, NORMAL_WB);
+      PRINT_TX_DESC(txptr, NORMAL_WB);
 #endif
 
 #ifndef DWC_ETH_QOS_CERTIFICATION_PKTBURSTCNT
-		/* update the tx error if any by looking at last segment
-		 * for NORMAL descriptors
-		 * */
-		if ((hw_if->get_tx_desc_ls(txptr)) && !(hw_if->get_tx_desc_ctxt(txptr))) {
-			/* check whether skb support hw tstamp */
-			if (pdata->hw_feat.tsstssel && buffer->skb &&
-				(skb_shinfo(buffer->skb)->tx_flags & SKBTX_IN_PROGRESS)) {
-				tstamp_taken = DWC_ETH_QOS_get_tx_hwtstamp(pdata, txptr, buffer->skb);
-				if (tstamp_taken) {
-					DBGPR_PTP("passed tx timestamp to stack[qInx = %d, dirty_tx = %d]\n",
-						qInx, desc_data->dirty_tx);
-				}
-			}
-
-			err_incremented = 0;
-			if (hw_if->tx_window_error) {
-				if (hw_if->tx_window_error(txptr)) {
-					err_incremented = 1;
-					dev->stats.tx_window_errors++;
-				}
-			}
-			if (hw_if->tx_aborted_error) {
-				if (hw_if->tx_aborted_error(txptr)) {
-					err_incremented = 1;
-					dev->stats.tx_aborted_errors++;
-					if (hw_if->tx_handle_aborted_error)
-						hw_if->tx_handle_aborted_error(txptr);
-				}
-			}
-			if (hw_if->tx_carrier_lost_error) {
-				if (hw_if->tx_carrier_lost_error(txptr)) {
-					err_incremented = 1;
-					dev->stats.tx_carrier_errors++;
-				}
-			}
-			if (hw_if->tx_fifo_underrun) {
-				if (hw_if->tx_fifo_underrun(txptr)) {
-					err_incremented = 1;
-					dev->stats.tx_fifo_errors++;
-					if (hw_if->tx_update_fifo_threshold)
-						hw_if->tx_update_fifo_threshold(txptr);
-				}
-			}
-			if (hw_if->tx_get_collision_count)
-				dev->stats.collisions +=
-				    hw_if->tx_get_collision_count(txptr);
-
-			if (err_incremented == 1)
-				dev->stats.tx_errors++;
-
-			pdata->xstats.q_tx_pkt_n[qInx]++;
-			pdata->xstats.tx_pkt_n++;
-			dev->stats.tx_packets++;
-		}
+      /* update the tx error if any by looking at last segment
+       * for NORMAL descriptors
+       * */
+      if ((hw_if->get_tx_desc_ls(txptr)) && !(hw_if->get_tx_desc_ctxt(txptr))) {
+         /* check whether skb support hw tstamp */
+         if (pdata->hw_feat.tsstssel && buffer->skb &&
+            (skb_shinfo(buffer->skb)->tx_flags & SKBTX_IN_PROGRESS)) {
+            tstamp_taken = DWC_ETH_QOS_get_tx_hwtstamp(pdata, txptr, buffer->skb);
+            if (tstamp_taken) {
+               DBGPR_PTP("passed tx timestamp to stack[qInx = %d, dirty_tx = %d]\n",
+                  qInx, desc_data->dirty_tx);
+            }
+         }
+
+         err_incremented = 0;
+         if (hw_if->tx_window_error) {
+            if (hw_if->tx_window_error(txptr)) {
+               err_incremented = 1;
+               dev->stats.tx_window_errors++;
+            }
+         }
+         if (hw_if->tx_aborted_error) {
+            if (hw_if->tx_aborted_error(txptr)) {
+               err_incremented = 1;
+               dev->stats.tx_aborted_errors++;
+               if (hw_if->tx_handle_aborted_error)
+                  hw_if->tx_handle_aborted_error(txptr);
+            }
+         }
+         if (hw_if->tx_carrier_lost_error) {
+            if (hw_if->tx_carrier_lost_error(txptr)) {
+               err_incremented = 1;
+               dev->stats.tx_carrier_errors++;
+            }
+         }
+         if (hw_if->tx_fifo_underrun) {
+            if (hw_if->tx_fifo_underrun(txptr)) {
+               err_incremented = 1;
+               dev->stats.tx_fifo_errors++;
+               if (hw_if->tx_update_fifo_threshold)
+                  hw_if->tx_update_fifo_threshold(txptr);
+            }
+         }
+         if (hw_if->tx_get_collision_count)
+            dev->stats.collisions +=
+                hw_if->tx_get_collision_count(txptr);
+
+         if (err_incremented == 1)
+            dev->stats.tx_errors++;
+
+         pdata->xstats.q_tx_pkt_n[qInx]++;
+         pdata->xstats.tx_pkt_n++;
+         dev->stats.tx_packets++;
+      }
 #else
-		if ((hw_if->get_tx_desc_ls(txptr)) && !(hw_if->get_tx_desc_ctxt(txptr))) {
-			/* check whether skb support hw tstamp */
-			if (pdata->hw_feat.tsstssel && buffer->skb &&
-				(skb_shinfo(buffer->skb)->tx_flags & SKBTX_IN_PROGRESS)) {
-				tstamp_taken = DWC_ETH_QOS_get_tx_hwtstamp(pdata,
-					txptr, buffer->skb);
-				if (tstamp_taken) {
-					DBGPR_PTP("passed tx timestamp to stack[qInx = %d, dirty_tx = %d]\n",
-						qInx, desc_data->dirty_tx);
-				}
-			}
-		}
-#endif
-		dev->stats.tx_bytes += buffer->len;
-		dev->stats.tx_bytes += buffer->len2;
-		desc_if->unmap_tx_skb(pdata, buffer);
-
-		/* reset the descriptor so that driver/host can reuse it */
-		hw_if->tx_desc_reset(desc_data->dirty_tx, pdata, qInx);
-
-		INCR_TX_DESC_INDEX(desc_data->dirty_tx, 1);
-		desc_data->free_desc_cnt++;
-		desc_data->tx_pkt_queued--;
-	}
-
-	if ((desc_data->queue_stopped == 1) && (desc_data->free_desc_cnt > 0)) {
-		desc_data->queue_stopped = 0;
-		netif_wake_subqueue(dev, qInx);
-	}
+      if ((hw_if->get_tx_desc_ls(txptr)) && !(hw_if->get_tx_desc_ctxt(txptr))) {
+         /* check whether skb support hw tstamp */
+         if (pdata->hw_feat.tsstssel && buffer->skb &&
+            (skb_shinfo(buffer->skb)->tx_flags & SKBTX_IN_PROGRESS)) {
+            tstamp_taken = DWC_ETH_QOS_get_tx_hwtstamp(pdata,
+               txptr, buffer->skb);
+            if (tstamp_taken) {
+               DBGPR_PTP("passed tx timestamp to stack[qInx = %d, dirty_tx = %d]\n",
+                  qInx, desc_data->dirty_tx);
+            }
+         }
+      }
+#endif
+      dev->stats.tx_bytes += buffer->len;
+      dev->stats.tx_bytes += buffer->len2;
+      desc_if->unmap_tx_skb(pdata, buffer);
+
+      /* reset the descriptor so that driver/host can reuse it */
+      hw_if->tx_desc_reset(desc_data->dirty_tx, pdata, qInx);
+
+      INCR_TX_DESC_INDEX(desc_data->dirty_tx, 1);
+      desc_data->free_desc_cnt++;
+      desc_data->tx_pkt_queued--;
+   }
+
+   if ((desc_data->queue_stopped == 1) && (desc_data->free_desc_cnt > 0)) {
+      desc_data->queue_stopped = 0;
+      netif_wake_subqueue(dev, qInx);
+   }
 #ifdef DWC_ETH_QOS_CERTIFICATION_PKTBURSTCNT
-	/* DMA has finished Transmitting data to MAC Tx-Fifo */
-	MAC_MCR_TE_UdfWr(1);
+   /* DMA has finished Transmitting data to MAC Tx-Fifo */
+   MAC_MCR_TE_UdfWr(1);
 #endif
 
-	if ((pdata->eee_enabled) && (!pdata->tx_path_in_lpi_mode) &&
-		(!pdata->use_lpi_tx_automate)) {
-		DWC_ETH_QOS_enable_eee_mode(pdata);
-		mod_timer(&pdata->eee_ctrl_timer,
-			DWC_ETH_QOS_LPI_TIMER(DWC_ETH_QOS_DEFAULT_LPI_TIMER));
-	}
+   if ((pdata->eee_enabled) && (!pdata->tx_path_in_lpi_mode) &&
+      (!pdata->use_lpi_tx_automate)) {
+      DWC_ETH_QOS_enable_eee_mode(pdata);
+      mod_timer(&pdata->eee_ctrl_timer,
+         DWC_ETH_QOS_LPI_TIMER(DWC_ETH_QOS_DEFAULT_LPI_TIMER));
+   }
 
-	spin_unlock_irqrestore(&pdata->tx_lock, flags);
+   spin_unlock_irqrestore(&pdata->tx_lock, flags);
 
-	DBGPR("<--DWC_ETH_QOS_tx_interrupt: desc_data->tx_pkt_queued = %d\n",
-	      desc_data->tx_pkt_queued);
+   DBGPR("<--DWC_ETH_QOS_tx_interrupt: desc_data->tx_pkt_queued = %d\n",
+         desc_data->tx_pkt_queued);
 }
 
 #ifdef YDEBUG_FILTER
 static void DWC_ETH_QOS_check_rx_filter_status(rx_descriptor_t *RX_NORMAL_DESC)
 {
-	uint32_t rdes2 = RX_NORMAL_DESC->RDES2;
-	uint32_t rdes3 = RX_NORMAL_DESC->RDES3;
+   uint32_t rdes2 = RX_NORMAL_DESC->RDES2;
+   uint32_t rdes3 = RX_NORMAL_DESC->RDES3;
 
-	/* Receive Status RDES2 Valid ? */
-	if ((rdes3 & 0x8000000) == 0x8000000) {
-		if ((rdes2 & 0x400) == 0x400)
-			printk(KERN_ALERT "ARP pkt received\n");
-		if ((rdes2 & 0x800) == 0x800)
-			printk(KERN_ALERT "ARP reply not generated\n");
-		if ((rdes2 & 0x8000) == 0x8000)
-			printk(KERN_ALERT "VLAN pkt passed VLAN filter\n");
-		if ((rdes2 & 0x10000) == 0x10000)
-			printk(KERN_ALERT "SA Address filter fail\n");
-		if ((rdes2 & 0x20000) == 0x20000)
-			printk(KERN_ALERT "DA Addess filter fail\n");
-		if ((rdes2 & 0x40000) == 0x40000)
-			printk(KERN_ALERT "pkt passed the HASH filter in MAC and HASH value = %#x\n",
-					(rdes2 >> 19) & 0xff);
-		if ((rdes2 & 0x8000000) == 0x8000000)
-			printk(KERN_ALERT "L3 filter(%d) Match\n", ((rdes2 >> 29) & 0x7));
-		if ((rdes2 & 0x10000000) == 0x10000000)
-			printk(KERN_ALERT "L4 filter(%d) Match\n", ((rdes2 >> 29) & 0x7));
-	}
+   /* Receive Status RDES2 Valid ? */
+   if ((rdes3 & 0x8000000) == 0x8000000) {
+      if ((rdes2 & 0x400) == 0x400)
+         printk(KERN_ALERT "ARP pkt received\n");
+      if ((rdes2 & 0x800) == 0x800)
+         printk(KERN_ALERT "ARP reply not generated\n");
+      if ((rdes2 & 0x8000) == 0x8000)
+         printk(KERN_ALERT "VLAN pkt passed VLAN filter\n");
+      if ((rdes2 & 0x10000) == 0x10000)
+         printk(KERN_ALERT "SA Address filter fail\n");
+      if ((rdes2 & 0x20000) == 0x20000)
+         printk(KERN_ALERT "DA Addess filter fail\n");
+      if ((rdes2 & 0x40000) == 0x40000)
+         printk(KERN_ALERT "pkt passed the HASH filter in MAC and HASH value = %#x\n",
+               (rdes2 >> 19) & 0xff);
+      if ((rdes2 & 0x8000000) == 0x8000000)
+         printk(KERN_ALERT "L3 filter(%d) Match\n", ((rdes2 >> 29) & 0x7));
+      if ((rdes2 & 0x10000000) == 0x10000000)
+         printk(KERN_ALERT "L4 filter(%d) Match\n", ((rdes2 >> 29) & 0x7));
+   }
 }
 #endif /* YDEBUG_FILTER */
 
 /* pass skb to upper layer */
 static void DWC_ETH_QOS_receive_skb(struct DWC_ETH_QOS_prv_data *pdata,
-				    struct net_device *dev, struct sk_buff *skb,
-				    uint32_t qInx)
+                struct net_device *dev, struct sk_buff *skb,
+                uint32_t qInx)
 {
-	struct DWC_ETH_QOS_rx_queue *rx_queue = GET_RX_QUEUE_PTR(qInx);
+   struct DWC_ETH_QOS_rx_queue *rx_queue = GET_RX_QUEUE_PTR(qInx);
 
-	skb_record_rx_queue(skb, qInx);
-	skb->dev = dev;
-	skb->protocol = eth_type_trans(skb, dev);
-
-	if (dev->features & NETIF_F_GRO) {
-		napi_gro_receive(&pdata->rx_napi, skb);
-	} else if ((dev->features & NETIF_F_LRO) &&
-		(skb->ip_summed == CHECKSUM_UNNECESSARY)) {
-		lro_receive_skb(&rx_queue->lro_mgr, skb, (void *)pdata);
-		rx_queue->lro_flush_needed = 1;
-	} else {
-		netif_receive_skb(skb);
-	}
+   skb_record_rx_queue(skb, qInx);
+   skb->dev = dev;
+   skb->protocol = eth_type_trans(skb, dev);
+
+   if (dev->features & NETIF_F_GRO) {
+      napi_gro_receive(&pdata->rx_napi, skb);
+   } else if ((dev->features & NETIF_F_LRO) &&
+      (skb->ip_summed == CHECKSUM_UNNECESSARY)) {
+      lro_receive_skb(&rx_queue->lro_mgr, skb, (void *)pdata);
+      rx_queue->lro_flush_needed = 1;
+   } else {
+      netif_receive_skb(skb);
+   }
 }
 
 static void DWC_ETH_QOS_consume_page(struct DWC_ETH_QOS_rx_buffer *buffer,
-				     struct sk_buff *skb,
-				     u16 length, u16 buf2_used)
+                 struct sk_buff *skb,
+                 u16 length, u16 buf2_used)
 {
-	buffer->page = NULL;
-	if (buf2_used)
-		buffer->page2 = NULL;
-	skb->len += length;
-	skb->data_len += length;
-	skb->truesize += length;
+   buffer->page = NULL;
+   if (buf2_used)
+      buffer->page2 = NULL;
+   skb->len += length;
+   skb->data_len += length;
+   skb->truesize += length;
 }
 
 static void DWC_ETH_QOS_consume_page_split_hdr(
-				struct DWC_ETH_QOS_rx_buffer *buffer,
-				struct sk_buff *skb,
-				u16 length,
-				uint16_t page2_used)
-{
-	if (page2_used)
-		buffer->page2 = NULL;
-
-	skb->len += length;
-	skb->data_len += length;
-	skb->truesize += length;
+            struct DWC_ETH_QOS_rx_buffer *buffer,
+            struct sk_buff *skb,
+            u16 length,
+            uint16_t page2_used)
+{
+   if (page2_used)
+      buffer->page2 = NULL;
+
+   skb->len += length;
+   skb->data_len += length;
+   skb->truesize += length;
 }
 
 /* Receive Checksum Offload configuration */
 static inline void DWC_ETH_QOS_config_rx_csum(struct DWC_ETH_QOS_prv_data *pdata,
-		struct sk_buff *skb,
-		rx_descriptor_t *rx_normal_desc)
+      struct sk_buff *skb,
+      rx_descriptor_t *rx_normal_desc)
 {
-	skb->ip_summed = CHECKSUM_NONE;
-	if ((pdata->dev_state & NETIF_F_RXCSUM) == NETIF_F_RXCSUM) {
-		/* Receive Status RDES1 Valid ? */
-		if ((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_RS1V)) {
-			/* check(RDES1.IPCE bit) whether device has done csum correctly or not */
-			if ((rx_normal_desc->RDES1 & 0xC8) == 0x0)
-				skb->ip_summed = CHECKSUM_UNNECESSARY;	/* csum done by device */
-		}
-	}
+   skb->ip_summed = CHECKSUM_NONE;
+   if ((pdata->dev_state & NETIF_F_RXCSUM) == NETIF_F_RXCSUM) {
+      /* Receive Status RDES1 Valid ? */
+      if ((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_RS1V)) {
+         /* check(RDES1.IPCE bit) whether device has done csum correctly or not */
+         if ((rx_normal_desc->RDES1 & 0xC8) == 0x0)
+            skb->ip_summed = CHECKSUM_UNNECESSARY;   /* csum done by device */
+      }
+   }
 }
 
 static inline void DWC_ETH_QOS_get_rx_vlan(struct DWC_ETH_QOS_prv_data *pdata,
-			struct sk_buff *skb,
-			rx_descriptor_t *rx_normal_desc)
+         struct sk_buff *skb,
+         rx_descriptor_t *rx_normal_desc)
 {
-	uint16_t vlan_tag = 0;
+   uint16_t vlan_tag = 0;
 
-	if ((pdata->dev_state & NETIF_F_HW_VLAN_CTAG_RX) == NETIF_F_HW_VLAN_CTAG_RX) {
-		/* Receive Status RDES0 Valid ? */
-		if ((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_RS0V)) {
-			/* device received frame with VLAN Tag or
-			 * double VLAN Tag ? */
-			if (((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_LT) == 0x40000)
-				|| ((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_LT) == 0x50000)) {
-				vlan_tag = rx_normal_desc->RDES0 & 0xffff;
-				/* insert VLAN tag into skb */
-				__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag);
-				pdata->xstats.rx_vlan_pkt_n++;
-			}
-		}
-	}
+   if ((pdata->dev_state & NETIF_F_HW_VLAN_CTAG_RX) == NETIF_F_HW_VLAN_CTAG_RX) {
+      /* Receive Status RDES0 Valid ? */
+      if ((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_RS0V)) {
+         /* device received frame with VLAN Tag or
+          * double VLAN Tag ? */
+         if (((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_LT) == 0x40000)
+            || ((rx_normal_desc->RDES3 & DWC_ETH_QOS_RDESC3_LT) == 0x50000)) {
+            vlan_tag = rx_normal_desc->RDES0 & 0xffff;
+            /* insert VLAN tag into skb */
+            __vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag);
+            pdata->xstats.rx_vlan_pkt_n++;
+         }
+      }
+   }
 }
 
 /* This api check for payload type and returns
@@ -2589,16 +2615,16 @@ static inline void DWC_ETH_QOS_get_rx_vl
  * */
 static int DWC_ETH_QOS_check_for_tcp_payload(rx_descriptor_t *rxdesc)
 {
-		uint32_t pt_type = 0;
-		int ret = 0;
+      uint32_t pt_type = 0;
+      int ret = 0;
 
-		if (rxdesc->RDES3 & DWC_ETH_QOS_RDESC3_RS1V) {
-				pt_type = rxdesc->RDES1 & DWC_ETH_QOS_RDESC1_PT;
-				if (pt_type == DWC_ETH_QOS_RDESC1_PT_TCP)
-						ret = 1;
-		}
+      if (rxdesc->RDES3 & DWC_ETH_QOS_RDESC3_RS1V) {
+            pt_type = rxdesc->RDES1 & DWC_ETH_QOS_RDESC1_PT;
+            if (pt_type == DWC_ETH_QOS_RDESC1_PT_TCP)
+                  ret = 1;
+      }
 
-		return ret;
+      return ret;
 }
 
 /*!
@@ -2620,266 +2646,266 @@ static int DWC_ETH_QOS_check_for_tcp_pay
 * \retval number of packets received.
 */
 static int DWC_ETH_QOS_clean_split_hdr_rx_irq(
-			struct DWC_ETH_QOS_prv_data *pdata,
-			int quota,
-			uint32_t qInx)
-{
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
-	    GET_RX_WRAPPER_DESC(qInx);
-	struct net_device *dev = pdata->dev;
-	struct desc_if_struct *desc_if = &pdata->desc_if;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct sk_buff *skb = NULL;
-	int received = 0;
-	struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
-	rx_descriptor_t *RX_NORMAL_DESC = NULL;
-	u16 pkt_len;
-	unsigned short hdr_len = 0;
-	unsigned short payload_len = 0;
-	unsigned char intermediate_desc_cnt = 0;
-	unsigned char buf2_used = 0;
-	int ret;
-
-	DBGPR("-->DWC_ETH_QOS_clean_split_hdr_rx_irq: qInx = %u, quota = %d\n",
-		qInx, quota);
-
-	while (received < quota) {
-		buffer = GET_RX_BUF_PTR(qInx, desc_data->cur_rx);
-		RX_NORMAL_DESC = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
+         struct DWC_ETH_QOS_prv_data *pdata,
+         int quota,
+         uint32_t qInx)
+{
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
+       GET_RX_WRAPPER_DESC(qInx);
+   struct net_device *dev = pdata->dev;
+   struct desc_if_struct *desc_if = &pdata->desc_if;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct sk_buff *skb = NULL;
+   int received = 0;
+   struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
+   rx_descriptor_t *RX_NORMAL_DESC = NULL;
+   u16 pkt_len;
+   unsigned short hdr_len = 0;
+   unsigned short payload_len = 0;
+   unsigned char intermediate_desc_cnt = 0;
+   unsigned char buf2_used = 0;
+   int ret;
+
+   DBGPR("-->DWC_ETH_QOS_clean_split_hdr_rx_irq: qInx = %u, quota = %d\n",
+      qInx, quota);
+
+   while (received < quota) {
+      buffer = GET_RX_BUF_PTR(qInx, desc_data->cur_rx);
+      RX_NORMAL_DESC = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
 
-		/* check for data availability */
-		if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
+      /* check for data availability */
+      if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
 #ifdef DWC_ETH_QOS_ENABLE_RX_DESC_DUMP
-			dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
+         dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
 #endif
-			/* assign it to new skb */
-			skb = buffer->skb;
-			buffer->skb = NULL;
-
-			/* first buffer pointer */
-			dma_unmap_single(&pdata->pdev->dev, buffer->dma,
-				       (2*buffer->rx_hdr_size), DMA_FROM_DEVICE);
-			buffer->dma = 0;
-
-			/* second buffer pointer */
-			dma_unmap_page(&pdata->pdev->dev, buffer->dma2,
-				       PAGE_SIZE, DMA_FROM_DEVICE);
-			buffer->dma2 = 0;
-
-			/* get the packet length */
-			pkt_len =
-			    (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_PL);
-
-			/* FIRST desc and Receive Status RDES2 Valid ? */
-			if ((RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD) &&
-				(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_RS2V)) {
-				/* get header length */
-				hdr_len = (RX_NORMAL_DESC->RDES2 & DWC_ETH_QOS_RDESC2_HL);
-				DBGPR("Device has %s HEADER SPLIT: hdr_len = %d\n",
-						(hdr_len ? "done" : "not done"), hdr_len);
-				if (hdr_len)
-					pdata->xstats.rx_split_hdr_pkt_n++;
-			}
-
-			/* check for bad packet,
-			 * error is valid only for last descriptor(OWN + LD bit set).
-			 * */
-			if ((RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_ES) &&
-			    (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
-				DBGPR("Error in rcved pkt, failed to pass it to upper layer\n");
+         /* assign it to new skb */
+         skb = buffer->skb;
+         buffer->skb = NULL;
+
+         /* first buffer pointer */
+         dma_unmap_single(&pdata->pdev->dev, buffer->dma,
+                   (2*buffer->rx_hdr_size), DMA_FROM_DEVICE);
+         buffer->dma = 0;
+
+         /* second buffer pointer */
+         dma_unmap_page(&pdata->pdev->dev, buffer->dma2,
+                   PAGE_SIZE, DMA_FROM_DEVICE);
+         buffer->dma2 = 0;
+
+         /* get the packet length */
+         pkt_len =
+             (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_PL);
+
+         /* FIRST desc and Receive Status RDES2 Valid ? */
+         if ((RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD) &&
+            (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_RS2V)) {
+            /* get header length */
+            hdr_len = (RX_NORMAL_DESC->RDES2 & DWC_ETH_QOS_RDESC2_HL);
+            DBGPR("Device has %s HEADER SPLIT: hdr_len = %d\n",
+                  (hdr_len ? "done" : "not done"), hdr_len);
+            if (hdr_len)
+               pdata->xstats.rx_split_hdr_pkt_n++;
+         }
+
+         /* check for bad packet,
+          * error is valid only for last descriptor(OWN + LD bit set).
+          * */
+         if ((RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_ES) &&
+             (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
+            DBGPR("Error in rcved pkt, failed to pass it to upper layer\n");
 #ifdef DWC_ETH_QOS_ENABLE_RX_DESC_DUMP
-				dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
+            dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
 #endif
-				dev->stats.rx_errors++;
-				DWC_ETH_QOS_update_rx_errors(dev,
-					RX_NORMAL_DESC->RDES3);
-
-				/* recycle both page/buff and skb */
-				buffer->skb = skb;
-				if (desc_data->skb_top)
-					dev_kfree_skb_any(desc_data->skb_top);
-
-				desc_data->skb_top = NULL;
-				goto next_desc;
-			}
-
-			if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
-				intermediate_desc_cnt++;
-				buf2_used = 1;
-				/* this descriptor is only the beginning/middle */
-				if (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD) {
-					/* this is the beginning of a chain */
-
-					/* here skb/skb_top may contain
-					 * if (device done split header)
-					 *	only header
-					 * else
-					 *	header/(header + payload)
-					 * */
-					desc_data->skb_top = skb;
-					/* page2 always contain only payload */
-					if (hdr_len) {
-						/* add header len to first skb->len */
-						skb_put(skb, hdr_len);
-						payload_len = pdata->rx_buffer_len;
-						skb_fill_page_desc(skb, 0,
-							buffer->page2, 0,
-							payload_len);
-					} else {
-						/* add header len to first skb->len */
-						skb_put(skb, buffer->rx_hdr_size);
-						/* No split header, hence
-						 * pkt_len = (payload + hdr_len)
-						 * */
-						payload_len = (pkt_len - buffer->rx_hdr_size);
-						skb_fill_page_desc(skb, 0,
-							buffer->page2, 0,
-							payload_len);
-					}
-				} else {
-					/* this is the middle of a chain */
-					payload_len = pdata->rx_buffer_len;
-					skb_fill_page_desc(desc_data->skb_top,
-						skb_shinfo(desc_data->skb_top)->nr_frags,
-						buffer->page2, 0,
-						payload_len);
-
-					/* re-use this skb, as consumed only the page */
-					buffer->skb = skb;
-				}
-				DWC_ETH_QOS_consume_page_split_hdr(buffer,
-							 desc_data->skb_top,
-							 payload_len, buf2_used);
-				goto next_desc;
-			} else {
-				if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD)) {
-					buf2_used = 1;
-					/* end of the chain */
-					if (hdr_len) {
-						payload_len = (pkt_len -
-							(pdata->rx_buffer_len * intermediate_desc_cnt) -
-							hdr_len);
-					} else {
-						payload_len = (pkt_len -
-							(pdata->rx_buffer_len * intermediate_desc_cnt) -
-							buffer->rx_hdr_size);
-					}
-
-					skb_fill_page_desc(desc_data->skb_top,
-						skb_shinfo(desc_data->skb_top)->nr_frags,
-						buffer->page2, 0,
-						payload_len);
-
-					/* re-use this skb, as consumed only the page */
-					buffer->skb = skb;
-					skb = desc_data->skb_top;
-					desc_data->skb_top = NULL;
-					DWC_ETH_QOS_consume_page_split_hdr(buffer, skb,
-								 payload_len, buf2_used);
-				} else {
-					/* no chain, got both FD + LD together */
-					if (hdr_len) {
-						buf2_used = 1;
-						/* add header len to first skb->len */
-						skb_put(skb, hdr_len);
-
-						payload_len = pkt_len - hdr_len;
-						skb_fill_page_desc(skb, 0,
-							buffer->page2, 0,
-							payload_len);
-					} else {
-						/* No split header, hence
-						 * payload_len = (payload + hdr_len)
-						 * */
-						if (pkt_len > buffer->rx_hdr_size) {
-							buf2_used = 1;
-							/* add header len to first skb->len */
-							skb_put(skb, buffer->rx_hdr_size);
-
-							payload_len = (pkt_len - buffer->rx_hdr_size);
-							skb_fill_page_desc(skb, 0,
-								buffer->page2, 0,
-								payload_len);
-						} else {
-							buf2_used = 0;
-							/* add header len to first skb->len */
-							skb_put(skb, pkt_len);
-							payload_len = 0; /* no data in page2 */
-						}
-					}
-					DWC_ETH_QOS_consume_page_split_hdr(buffer,
-							skb, payload_len,
-							buf2_used);
-				}
-				/* reset for next new packet/frame */
-				intermediate_desc_cnt = 0;
-				hdr_len = 0;
-			}
+            dev->stats.rx_errors++;
+            DWC_ETH_QOS_update_rx_errors(dev,
+               RX_NORMAL_DESC->RDES3);
+
+            /* recycle both page/buff and skb */
+            buffer->skb = skb;
+            if (desc_data->skb_top)
+               dev_kfree_skb_any(desc_data->skb_top);
+
+            desc_data->skb_top = NULL;
+            goto next_desc;
+         }
+
+         if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
+            intermediate_desc_cnt++;
+            buf2_used = 1;
+            /* this descriptor is only the beginning/middle */
+            if (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD) {
+               /* this is the beginning of a chain */
+
+               /* here skb/skb_top may contain
+                * if (device done split header)
+                *   only header
+                * else
+                *   header/(header + payload)
+                * */
+               desc_data->skb_top = skb;
+               /* page2 always contain only payload */
+               if (hdr_len) {
+                  /* add header len to first skb->len */
+                  skb_put(skb, hdr_len);
+                  payload_len = pdata->rx_buffer_len;
+                  skb_fill_page_desc(skb, 0,
+                     buffer->page2, 0,
+                     payload_len);
+               } else {
+                  /* add header len to first skb->len */
+                  skb_put(skb, buffer->rx_hdr_size);
+                  /* No split header, hence
+                   * pkt_len = (payload + hdr_len)
+                   * */
+                  payload_len = (pkt_len - buffer->rx_hdr_size);
+                  skb_fill_page_desc(skb, 0,
+                     buffer->page2, 0,
+                     payload_len);
+               }
+            } else {
+               /* this is the middle of a chain */
+               payload_len = pdata->rx_buffer_len;
+               skb_fill_page_desc(desc_data->skb_top,
+                  skb_shinfo(desc_data->skb_top)->nr_frags,
+                  buffer->page2, 0,
+                  payload_len);
+
+               /* re-use this skb, as consumed only the page */
+               buffer->skb = skb;
+            }
+            DWC_ETH_QOS_consume_page_split_hdr(buffer,
+                      desc_data->skb_top,
+                      payload_len, buf2_used);
+            goto next_desc;
+         } else {
+            if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD)) {
+               buf2_used = 1;
+               /* end of the chain */
+               if (hdr_len) {
+                  payload_len = (pkt_len -
+                     (pdata->rx_buffer_len * intermediate_desc_cnt) -
+                     hdr_len);
+               } else {
+                  payload_len = (pkt_len -
+                     (pdata->rx_buffer_len * intermediate_desc_cnt) -
+                     buffer->rx_hdr_size);
+               }
+
+               skb_fill_page_desc(desc_data->skb_top,
+                  skb_shinfo(desc_data->skb_top)->nr_frags,
+                  buffer->page2, 0,
+                  payload_len);
+
+               /* re-use this skb, as consumed only the page */
+               buffer->skb = skb;
+               skb = desc_data->skb_top;
+               desc_data->skb_top = NULL;
+               DWC_ETH_QOS_consume_page_split_hdr(buffer, skb,
+                         payload_len, buf2_used);
+            } else {
+               /* no chain, got both FD + LD together */
+               if (hdr_len) {
+                  buf2_used = 1;
+                  /* add header len to first skb->len */
+                  skb_put(skb, hdr_len);
+
+                  payload_len = pkt_len - hdr_len;
+                  skb_fill_page_desc(skb, 0,
+                     buffer->page2, 0,
+                     payload_len);
+               } else {
+                  /* No split header, hence
+                   * payload_len = (payload + hdr_len)
+                   * */
+                  if (pkt_len > buffer->rx_hdr_size) {
+                     buf2_used = 1;
+                     /* add header len to first skb->len */
+                     skb_put(skb, buffer->rx_hdr_size);
+
+                     payload_len = (pkt_len - buffer->rx_hdr_size);
+                     skb_fill_page_desc(skb, 0,
+                        buffer->page2, 0,
+                        payload_len);
+                  } else {
+                     buf2_used = 0;
+                     /* add header len to first skb->len */
+                     skb_put(skb, pkt_len);
+                     payload_len = 0; /* no data in page2 */
+                  }
+               }
+               DWC_ETH_QOS_consume_page_split_hdr(buffer,
+                     skb, payload_len,
+                     buf2_used);
+            }
+            /* reset for next new packet/frame */
+            intermediate_desc_cnt = 0;
+            hdr_len = 0;
+         }
 
-			DWC_ETH_QOS_config_rx_csum(pdata, skb, RX_NORMAL_DESC);
+         DWC_ETH_QOS_config_rx_csum(pdata, skb, RX_NORMAL_DESC);
 
 #ifdef DWC_ETH_QOS_ENABLE_VLAN_TAG
-			DWC_ETH_QOS_get_rx_vlan(pdata, skb, RX_NORMAL_DESC);
+         DWC_ETH_QOS_get_rx_vlan(pdata, skb, RX_NORMAL_DESC);
 #endif
 
 #ifdef YDEBUG_FILTER
-			DWC_ETH_QOS_check_rx_filter_status(RX_NORMAL_DESC);
+         DWC_ETH_QOS_check_rx_filter_status(RX_NORMAL_DESC);
 #endif
 
-			if ((pdata->hw_feat.tsstssel) && (pdata->hwts_rx_en)) {
-				/* get rx tstamp if available */
-				if (hw_if->rx_tstamp_available(RX_NORMAL_DESC)) {
-					ret = DWC_ETH_QOS_get_rx_hwtstamp(pdata,
-							skb, desc_data, qInx);
-					if (ret == 0) {
-						/* device has not yet updated the CONTEXT desc to hold the
-						 * time stamp, hence delay the packet reception
-						 * */
-						buffer->skb = skb;
-						buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
-								pdata->rx_buffer_len, DMA_FROM_DEVICE);
-						if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
-							printk(KERN_ALERT "failed to do the RX dma map\n");
-
-						goto rx_tstmp_failed;
-					}
-				}
-			}
-
-			if (!(dev->features & NETIF_F_GRO) &&
-						(dev->features & NETIF_F_LRO)) {
-					pdata->tcp_pkt =
-							DWC_ETH_QOS_check_for_tcp_payload(RX_NORMAL_DESC);
-			}
-
-			dev->last_rx = jiffies;
-			/* update the statistics */
-			dev->stats.rx_packets++;
-			dev->stats.rx_bytes += skb->len;
-			DWC_ETH_QOS_receive_skb(pdata, dev, skb, qInx);
-			received++;
+         if ((pdata->hw_feat.tsstssel) && (pdata->hwts_rx_en)) {
+            /* get rx tstamp if available */
+            if (hw_if->rx_tstamp_available(RX_NORMAL_DESC)) {
+               ret = DWC_ETH_QOS_get_rx_hwtstamp(pdata,
+                     skb, desc_data, qInx);
+               if (ret == 0) {
+                  /* device has not yet updated the CONTEXT desc to hold the
+                   * time stamp, hence delay the packet reception
+                   * */
+                  buffer->skb = skb;
+                  buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
+                        pdata->rx_buffer_len, DMA_FROM_DEVICE);
+                  if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
+                     printk(KERN_ALERT "failed to do the RX dma map\n");
+
+                  goto rx_tstmp_failed;
+               }
+            }
+         }
+
+         if (!(dev->features & NETIF_F_GRO) &&
+                  (dev->features & NETIF_F_LRO)) {
+               pdata->tcp_pkt =
+                     DWC_ETH_QOS_check_for_tcp_payload(RX_NORMAL_DESC);
+         }
+
+         dev->last_rx = jiffies;
+         /* update the statistics */
+         dev->stats.rx_packets++;
+         dev->stats.rx_bytes += skb->len;
+         DWC_ETH_QOS_receive_skb(pdata, dev, skb, qInx);
+         received++;
  next_desc:
-			desc_data->dirty_rx++;
-			if (desc_data->dirty_rx >= desc_data->skb_realloc_threshold)
-				desc_if->realloc_skb(pdata, qInx);
-
-			INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
-			buf2_used = 0;
-		} else {
-			/* no more data to read */
-			break;
-		}
-	}
+         desc_data->dirty_rx++;
+         if (desc_data->dirty_rx >= desc_data->skb_realloc_threshold)
+            desc_if->realloc_skb(pdata, qInx);
+
+         INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
+         buf2_used = 0;
+      } else {
+         /* no more data to read */
+         break;
+      }
+   }
 
 rx_tstmp_failed:
 
-	if (desc_data->dirty_rx)
-		desc_if->realloc_skb(pdata, qInx);
+   if (desc_data->dirty_rx)
+      desc_if->realloc_skb(pdata, qInx);
 
-	DBGPR("<--DWC_ETH_QOS_clean_split_hdr_rx_irq: received = %d\n",
-		received);
+   DBGPR("<--DWC_ETH_QOS_clean_split_hdr_rx_irq: received = %d\n",
+      received);
 
-	return received;
+   return received;
 }
 
 /*!
@@ -2901,259 +2927,259 @@ rx_tstmp_failed:
 * \retval number of packets received.
 */
 static int DWC_ETH_QOS_clean_jumbo_rx_irq(struct DWC_ETH_QOS_prv_data *pdata,
-					  int quota,
-					  uint32_t qInx)
+                 int quota,
+                 uint32_t qInx)
 {
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
-	    GET_RX_WRAPPER_DESC(qInx);
-	struct net_device *dev = pdata->dev;
-	struct desc_if_struct *desc_if = &pdata->desc_if;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct sk_buff *skb = NULL;
-	int received = 0;
-	struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
-	rx_descriptor_t *RX_NORMAL_DESC = NULL;
-	u16 pkt_len;
-	uint8_t intermediate_desc_cnt = 0;
-	unsigned int buf2_used;
-	int ret;
-
-	DBGPR("-->DWC_ETH_QOS_clean_jumbo_rx_irq: qInx = %u, quota = %d\n",
-		qInx, quota);
-
-	while (received < quota) {
-		buffer = GET_RX_BUF_PTR(qInx, desc_data->cur_rx);
-		RX_NORMAL_DESC = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
+       GET_RX_WRAPPER_DESC(qInx);
+   struct net_device *dev = pdata->dev;
+   struct desc_if_struct *desc_if = &pdata->desc_if;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct sk_buff *skb = NULL;
+   int received = 0;
+   struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
+   rx_descriptor_t *RX_NORMAL_DESC = NULL;
+   u16 pkt_len;
+   uint8_t intermediate_desc_cnt = 0;
+   unsigned int buf2_used;
+   int ret;
+
+   DBGPR("-->DWC_ETH_QOS_clean_jumbo_rx_irq: qInx = %u, quota = %d\n",
+      qInx, quota);
+
+   while (received < quota) {
+      buffer = GET_RX_BUF_PTR(qInx, desc_data->cur_rx);
+      RX_NORMAL_DESC = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
 
-		/* check for data availability */
-		if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
+      /* check for data availability */
+      if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
 #ifdef DWC_ETH_QOS_ENABLE_RX_DESC_DUMP
-			dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
+         dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
 #endif
-			/* assign it to new skb */
-			skb = buffer->skb;
-			buffer->skb = NULL;
-
-			/* first buffer pointer */
-			dma_unmap_page(&pdata->pdev->dev, buffer->dma,
-				       PAGE_SIZE, DMA_FROM_DEVICE);
-			buffer->dma = 0;
-
-			/* second buffer pointer */
-			dma_unmap_page(&pdata->pdev->dev, buffer->dma2,
-				       PAGE_SIZE, DMA_FROM_DEVICE);
-			buffer->dma2 = 0;
-
-			/* get the packet length */
-			pkt_len =
-				(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_PL);
-
-			/* check for bad packet,
-			 * error is valid only for last descriptor (OWN + LD bit set).
-			 * */
-			if ((RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_ES) &&
-			    (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
-				DBGPR("Error in rcved pkt, failed to pass it to upper layer\n");
+         /* assign it to new skb */
+         skb = buffer->skb;
+         buffer->skb = NULL;
+
+         /* first buffer pointer */
+         dma_unmap_page(&pdata->pdev->dev, buffer->dma,
+                   PAGE_SIZE, DMA_FROM_DEVICE);
+         buffer->dma = 0;
+
+         /* second buffer pointer */
+         dma_unmap_page(&pdata->pdev->dev, buffer->dma2,
+                   PAGE_SIZE, DMA_FROM_DEVICE);
+         buffer->dma2 = 0;
+
+         /* get the packet length */
+         pkt_len =
+            (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_PL);
+
+         /* check for bad packet,
+          * error is valid only for last descriptor (OWN + LD bit set).
+          * */
+         if ((RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_ES) &&
+             (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
+            DBGPR("Error in rcved pkt, failed to pass it to upper layer\n");
 #ifdef DWC_ETH_QOS_ENABLE_RX_DESC_DUMP
-				dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
+            dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
 #endif
-				dev->stats.rx_errors++;
-				DWC_ETH_QOS_update_rx_errors(dev,
-					RX_NORMAL_DESC->RDES3);
-
-				/* recycle both page and skb */
-				buffer->skb = skb;
-				if (desc_data->skb_top)
-					dev_kfree_skb_any(desc_data->skb_top);
-
-				desc_data->skb_top = NULL;
-				goto next_desc;
-			}
-
-			if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
-				intermediate_desc_cnt++;
-				buf2_used = 1;
-				/* this descriptor is only the beginning/middle */
-				if (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD) {
-					/* this is the beginning of a chain */
-					desc_data->skb_top = skb;
-					skb_fill_page_desc(skb, 0,
-						buffer->page, 0,
-						pdata->rx_buffer_len);
-
-					DBGPR("RX: pkt in second buffer pointer\n");
-					skb_fill_page_desc(
-						desc_data->skb_top,
-						skb_shinfo(desc_data->skb_top)->nr_frags,
-						buffer->page2, 0,
-						pdata->rx_buffer_len);
-				} else {
-					/* this is the middle of a chain */
-					skb_fill_page_desc(desc_data->skb_top,
-						skb_shinfo(desc_data->skb_top)->nr_frags,
-						buffer->page, 0,
-						pdata->rx_buffer_len);
-
-					DBGPR("RX: pkt in second buffer pointer\n");
-					skb_fill_page_desc(desc_data->skb_top,
-						skb_shinfo(desc_data->skb_top)->nr_frags,
-						buffer->page2, 0,
-						pdata->rx_buffer_len);
-					/* re-use this skb, as consumed only the page */
-					buffer->skb = skb;
-				}
-				DWC_ETH_QOS_consume_page(buffer,
-							 desc_data->skb_top,
-							 (pdata->rx_buffer_len * 2),
-							 buf2_used);
-				goto next_desc;
-			} else {
-				if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD)) {
-					/* end of the chain */
-					pkt_len =
-						(pkt_len - (pdata->rx_buffer_len * intermediate_desc_cnt));
-					if (pkt_len > pdata->rx_buffer_len) {
-						skb_fill_page_desc(desc_data->skb_top,
-							skb_shinfo(desc_data->skb_top)->nr_frags,
-							buffer->page, 0,
-							pdata->rx_buffer_len);
-
-						DBGPR("RX: pkt in second buffer pointer\n");
-						skb_fill_page_desc(desc_data->skb_top,
-							skb_shinfo(desc_data->skb_top)->nr_frags,
-							buffer->page2, 0,
-							(pkt_len - pdata->rx_buffer_len));
-						buf2_used = 1;
-					} else {
-						skb_fill_page_desc(desc_data->skb_top,
-							skb_shinfo(desc_data->skb_top)->nr_frags,
-							buffer->page, 0,
-							pkt_len);
-						buf2_used = 0;
-					}
-					/* re-use this skb, as consumed only the page */
-					buffer->skb = skb;
-					skb = desc_data->skb_top;
-					desc_data->skb_top = NULL;
-					DWC_ETH_QOS_consume_page(buffer, skb,
-								 pkt_len,
-								 buf2_used);
-				} else {
-					/* no chain, got both FD + LD together */
-
-					/* code added for copybreak, this should improve
-					 * performance for small pkts with large amount
-					 * of reassembly being done in the stack
-					 * */
-					if ((pkt_len <= DWC_ETH_QOS_COPYBREAK_DEFAULT)
-					    && (skb_tailroom(skb) >= pkt_len)) {
-						u8 *vaddr;
-						vaddr =
-						    kmap_atomic(buffer->page);
-						memcpy(skb_tail_pointer(skb),
-						       vaddr, pkt_len);
-						kunmap_atomic(vaddr);
-						/* re-use the page, so don't erase buffer->page/page2 */
-						skb_put(skb, pkt_len);
-					} else {
-						if (pkt_len > pdata->rx_buffer_len) {
-							skb_fill_page_desc(skb,
-								0, buffer->page,
-								0,
-								pdata->rx_buffer_len);
-
-							DBGPR ("RX: pkt in second buffer pointer\n");
-							skb_fill_page_desc(skb,
-								skb_shinfo(skb)->nr_frags, buffer->page2,
-								0,
-								(pkt_len - pdata->rx_buffer_len));
-							buf2_used = 1;
-						} else {
-							skb_fill_page_desc(skb,
-								0, buffer->page,
-								0,
-								pkt_len);
-							buf2_used = 0;
-						}
-						DWC_ETH_QOS_consume_page(buffer,
-								skb,
-								pkt_len,
-								buf2_used);
-					}
-				}
-				intermediate_desc_cnt = 0;
-			}
+            dev->stats.rx_errors++;
+            DWC_ETH_QOS_update_rx_errors(dev,
+               RX_NORMAL_DESC->RDES3);
+
+            /* recycle both page and skb */
+            buffer->skb = skb;
+            if (desc_data->skb_top)
+               dev_kfree_skb_any(desc_data->skb_top);
+
+            desc_data->skb_top = NULL;
+            goto next_desc;
+         }
+
+         if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
+            intermediate_desc_cnt++;
+            buf2_used = 1;
+            /* this descriptor is only the beginning/middle */
+            if (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD) {
+               /* this is the beginning of a chain */
+               desc_data->skb_top = skb;
+               skb_fill_page_desc(skb, 0,
+                  buffer->page, 0,
+                  pdata->rx_buffer_len);
+
+               DBGPR("RX: pkt in second buffer pointer\n");
+               skb_fill_page_desc(
+                  desc_data->skb_top,
+                  skb_shinfo(desc_data->skb_top)->nr_frags,
+                  buffer->page2, 0,
+                  pdata->rx_buffer_len);
+            } else {
+               /* this is the middle of a chain */
+               skb_fill_page_desc(desc_data->skb_top,
+                  skb_shinfo(desc_data->skb_top)->nr_frags,
+                  buffer->page, 0,
+                  pdata->rx_buffer_len);
+
+               DBGPR("RX: pkt in second buffer pointer\n");
+               skb_fill_page_desc(desc_data->skb_top,
+                  skb_shinfo(desc_data->skb_top)->nr_frags,
+                  buffer->page2, 0,
+                  pdata->rx_buffer_len);
+               /* re-use this skb, as consumed only the page */
+               buffer->skb = skb;
+            }
+            DWC_ETH_QOS_consume_page(buffer,
+                      desc_data->skb_top,
+                      (pdata->rx_buffer_len * 2),
+                      buf2_used);
+            goto next_desc;
+         } else {
+            if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_FD)) {
+               /* end of the chain */
+               pkt_len =
+                  (pkt_len - (pdata->rx_buffer_len * intermediate_desc_cnt));
+               if (pkt_len > pdata->rx_buffer_len) {
+                  skb_fill_page_desc(desc_data->skb_top,
+                     skb_shinfo(desc_data->skb_top)->nr_frags,
+                     buffer->page, 0,
+                     pdata->rx_buffer_len);
+
+                  DBGPR("RX: pkt in second buffer pointer\n");
+                  skb_fill_page_desc(desc_data->skb_top,
+                     skb_shinfo(desc_data->skb_top)->nr_frags,
+                     buffer->page2, 0,
+                     (pkt_len - pdata->rx_buffer_len));
+                  buf2_used = 1;
+               } else {
+                  skb_fill_page_desc(desc_data->skb_top,
+                     skb_shinfo(desc_data->skb_top)->nr_frags,
+                     buffer->page, 0,
+                     pkt_len);
+                  buf2_used = 0;
+               }
+               /* re-use this skb, as consumed only the page */
+               buffer->skb = skb;
+               skb = desc_data->skb_top;
+               desc_data->skb_top = NULL;
+               DWC_ETH_QOS_consume_page(buffer, skb,
+                         pkt_len,
+                         buf2_used);
+            } else {
+               /* no chain, got both FD + LD together */
+
+               /* code added for copybreak, this should improve
+                * performance for small pkts with large amount
+                * of reassembly being done in the stack
+                * */
+               if ((pkt_len <= DWC_ETH_QOS_COPYBREAK_DEFAULT)
+                   && (skb_tailroom(skb) >= pkt_len)) {
+                  u8 *vaddr;
+                  vaddr =
+                      kmap_atomic(buffer->page);
+                  memcpy(skb_tail_pointer(skb),
+                         vaddr, pkt_len);
+                  kunmap_atomic(vaddr);
+                  /* re-use the page, so don't erase buffer->page/page2 */
+                  skb_put(skb, pkt_len);
+               } else {
+                  if (pkt_len > pdata->rx_buffer_len) {
+                     skb_fill_page_desc(skb,
+                        0, buffer->page,
+                        0,
+                        pdata->rx_buffer_len);
+
+                     DBGPR ("RX: pkt in second buffer pointer\n");
+                     skb_fill_page_desc(skb,
+                        skb_shinfo(skb)->nr_frags, buffer->page2,
+                        0,
+                        (pkt_len - pdata->rx_buffer_len));
+                     buf2_used = 1;
+                  } else {
+                     skb_fill_page_desc(skb,
+                        0, buffer->page,
+                        0,
+                        pkt_len);
+                     buf2_used = 0;
+                  }
+                  DWC_ETH_QOS_consume_page(buffer,
+                        skb,
+                        pkt_len,
+                        buf2_used);
+               }
+            }
+            intermediate_desc_cnt = 0;
+         }
 
-			DWC_ETH_QOS_config_rx_csum(pdata, skb, RX_NORMAL_DESC);
+         DWC_ETH_QOS_config_rx_csum(pdata, skb, RX_NORMAL_DESC);
 
 #ifdef DWC_ETH_QOS_ENABLE_VLAN_TAG
-			DWC_ETH_QOS_get_rx_vlan(pdata, skb, RX_NORMAL_DESC);
+         DWC_ETH_QOS_get_rx_vlan(pdata, skb, RX_NORMAL_DESC);
 #endif
 
 #ifdef YDEBUG_FILTER
-			DWC_ETH_QOS_check_rx_filter_status(RX_NORMAL_DESC);
+         DWC_ETH_QOS_check_rx_filter_status(RX_NORMAL_DESC);
 #endif
 
-			if ((pdata->hw_feat.tsstssel) && (pdata->hwts_rx_en)) {
-				/* get rx tstamp if available */
-				if (hw_if->rx_tstamp_available(RX_NORMAL_DESC)) {
-					ret = DWC_ETH_QOS_get_rx_hwtstamp(pdata,
-							skb, desc_data, qInx);
-					if (ret == 0) {
-						/* device has not yet updated the CONTEXT desc to hold the
-						 * time stamp, hence delay the packet reception
-						 * */
-						buffer->skb = skb;
-						buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
-								pdata->rx_buffer_len, DMA_FROM_DEVICE);
-						if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
-							printk(KERN_ALERT "failed to do the RX dma map\n");
-
-						goto rx_tstmp_failed;
-					}
-				}
-			}
-
-			if (!(dev->features & NETIF_F_GRO) &&
-						(dev->features & NETIF_F_LRO)) {
-					pdata->tcp_pkt =
-							DWC_ETH_QOS_check_for_tcp_payload(RX_NORMAL_DESC);
-			}
-
-			dev->last_rx = jiffies;
-			/* update the statistics */
-			dev->stats.rx_packets++;
-			dev->stats.rx_bytes += skb->len;
-
-			/* eth type trans needs skb->data to point to something */
-			if (!pskb_may_pull(skb, ETH_HLEN)) {
-				printk(KERN_ALERT "pskb_may_pull failed\n");
-				dev_kfree_skb_any(skb);
-				goto next_desc;
-			}
+         if ((pdata->hw_feat.tsstssel) && (pdata->hwts_rx_en)) {
+            /* get rx tstamp if available */
+            if (hw_if->rx_tstamp_available(RX_NORMAL_DESC)) {
+               ret = DWC_ETH_QOS_get_rx_hwtstamp(pdata,
+                     skb, desc_data, qInx);
+               if (ret == 0) {
+                  /* device has not yet updated the CONTEXT desc to hold the
+                   * time stamp, hence delay the packet reception
+                   * */
+                  buffer->skb = skb;
+                  buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
+                        pdata->rx_buffer_len, DMA_FROM_DEVICE);
+                  if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
+                     printk(KERN_ALERT "failed to do the RX dma map\n");
+
+                  goto rx_tstmp_failed;
+               }
+            }
+         }
+
+         if (!(dev->features & NETIF_F_GRO) &&
+                  (dev->features & NETIF_F_LRO)) {
+               pdata->tcp_pkt =
+                     DWC_ETH_QOS_check_for_tcp_payload(RX_NORMAL_DESC);
+         }
+
+         dev->last_rx = jiffies;
+         /* update the statistics */
+         dev->stats.rx_packets++;
+         dev->stats.rx_bytes += skb->len;
+
+         /* eth type trans needs skb->data to point to something */
+         if (!pskb_may_pull(skb, ETH_HLEN)) {
+            printk(KERN_ALERT "pskb_may_pull failed\n");
+            dev_kfree_skb_any(skb);
+            goto next_desc;
+         }
 
-			DWC_ETH_QOS_receive_skb(pdata, dev, skb, qInx);
-			received++;
+         DWC_ETH_QOS_receive_skb(pdata, dev, skb, qInx);
+         received++;
  next_desc:
-			desc_data->dirty_rx++;
-			if (desc_data->dirty_rx >= desc_data->skb_realloc_threshold)
-				desc_if->realloc_skb(pdata, qInx);
-
-			INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
-		} else {
-			/* no more data to read */
-			break;
-		}
-	}
+         desc_data->dirty_rx++;
+         if (desc_data->dirty_rx >= desc_data->skb_realloc_threshold)
+            desc_if->realloc_skb(pdata, qInx);
+
+         INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
+      } else {
+         /* no more data to read */
+         break;
+      }
+   }
 
 rx_tstmp_failed:
 
-	if (desc_data->dirty_rx)
-		desc_if->realloc_skb(pdata, qInx);
+   if (desc_data->dirty_rx)
+      desc_if->realloc_skb(pdata, qInx);
 
-	DBGPR("<--DWC_ETH_QOS_clean_jumbo_rx_irq: received = %d\n", received);
+   DBGPR("<--DWC_ETH_QOS_clean_jumbo_rx_irq: received = %d\n", received);
 
-	return received;
+   return received;
 }
 
 /*!
@@ -3175,151 +3201,151 @@ rx_tstmp_failed:
 * \retval number of packets received.
 */
 static int DWC_ETH_QOS_clean_rx_irq(struct DWC_ETH_QOS_prv_data *pdata,
-				    int quota,
-				    uint32_t qInx)
+                int quota,
+                uint32_t qInx)
 {
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
-	    GET_RX_WRAPPER_DESC(qInx);
-	struct net_device *dev = pdata->dev;
-	struct desc_if_struct *desc_if = &pdata->desc_if;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct sk_buff *skb = NULL;
-	int received = 0;
-	struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
-	rx_descriptor_t *RX_NORMAL_DESC = NULL;
-	uint32_t pkt_len;
-	int ret;
-
-	DBGPR("-->DWC_ETH_QOS_clean_rx_irq: qInx = %u, quota = %d\n",
-		qInx, quota);
-
-	while (received < quota) {
-		buffer = GET_RX_BUF_PTR(qInx, desc_data->cur_rx);
-		RX_NORMAL_DESC = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *desc_data =
+       GET_RX_WRAPPER_DESC(qInx);
+   struct net_device *dev = pdata->dev;
+   struct desc_if_struct *desc_if = &pdata->desc_if;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct sk_buff *skb = NULL;
+   int received = 0;
+   struct DWC_ETH_QOS_rx_buffer *buffer = NULL;
+   rx_descriptor_t *RX_NORMAL_DESC = NULL;
+   uint32_t pkt_len;
+   int ret;
+
+   DBGPR("-->DWC_ETH_QOS_clean_rx_irq: qInx = %u, quota = %d\n",
+      qInx, quota);
+
+   while (received < quota) {
+      buffer = GET_RX_BUF_PTR(qInx, desc_data->cur_rx);
+      RX_NORMAL_DESC = GET_RX_DESC_PTR(qInx, desc_data->cur_rx);
 
-		/* check for data availability */
-		if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
+      /* check for data availability */
+      if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
 #ifdef DWC_ETH_QOS_ENABLE_RX_DESC_DUMP
-			dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
+         dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
 #endif
-			/* assign it to new skb */
-			skb = buffer->skb;
-			buffer->skb = NULL;
-			dma_unmap_single(&pdata->pdev->dev, buffer->dma,
-					 pdata->rx_buffer_len, DMA_FROM_DEVICE);
-			buffer->dma = 0;
-
-			/* get the packet length */
-			pkt_len = (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_PL);
-
-			/* check for bad/oversized packet,
-			 * error is valid only for last descriptor (OWN + LD bit set).
-			 * */
-			if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_ES) &&
-			    (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
-				/* pkt_len = pkt_len - 4; */ /* CRC stripping */
-				/* code added for copybreak, this should improve
-				 * performance for small pkts with large amount
-				 * of reassembly being done in the stack
-				 * */
+         /* assign it to new skb */
+         skb = buffer->skb;
+         buffer->skb = NULL;
+         dma_unmap_single(&pdata->pdev->dev, buffer->dma,
+                pdata->rx_buffer_len, DMA_FROM_DEVICE);
+         buffer->dma = 0;
+
+         /* get the packet length */
+         pkt_len = (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_PL);
+
+         /* check for bad/oversized packet,
+          * error is valid only for last descriptor (OWN + LD bit set).
+          * */
+         if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_ES) &&
+             (RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD)) {
+            /* pkt_len = pkt_len - 4; */ /* CRC stripping */
+            /* code added for copybreak, this should improve
+             * performance for small pkts with large amount
+             * of reassembly being done in the stack
+             * */
 // Disable this logic to avoid conflict with not updated timestamp logic below
 #if 0
-				if (pkt_len < DWC_ETH_QOS_COPYBREAK_DEFAULT) {
-					struct sk_buff *new_skb =
-					    netdev_alloc_skb_ip_align(dev, pkt_len);
-					if (new_skb) {
-						skb_copy_to_linear_data_offset(new_skb,
-							-NET_IP_ALIGN,
-							(skb->data - NET_IP_ALIGN),
-							(pkt_len + NET_IP_ALIGN));
-						/* recycle actual desc skb */
-						buffer->skb = skb;
-						skb = new_skb;
-					} else {
-						/* just continue with the old skb */
-					}
-				}
+            if (pkt_len < DWC_ETH_QOS_COPYBREAK_DEFAULT) {
+               struct sk_buff *new_skb =
+                   netdev_alloc_skb_ip_align(dev, pkt_len);
+               if (new_skb) {
+                  skb_copy_to_linear_data_offset(new_skb,
+                     -NET_IP_ALIGN,
+                     (skb->data - NET_IP_ALIGN),
+                     (pkt_len + NET_IP_ALIGN));
+                  /* recycle actual desc skb */
+                  buffer->skb = skb;
+                  skb = new_skb;
+               } else {
+                  /* just continue with the old skb */
+               }
+            }
 #endif
-				skb_put(skb, pkt_len);
+            skb_put(skb, pkt_len);
 #ifdef GBE_DEBUG
-				if(print_rx_pkts)
-					print_skb(skb, true);
+            if(print_rx_pkts)
+               print_skb(skb, true);
 #endif
-				DWC_ETH_QOS_config_rx_csum(pdata, skb, RX_NORMAL_DESC);
+            DWC_ETH_QOS_config_rx_csum(pdata, skb, RX_NORMAL_DESC);
 
 #ifdef DWC_ETH_QOS_ENABLE_VLAN_TAG
-				DWC_ETH_QOS_get_rx_vlan(pdata, skb, RX_NORMAL_DESC);
+            DWC_ETH_QOS_get_rx_vlan(pdata, skb, RX_NORMAL_DESC);
 #endif
 
 #ifdef YDEBUG_FILTER
-				DWC_ETH_QOS_check_rx_filter_status(RX_NORMAL_DESC);
+            DWC_ETH_QOS_check_rx_filter_status(RX_NORMAL_DESC);
 #endif
 
-				if ((pdata->hw_feat.tsstssel) && (pdata->hwts_rx_en)) {
-					/* get rx tstamp if available */
-					if (hw_if->rx_tstamp_available(RX_NORMAL_DESC)) {
-						ret = DWC_ETH_QOS_get_rx_hwtstamp(pdata,
-								skb, desc_data, qInx);
-						if (ret == 0) {
-							/* device has not yet updated the CONTEXT desc to hold the
-							 * time stamp, hence delay the packet reception
-							 * */
-							buffer->skb = skb; //Potential conflict when above COPYBREAK logic is enabled!!!
-							buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
-									pdata->rx_buffer_len, DMA_FROM_DEVICE);
-							if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
-								printk(KERN_ALERT "failed to do the RX dma map\n");
-
-							goto rx_tstmp_failed;
-						}
-					}
-				}
-
-				if (!(dev->features & NETIF_F_GRO) &&
-						(dev->features & NETIF_F_LRO)) {
-						pdata->tcp_pkt =
-								DWC_ETH_QOS_check_for_tcp_payload(RX_NORMAL_DESC);
-				}
-
-				dev->last_rx = jiffies;
-				/* update the statistics */
-				dev->stats.rx_packets++;
-				dev->stats.rx_bytes += skb->len;
-				DWC_ETH_QOS_receive_skb(pdata, dev, skb, qInx);
-				received++;
-			} else {
+            if ((pdata->hw_feat.tsstssel) && (pdata->hwts_rx_en)) {
+               /* get rx tstamp if available */
+               if (hw_if->rx_tstamp_available(RX_NORMAL_DESC)) {
+                  ret = DWC_ETH_QOS_get_rx_hwtstamp(pdata,
+                        skb, desc_data, qInx);
+                  if (ret == 0) {
+                     /* device has not yet updated the CONTEXT desc to hold the
+                      * time stamp, hence delay the packet reception
+                      * */
+                     buffer->skb = skb; //Potential conflict when above COPYBREAK logic is enabled!!!
+                     buffer->dma = dma_map_single(&pdata->pdev->dev, skb->data,
+                           pdata->rx_buffer_len, DMA_FROM_DEVICE);
+                     if (dma_mapping_error(&pdata->pdev->dev, buffer->dma))
+                        printk(KERN_ALERT "failed to do the RX dma map\n");
+
+                     goto rx_tstmp_failed;
+                  }
+               }
+            }
+
+            if (!(dev->features & NETIF_F_GRO) &&
+                  (dev->features & NETIF_F_LRO)) {
+                  pdata->tcp_pkt =
+                        DWC_ETH_QOS_check_for_tcp_payload(RX_NORMAL_DESC);
+            }
+
+            dev->last_rx = jiffies;
+            /* update the statistics */
+            dev->stats.rx_packets++;
+            dev->stats.rx_bytes += skb->len;
+            DWC_ETH_QOS_receive_skb(pdata, dev, skb, qInx);
+            received++;
+         } else {
 #ifdef DWC_ETH_QOS_ENABLE_RX_DESC_DUMP
-				dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
+            dump_rx_desc(qInx, RX_NORMAL_DESC, desc_data->cur_rx);
 #endif
-				if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD))
-					DBGPR("Received oversized pkt, spanned across multiple desc\n");
+            if (!(RX_NORMAL_DESC->RDES3 & DWC_ETH_QOS_RDESC3_LD))
+               DBGPR("Received oversized pkt, spanned across multiple desc\n");
 
-				/* recycle skb */
-				buffer->skb = skb;
-				dev->stats.rx_errors++;
-				DWC_ETH_QOS_update_rx_errors(dev,
-					RX_NORMAL_DESC->RDES3);
-			}
-
-			desc_data->dirty_rx++;
-			if (desc_data->dirty_rx >= desc_data->skb_realloc_threshold)
-				desc_if->realloc_skb(pdata, qInx);
-
-			INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
-		} else {
-			/* no more data to read */
-			break;
-		}
-	}
+            /* recycle skb */
+            buffer->skb = skb;
+            dev->stats.rx_errors++;
+            DWC_ETH_QOS_update_rx_errors(dev,
+               RX_NORMAL_DESC->RDES3);
+         }
+
+         desc_data->dirty_rx++;
+         if (desc_data->dirty_rx >= desc_data->skb_realloc_threshold)
+            desc_if->realloc_skb(pdata, qInx);
+
+         INCR_RX_DESC_INDEX(desc_data->cur_rx, 1);
+      } else {
+         /* no more data to read */
+         break;
+      }
+   }
 
 rx_tstmp_failed:
 
-	if (desc_data->dirty_rx)
-		desc_if->realloc_skb(pdata, qInx);
+   if (desc_data->dirty_rx)
+      desc_if->realloc_skb(pdata, qInx);
 
-	DBGPR("<--DWC_ETH_QOS_clean_rx_irq: received = %d\n", received);
+   DBGPR("<--DWC_ETH_QOS_clean_rx_irq: received = %d\n", received);
 
-	return received;
+   return received;
 }
 
 /*!
@@ -3334,23 +3360,23 @@ rx_tstmp_failed:
 * \return void.
 */
 void DWC_ETH_QOS_update_rx_errors(struct net_device *dev,
-				 unsigned int rx_status)
+             unsigned int rx_status)
 {
-	DBGPR("-->DWC_ETH_QOS_update_rx_errors\n");
+   DBGPR("-->DWC_ETH_QOS_update_rx_errors\n");
 
-	/* received pkt with crc error */
-	if ((rx_status & 0x1000000))
-		dev->stats.rx_crc_errors++;
+   /* received pkt with crc error */
+   if ((rx_status & 0x1000000))
+      dev->stats.rx_crc_errors++;
 
-	/* received frame alignment */
-	if ((rx_status & 0x100000))
-		dev->stats.rx_frame_errors++;
+   /* received frame alignment */
+   if ((rx_status & 0x100000))
+      dev->stats.rx_frame_errors++;
 
-	/* receiver fifo overrun */
-	if ((rx_status & 0x200000))
-		dev->stats.rx_fifo_errors++;
+   /* receiver fifo overrun */
+   if ((rx_status & 0x200000))
+      dev->stats.rx_fifo_errors++;
 
-	DBGPR("<--DWC_ETH_QOS_update_rx_errors\n");
+   DBGPR("<--DWC_ETH_QOS_update_rx_errors\n");
 }
 
 /*!
@@ -3369,62 +3395,62 @@ void DWC_ETH_QOS_update_rx_errors(struct
 */
 int DWC_ETH_QOS_poll_rx(struct napi_struct *napi, int budget)
 {
-	struct DWC_ETH_QOS_prv_data *pdata =
-		container_of(napi, struct DWC_ETH_QOS_prv_data, rx_napi);
-	struct DWC_ETH_QOS_rx_queue *rx_queue = NULL;
-	/* divide the budget evenly among all the queues */
-	int per_q_budget = budget / DWC_ETH_QOS_RX_QUEUE_CNT;
-	int qInx = 0;
-	int received = 0, per_q_received = 0;
-
-	DBGPR("-->DWC_ETH_QOS_poll_mq: budget = %d\n", budget);
-
-	pdata->xstats.napi_poll_n++;
-	for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
-		rx_queue = GET_RX_QUEUE_PTR(qInx);
+   struct DWC_ETH_QOS_prv_data *pdata =
+      container_of(napi, struct DWC_ETH_QOS_prv_data, rx_napi);
+   struct DWC_ETH_QOS_rx_queue *rx_queue = NULL;
+   /* divide the budget evenly among all the queues */
+   int per_q_budget = budget / DWC_ETH_QOS_RX_QUEUE_CNT;
+   int qInx = 0;
+   int received = 0, per_q_received = 0;
+
+   DBGPR("-->DWC_ETH_QOS_poll_mq: budget = %d\n", budget);
+
+   pdata->xstats.napi_poll_n++;
+   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
+      rx_queue = GET_RX_QUEUE_PTR(qInx);
 
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-		/* check for tx descriptor status */
-		DWC_ETH_QOS_tx_interrupt(pdata->dev, pdata, qInx);
+      /* check for tx descriptor status */
+      DWC_ETH_QOS_tx_interrupt(pdata->dev, pdata, qInx);
 #endif
-		rx_queue->lro_flush_needed = 0;
+      rx_queue->lro_flush_needed = 0;
 
 #ifdef RX_OLD_CODE
-		per_q_received = DWC_ETH_QOS_poll(pdata, per_q_budget, qInx);
+      per_q_received = DWC_ETH_QOS_poll(pdata, per_q_budget, qInx);
 #else
-		per_q_received = pdata->clean_rx(pdata, per_q_budget, qInx);
+      per_q_received = pdata->clean_rx(pdata, per_q_budget, qInx);
 #endif
-		received += per_q_received;
-		pdata->xstats.rx_pkt_n += per_q_received;
-		pdata->xstats.q_rx_pkt_n[qInx] += per_q_received;
-
-		if (rx_queue->lro_flush_needed)
-			lro_flush_all(&rx_queue->lro_mgr);
-	}
-
-	/* If we processed all pkts, we are done;
-	 * tell the kernel & re-enable interrupt */
-	if (received < budget) {
-		unsigned long flags;
-		/* Turn off polling */
-		if (pdata->dev->features & NETIF_F_GRO)
-			napi_complete(napi);
-		else
-			__napi_complete(napi);
-		spin_lock_irqsave(&pdata->lock, flags);
-		/* Enable Rx interrupts */
-		DWC_ETH_QOS_enable_rx_interrupts(pdata);
+      received += per_q_received;
+      pdata->xstats.rx_pkt_n += per_q_received;
+      pdata->xstats.q_rx_pkt_n[qInx] += per_q_received;
+
+      if (rx_queue->lro_flush_needed)
+         lro_flush_all(&rx_queue->lro_mgr);
+   }
+
+   /* If we processed all pkts, we are done;
+    * tell the kernel & re-enable interrupt */
+   if (received < budget) {
+      unsigned long flags;
+      /* Turn off polling */
+      if (pdata->dev->features & NETIF_F_GRO)
+         napi_complete(napi);
+      else
+         __napi_complete(napi);
+      spin_lock_irqsave(&pdata->lock, flags);
+      /* Enable Rx interrupts */
+      DWC_ETH_QOS_enable_rx_interrupts(pdata);
 #ifdef DWC_ETH_QOS_TXPOLLING_MODE_ENABLE
-		/* Enable Tx interrupts */
-		DWC_ETH_QOS_enable_tx_interrupts(pdata);
+      /* Enable Tx interrupts */
+      DWC_ETH_QOS_enable_tx_interrupts(pdata);
 #endif
-		pdata->rx_napi_pending = false;
-		spin_unlock_irqrestore(&pdata->lock, flags);
-	}
+      pdata->rx_napi_pending = false;
+      spin_unlock_irqrestore(&pdata->lock, flags);
+   }
 
-	DBGPR("<--DWC_ETH_QOS_poll_mq\n");
+   DBGPR("<--DWC_ETH_QOS_poll_mq\n");
 
-	return received;
+   return received;
 }
 
 /*!
@@ -3440,11 +3466,9 @@ int DWC_ETH_QOS_poll_rx(struct napi_stru
 *
 * \retval net_device_stats - returns pointer to net_device_stats structure.
 */
-
 static struct net_device_stats *DWC_ETH_QOS_get_stats(struct net_device *dev)
 {
-
-	return &dev->stats;
+   return &dev->stats;
 }
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
@@ -3461,18 +3485,18 @@ static struct net_device_stats *DWC_ETH_
 */
 static void DWC_ETH_QOS_poll_controller(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
 
-	DBGPR("-->DWC_ETH_QOS_poll_controller\n");
+   DBGPR("-->DWC_ETH_QOS_poll_controller\n");
 
-	disable_irq(pdata->irq_number);
-	DWC_ETH_QOS_ISR_SW_DWC_ETH_QOS(pdata->irq_number, pdata);
-	enable_irq(pdata->irq_number);
+   disable_irq(pdata->irq_number);
+   DWC_ETH_QOS_ISR(pdata->irq_number, pdata);
+   enable_irq(pdata->irq_number);
 
-	DBGPR("<--DWC_ETH_QOS_poll_controller\n");
+   DBGPR("<--DWC_ETH_QOS_poll_controller\n");
 }
 
-#endif	/*end of CONFIG_NET_POLL_CONTROLLER */
+#endif   /*end of CONFIG_NET_POLL_CONTROLLER */
 
 /*!
  * \brief User defined parameter setting API
@@ -3490,55 +3514,55 @@ static void DWC_ETH_QOS_poll_controller(
  */
 static int DWC_ETH_QOS_set_features(struct net_device *dev, netdev_features_t features)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t dev_rxcsum_enable;
-	uint32_t dev_rxvlan_enable, dev_txvlan_enable;
-
-	if (pdata->hw_feat.rx_coe_sel) {
-		dev_rxcsum_enable = !!(pdata->dev_state & NETIF_F_RXCSUM);
-
-		if (((features & NETIF_F_RXCSUM) == NETIF_F_RXCSUM)
-		    && !dev_rxcsum_enable) {
-			hw_if->enable_rx_csum();
-			pdata->dev_state |= NETIF_F_RXCSUM;
-			printk(KERN_ALERT "State change - rxcsum enable\n");
-		} else if (((features & NETIF_F_RXCSUM) == 0)
-			   && dev_rxcsum_enable) {
-			hw_if->disable_rx_csum();
-			pdata->dev_state &= ~NETIF_F_RXCSUM;
-			printk(KERN_ALERT "State change - rxcsum disable\n");
-		}
-	}
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t dev_rxcsum_enable;
+   uint32_t dev_rxvlan_enable, dev_txvlan_enable;
+
+   if (pdata->hw_feat.rx_coe_sel) {
+      dev_rxcsum_enable = !!(pdata->dev_state & NETIF_F_RXCSUM);
+
+      if (((features & NETIF_F_RXCSUM) == NETIF_F_RXCSUM)
+          && !dev_rxcsum_enable) {
+         hw_if->enable_rx_csum();
+         pdata->dev_state |= NETIF_F_RXCSUM;
+         printk(KERN_ALERT "State change - rxcsum enable\n");
+      } else if (((features & NETIF_F_RXCSUM) == 0)
+            && dev_rxcsum_enable) {
+         hw_if->disable_rx_csum();
+         pdata->dev_state &= ~NETIF_F_RXCSUM;
+         printk(KERN_ALERT "State change - rxcsum disable\n");
+      }
+   }
 #ifdef DWC_ETH_QOS_ENABLE_VLAN_TAG
-	dev_rxvlan_enable = !!(pdata->dev_state & NETIF_F_HW_VLAN_CTAG_RX);
-	if (((features & NETIF_F_HW_VLAN_CTAG_RX) == NETIF_F_HW_VLAN_CTAG_RX)
-	    && !dev_rxvlan_enable) {
-		pdata->dev_state |= NETIF_F_HW_VLAN_CTAG_RX;
-		hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
-		printk(KERN_ALERT "State change - rxvlan enable\n");
-	} else if (((features & NETIF_F_HW_VLAN_CTAG_RX) == 0) &&
-			dev_rxvlan_enable) {
-		pdata->dev_state &= ~NETIF_F_HW_VLAN_CTAG_RX;
-		hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
-		printk(KERN_ALERT "State change - rxvlan disable\n");
-	}
-
-	dev_txvlan_enable = !!(pdata->dev_state & NETIF_F_HW_VLAN_CTAG_TX);
-	if (((features & NETIF_F_HW_VLAN_CTAG_TX) == NETIF_F_HW_VLAN_CTAG_TX)
-	    && !dev_txvlan_enable) {
-		pdata->dev_state |= NETIF_F_HW_VLAN_CTAG_TX;
-		printk(KERN_ALERT "State change - txvlan enable\n");
-	} else if (((features & NETIF_F_HW_VLAN_CTAG_TX) == 0) &&
-			dev_txvlan_enable) {
-		pdata->dev_state &= ~NETIF_F_HW_VLAN_CTAG_TX;
-		printk(KERN_ALERT "State change - txvlan disable\n");
-	}
-#endif	/* DWC_ETH_QOS_ENABLE_VLAN_TAG */
+   dev_rxvlan_enable = !!(pdata->dev_state & NETIF_F_HW_VLAN_CTAG_RX);
+   if (((features & NETIF_F_HW_VLAN_CTAG_RX) == NETIF_F_HW_VLAN_CTAG_RX)
+       && !dev_rxvlan_enable) {
+      pdata->dev_state |= NETIF_F_HW_VLAN_CTAG_RX;
+      hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
+      printk(KERN_ALERT "State change - rxvlan enable\n");
+   } else if (((features & NETIF_F_HW_VLAN_CTAG_RX) == 0) &&
+         dev_rxvlan_enable) {
+      pdata->dev_state &= ~NETIF_F_HW_VLAN_CTAG_RX;
+      hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
+      printk(KERN_ALERT "State change - rxvlan disable\n");
+   }
 
-	DBGPR("<--DWC_ETH_QOS_set_features\n");
+   dev_txvlan_enable = !!(pdata->dev_state & NETIF_F_HW_VLAN_CTAG_TX);
+   if (((features & NETIF_F_HW_VLAN_CTAG_TX) == NETIF_F_HW_VLAN_CTAG_TX)
+       && !dev_txvlan_enable) {
+      pdata->dev_state |= NETIF_F_HW_VLAN_CTAG_TX;
+      printk(KERN_ALERT "State change - txvlan enable\n");
+   } else if (((features & NETIF_F_HW_VLAN_CTAG_TX) == 0) &&
+         dev_txvlan_enable) {
+      pdata->dev_state &= ~NETIF_F_HW_VLAN_CTAG_TX;
+      printk(KERN_ALERT "State change - txvlan disable\n");
+   }
+#endif   /* DWC_ETH_QOS_ENABLE_VLAN_TAG */
 
-	return 0;
+   DBGPR("<--DWC_ETH_QOS_set_features\n");
+
+   return 0;
 }
 
 /*!
@@ -3555,29 +3579,28 @@ static int DWC_ETH_QOS_set_features(stru
  *
  * \retval modified flag
  */
-
 static netdev_features_t DWC_ETH_QOS_fix_features(struct net_device *dev,
       netdev_features_t features)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
 
-	DBGPR("-->DWC_ETH_QOS_fix_features: %#llx\n", features);
+   DBGPR("-->DWC_ETH_QOS_fix_features: %#llx\n", features);
 
 #ifdef DWC_ETH_QOS_ENABLE_VLAN_TAG
-	if (pdata->rx_split_hdr) {
-		/* The VLAN tag stripping must be set for the split function.
-		 * For instance, the DMA separates the header and payload of
-		 * an untagged packet only. Hence, when a tagged packet is
-		 * received, the QOS must be programmed such that the VLAN
-		 * tags are deleted/stripped from the received packets.
-		 * */
-		features |= NETIF_F_HW_VLAN_CTAG_RX;
-	}
+   if (pdata->rx_split_hdr) {
+      /* The VLAN tag stripping must be set for the split function.
+       * For instance, the DMA separates the header and payload of
+       * an untagged packet only. Hence, when a tagged packet is
+       * received, the QOS must be programmed such that the VLAN
+       * tags are deleted/stripped from the received packets.
+       * */
+      features |= NETIF_F_HW_VLAN_CTAG_RX;
+   }
 #endif /* end of DWC_ETH_QOS_ENABLE_VLAN_TAG */
 
-	DBGPR("<--DWC_ETH_QOS_fix_features: %#llx\n", features);
+   DBGPR("<--DWC_ETH_QOS_fix_features: %#llx\n", features);
 
-	return features;
+   return features;
 }
 
 /*!
@@ -3593,47 +3616,44 @@ static netdev_features_t DWC_ETH_QOS_fix
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_rx_split_hdr_mode(struct net_device *dev,
-		unsigned int flags)
+      unsigned int flags)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	unsigned int qInx;
-	int ret = 0;
-
-	DBGPR("-->DWC_ETH_QOS_config_rx_split_hdr_mode\n");
-
-	if (flags && pdata->rx_split_hdr) {
-		printk(KERN_ALERT
-			"Rx Split header mode is already enabled\n");
-		return -EINVAL;
-	}
-
-	if (!flags && !pdata->rx_split_hdr) {
-		printk(KERN_ALERT
-			"Rx Split header mode is already disabled\n");
-		return -EINVAL;
-	}
-
-	DWC_ETH_QOS_stop_dev(pdata);
-
-	/* If split header mode is disabled(ie flags == 0)
-	 * then RX will be in default/jumbo mode based on MTU
-	 * */
-	pdata->rx_split_hdr = !!flags;
-
-	DWC_ETH_QOS_start_dev(pdata);
-
-	hw_if->config_header_size(DWC_ETH_QOS_MAX_HDR_SIZE);
-	/* enable/disable split header for all RX DMA channel */
-	for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
-		hw_if->config_split_header_mode(qInx, pdata->rx_split_hdr);
-
-	printk(KERN_ALERT "Succesfully %s Rx Split header mode\n",
-		(flags ? "enabled" : "disabled"));
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   unsigned int qInx;
+   gbe_power_state_t state;
+
+   DBGPR("-->DWC_ETH_QOS_config_rx_split_hdr_mode\n");
+
+   if (flags && pdata->rx_split_hdr) {
+      WRN_PRINT("Rx Split header mode is already enabled\n");
+      return 0;
+   }
+   if (!flags && !pdata->rx_split_hdr) {
+      WRN_PRINT("Rx Split header mode is already disabled\n");
+      return 0;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_config_rx_split_hdr_mode\n");
+   state = DWC_ETH_QOS_stop_dev(pdata);
+   /* If split header mode is disabled(ie flags == 0)
+    * then RX will be in default/jumbo mode based on MTU
+    * */
+   pdata->rx_split_hdr = !!flags;
+   CFG_PRINT("Rx Split header mode set(%d)!\n", flags);
+
+   if (state == GBE_RUN_STATE) {
+      hw_if->config_header_size(DWC_ETH_QOS_MAX_HDR_SIZE);
+      /* Enable/disable split header for all RX DMA channel */
+      for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
+         hw_if->config_split_header_mode(qInx, pdata->rx_split_hdr);
+      DWC_ETH_QOS_start_dev(pdata);
+   } else if (state == GBE_STANDBY_STATE) {
+      /* Save request to apply it when device is powered up */
+      pdata->power_state |= DWC_ETH_QOS_NETIP_SPLHDR_REQ;
+   }
 
-	return ret;
+   DBGPR("<--DWC_ETH_QOS_config_rx_split_hdr_mode\n");
+   return 0;
 }
 
 /*!
@@ -3649,35 +3669,35 @@ static int DWC_ETH_QOS_config_rx_split_h
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_l3_l4_filtering(struct net_device *dev,
-		unsigned int flags)
+      unsigned int flags)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	int ret = 0;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_config_l3_l4_filtering\n");
-
-	if (flags && pdata->l3_l4_filter) {
-		printk(KERN_ALERT
-			"L3/L4 filtering is already enabled\n");
-		return -EINVAL;
-	}
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   int ret = 0;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_config_l3_l4_filtering\n");
+
+   if (flags && pdata->l3_l4_filter) {
+      printk(KERN_ALERT
+         "L3/L4 filtering is already enabled\n");
+      return -EINVAL;
+   }
 
-	if (!flags && !pdata->l3_l4_filter) {
-		printk(KERN_ALERT
-			"L3/L4 filtering is already disabled\n");
-		return -EINVAL;
-	}
+   if (!flags && !pdata->l3_l4_filter) {
+      printk(KERN_ALERT
+         "L3/L4 filtering is already disabled\n");
+      return -EINVAL;
+   }
 
-	pdata->l3_l4_filter = !!flags;
-	hw_if->config_l3_l4_filter_enable(pdata->l3_l4_filter);
+   pdata->l3_l4_filter = !!flags;
+   hw_if->config_l3_l4_filter_enable(pdata->l3_l4_filter);
 
-	DBGPR_FILTER("Succesfully %s L3/L4 filtering\n",
-		(flags ? "ENABLED" : "DISABLED"));
+   DBGPR_FILTER("Succesfully %s L3/L4 filtering\n",
+      (flags ? "ENABLED" : "DISABLED"));
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_config_l3_l4_filtering\n");
+   DBGPR_FILTER("<--DWC_ETH_QOS_config_l3_l4_filtering\n");
 
-	return ret;
+   return ret;
 }
 
 /*!
@@ -3696,57 +3716,57 @@ static int DWC_ETH_QOS_config_l3_l4_filt
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_ip4_filters(struct net_device *dev,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct DWC_ETH_QOS_l3_l4_filter *u_l3_filter =
-		(struct DWC_ETH_QOS_l3_l4_filter *)req->ptr;
-	struct DWC_ETH_QOS_l3_l4_filter l_l3_filter;
-	int ret = 0;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_config_ip4_filters\n");
-
-	if (pdata->hw_feat.l3l4_filter_num == 0)
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-
-	if (copy_from_user(&l_l3_filter, u_l3_filter,
-		sizeof(struct DWC_ETH_QOS_l3_l4_filter)))
-		return -EFAULT;
-
-	if ((l_l3_filter.filter_no + 1) > pdata->hw_feat.l3l4_filter_num) {
-		printk(KERN_ALERT "%d filter is not supported in the HW\n",
-			l_l3_filter.filter_no);
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-	}
-
-	if (!pdata->l3_l4_filter) {
-		hw_if->config_l3_l4_filter_enable(1);
-		pdata->l3_l4_filter = 1;
-	}
-
-	/* configure the L3 filters */
-	hw_if->config_l3_filters(l_l3_filter.filter_no,
-			l_l3_filter.filter_enb_dis, 0,
-			l_l3_filter.src_dst_addr_match,
-			l_l3_filter.perfect_inverse_match);
-
-	if (!l_l3_filter.src_dst_addr_match)
-		hw_if->update_ip4_addr0(l_l3_filter.filter_no,
-				l_l3_filter.ip4_addr);
-	else
-		hw_if->update_ip4_addr1(l_l3_filter.filter_no,
-				l_l3_filter.ip4_addr);
-
-	DBGPR_FILTER("Successfully %s IPv4 %s %s addressing filtering on %d filter\n",
-		(l_l3_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
-		(l_l3_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"),
-		(l_l3_filter.src_dst_addr_match ? "DESTINATION" : "SOURCE"),
-		l_l3_filter.filter_no);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_l3_l4_filter *u_l3_filter =
+      (struct DWC_ETH_QOS_l3_l4_filter *)req->ptr;
+   struct DWC_ETH_QOS_l3_l4_filter l_l3_filter;
+   int ret = 0;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_config_ip4_filters\n");
+
+   if (pdata->hw_feat.l3l4_filter_num == 0)
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+
+   if (copy_from_user(&l_l3_filter, u_l3_filter,
+      sizeof(struct DWC_ETH_QOS_l3_l4_filter)))
+      return -EFAULT;
+
+   if ((l_l3_filter.filter_no + 1) > pdata->hw_feat.l3l4_filter_num) {
+      printk(KERN_ALERT "%d filter is not supported in the HW\n",
+         l_l3_filter.filter_no);
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+   }
+
+   if (!pdata->l3_l4_filter) {
+      hw_if->config_l3_l4_filter_enable(1);
+      pdata->l3_l4_filter = 1;
+   }
+
+   /* configure the L3 filters */
+   hw_if->config_l3_filters(l_l3_filter.filter_no,
+         l_l3_filter.filter_enb_dis, 0,
+         l_l3_filter.src_dst_addr_match,
+         l_l3_filter.perfect_inverse_match);
+
+   if (!l_l3_filter.src_dst_addr_match)
+      hw_if->update_ip4_addr0(l_l3_filter.filter_no,
+            l_l3_filter.ip4_addr);
+   else
+      hw_if->update_ip4_addr1(l_l3_filter.filter_no,
+            l_l3_filter.ip4_addr);
+
+   DBGPR_FILTER("Successfully %s IPv4 %s %s addressing filtering on %d filter\n",
+      (l_l3_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
+      (l_l3_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"),
+      (l_l3_filter.src_dst_addr_match ? "DESTINATION" : "SOURCE"),
+      l_l3_filter.filter_no);
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_config_ip4_filters\n");
+   DBGPR_FILTER("<--DWC_ETH_QOS_config_ip4_filters\n");
 
-	return ret;
+   return ret;
 }
 
 /*!
@@ -3765,53 +3785,53 @@ static int DWC_ETH_QOS_config_ip4_filter
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_ip6_filters(struct net_device *dev,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct DWC_ETH_QOS_l3_l4_filter *u_l3_filter =
-		(struct DWC_ETH_QOS_l3_l4_filter *)req->ptr;
-	struct DWC_ETH_QOS_l3_l4_filter l_l3_filter;
-	int ret = 0;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_config_ip6_filters\n");
-
-	if (pdata->hw_feat.l3l4_filter_num == 0)
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-
-	if (copy_from_user(&l_l3_filter, u_l3_filter,
-		sizeof(struct DWC_ETH_QOS_l3_l4_filter)))
-		return -EFAULT;
-
-	if ((l_l3_filter.filter_no + 1) > pdata->hw_feat.l3l4_filter_num) {
-		printk(KERN_ALERT "%d filter is not supported in the HW\n",
-			l_l3_filter.filter_no);
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-	}
-
-	if (!pdata->l3_l4_filter) {
-		hw_if->config_l3_l4_filter_enable(1);
-		pdata->l3_l4_filter = 1;
-	}
-
-	/* configure the L3 filters */
-	hw_if->config_l3_filters(l_l3_filter.filter_no,
-			l_l3_filter.filter_enb_dis, 1,
-			l_l3_filter.src_dst_addr_match,
-			l_l3_filter.perfect_inverse_match);
-
-	hw_if->update_ip6_addr(l_l3_filter.filter_no,
-			l_l3_filter.ip6_addr);
-
-	DBGPR_FILTER("Successfully %s IPv6 %s %s addressing filtering on %d filter\n",
-		(l_l3_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
-		(l_l3_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"),
-		(l_l3_filter.src_dst_addr_match ? "DESTINATION" : "SOURCE"),
-		l_l3_filter.filter_no);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_l3_l4_filter *u_l3_filter =
+      (struct DWC_ETH_QOS_l3_l4_filter *)req->ptr;
+   struct DWC_ETH_QOS_l3_l4_filter l_l3_filter;
+   int ret = 0;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_config_ip6_filters\n");
+
+   if (pdata->hw_feat.l3l4_filter_num == 0)
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+
+   if (copy_from_user(&l_l3_filter, u_l3_filter,
+      sizeof(struct DWC_ETH_QOS_l3_l4_filter)))
+      return -EFAULT;
+
+   if ((l_l3_filter.filter_no + 1) > pdata->hw_feat.l3l4_filter_num) {
+      printk(KERN_ALERT "%d filter is not supported in the HW\n",
+         l_l3_filter.filter_no);
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+   }
+
+   if (!pdata->l3_l4_filter) {
+      hw_if->config_l3_l4_filter_enable(1);
+      pdata->l3_l4_filter = 1;
+   }
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_config_ip6_filters\n");
+   /* configure the L3 filters */
+   hw_if->config_l3_filters(l_l3_filter.filter_no,
+         l_l3_filter.filter_enb_dis, 1,
+         l_l3_filter.src_dst_addr_match,
+         l_l3_filter.perfect_inverse_match);
+
+   hw_if->update_ip6_addr(l_l3_filter.filter_no,
+         l_l3_filter.ip6_addr);
+
+   DBGPR_FILTER("Successfully %s IPv6 %s %s addressing filtering on %d filter\n",
+      (l_l3_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
+      (l_l3_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"),
+      (l_l3_filter.src_dst_addr_match ? "DESTINATION" : "SOURCE"),
+      l_l3_filter.filter_no);
 
-	return ret;
+   DBGPR_FILTER("<--DWC_ETH_QOS_config_ip6_filters\n");
+
+   return ret;
 }
 
 /*!
@@ -3832,60 +3852,60 @@ static int DWC_ETH_QOS_config_ip6_filter
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_tcp_udp_filters(struct net_device *dev,
-		struct ifr_data_struct *req,
-		int tcp_udp)
+      struct ifr_data_struct *req,
+      int tcp_udp)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct DWC_ETH_QOS_l3_l4_filter *u_l4_filter =
-		(struct DWC_ETH_QOS_l3_l4_filter *)req->ptr;
-	struct DWC_ETH_QOS_l3_l4_filter l_l4_filter;
-	int ret = 0;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_config_tcp_udp_filters\n");
-
-	if (pdata->hw_feat.l3l4_filter_num == 0)
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-
-	if (copy_from_user(&l_l4_filter, u_l4_filter,
-		sizeof(struct DWC_ETH_QOS_l3_l4_filter)))
-		return -EFAULT;
-
-	if ((l_l4_filter.filter_no + 1) > pdata->hw_feat.l3l4_filter_num) {
-		printk(KERN_ALERT "%d filter is not supported in the HW\n",
-			l_l4_filter.filter_no);
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-	}
-
-	if (!pdata->l3_l4_filter) {
-		hw_if->config_l3_l4_filter_enable(1);
-		pdata->l3_l4_filter = 1;
-	}
-
-	/* configure the L4 filters */
-	hw_if->config_l4_filters(l_l4_filter.filter_no,
-			l_l4_filter.filter_enb_dis,
-			tcp_udp,
-			l_l4_filter.src_dst_addr_match,
-			l_l4_filter.perfect_inverse_match);
-
-	if (l_l4_filter.src_dst_addr_match)
-		hw_if->update_l4_da_port_no(l_l4_filter.filter_no,
-				l_l4_filter.port_no);
-	else
-		hw_if->update_l4_sa_port_no(l_l4_filter.filter_no,
-				l_l4_filter.port_no);
-
-	DBGPR_FILTER("Successfully %s %s %s %s Port number filtering on %d filter\n",
-		(l_l4_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
-		(tcp_udp ? "UDP" : "TCP"),
-		(l_l4_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"),
-		(l_l4_filter.src_dst_addr_match ? "DESTINATION" : "SOURCE"),
-		l_l4_filter.filter_no);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_l3_l4_filter *u_l4_filter =
+      (struct DWC_ETH_QOS_l3_l4_filter *)req->ptr;
+   struct DWC_ETH_QOS_l3_l4_filter l_l4_filter;
+   int ret = 0;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_config_tcp_udp_filters\n");
+
+   if (pdata->hw_feat.l3l4_filter_num == 0)
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+
+   if (copy_from_user(&l_l4_filter, u_l4_filter,
+      sizeof(struct DWC_ETH_QOS_l3_l4_filter)))
+      return -EFAULT;
+
+   if ((l_l4_filter.filter_no + 1) > pdata->hw_feat.l3l4_filter_num) {
+      printk(KERN_ALERT "%d filter is not supported in the HW\n",
+         l_l4_filter.filter_no);
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+   }
+
+   if (!pdata->l3_l4_filter) {
+      hw_if->config_l3_l4_filter_enable(1);
+      pdata->l3_l4_filter = 1;
+   }
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_config_tcp_udp_filters\n");
+   /* configure the L4 filters */
+   hw_if->config_l4_filters(l_l4_filter.filter_no,
+         l_l4_filter.filter_enb_dis,
+         tcp_udp,
+         l_l4_filter.src_dst_addr_match,
+         l_l4_filter.perfect_inverse_match);
+
+   if (l_l4_filter.src_dst_addr_match)
+      hw_if->update_l4_da_port_no(l_l4_filter.filter_no,
+            l_l4_filter.port_no);
+   else
+      hw_if->update_l4_sa_port_no(l_l4_filter.filter_no,
+            l_l4_filter.port_no);
+
+   DBGPR_FILTER("Successfully %s %s %s %s Port number filtering on %d filter\n",
+      (l_l4_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
+      (tcp_udp ? "UDP" : "TCP"),
+      (l_l4_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"),
+      (l_l4_filter.src_dst_addr_match ? "DESTINATION" : "SOURCE"),
+      l_l4_filter.filter_no);
 
-	return ret;
+   DBGPR_FILTER("<--DWC_ETH_QOS_config_tcp_udp_filters\n");
+
+   return ret;
 }
 
 /*!
@@ -3902,41 +3922,41 @@ static int DWC_ETH_QOS_config_tcp_udp_fi
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_vlan_filter(struct net_device *dev,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct DWC_ETH_QOS_vlan_filter *u_vlan_filter =
-		(struct DWC_ETH_QOS_vlan_filter *)req->ptr;
-	struct DWC_ETH_QOS_vlan_filter l_vlan_filter;
-	int ret = 0;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_config_vlan_filter\n");
-
-	if (copy_from_user(&l_vlan_filter, u_vlan_filter,
-		sizeof(struct DWC_ETH_QOS_vlan_filter)))
-		return -EFAULT;
-
-	if ((l_vlan_filter.perfect_hash) &&
-		(pdata->hw_feat.vlan_hash_en == 0)) {
-		printk(KERN_ALERT "VLAN HASH filtering is not supported\n");
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-	}
-
-	/* configure the vlan filter */
-	hw_if->config_vlan_filtering(l_vlan_filter.filter_enb_dis,
-					l_vlan_filter.perfect_hash,
-					l_vlan_filter.perfect_inverse_match);
-	pdata->vlan_hash_filtering = l_vlan_filter.perfect_hash;
-
-	DBGPR_FILTER("Successfully %s VLAN %s filtering and %s matching\n",
-		(l_vlan_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
-		(l_vlan_filter.perfect_hash ? "HASH" : "PERFECT"),
-		(l_vlan_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"));
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_vlan_filter *u_vlan_filter =
+      (struct DWC_ETH_QOS_vlan_filter *)req->ptr;
+   struct DWC_ETH_QOS_vlan_filter l_vlan_filter;
+   int ret = 0;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_config_vlan_filter\n");
+
+   if (copy_from_user(&l_vlan_filter, u_vlan_filter,
+      sizeof(struct DWC_ETH_QOS_vlan_filter)))
+      return -EFAULT;
+
+   if ((l_vlan_filter.perfect_hash) &&
+      (pdata->hw_feat.vlan_hash_en == 0)) {
+      printk(KERN_ALERT "VLAN HASH filtering is not supported\n");
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+   }
+
+   /* configure the vlan filter */
+   hw_if->config_vlan_filtering(l_vlan_filter.filter_enb_dis,
+               l_vlan_filter.perfect_hash,
+               l_vlan_filter.perfect_inverse_match);
+   pdata->vlan_hash_filtering = l_vlan_filter.perfect_hash;
+
+   DBGPR_FILTER("Successfully %s VLAN %s filtering and %s matching\n",
+      (l_vlan_filter.filter_enb_dis ? "ENABLED" : "DISABLED"),
+      (l_vlan_filter.perfect_hash ? "HASH" : "PERFECT"),
+      (l_vlan_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"));
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_config_vlan_filter\n");
+   DBGPR_FILTER("<--DWC_ETH_QOS_config_vlan_filter\n");
 
-	return ret;
+   return ret;
 }
 
 /*!
@@ -3951,38 +3971,37 @@ static int DWC_ETH_QOS_config_vlan_filte
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_arp_offload(struct net_device *dev,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct DWC_ETH_QOS_arp_offload *u_arp_offload =
-		(struct DWC_ETH_QOS_arp_offload *)req->ptr;
-	struct DWC_ETH_QOS_arp_offload l_arp_offload;
-	int ret = 0;
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_arp_offload *u_arp_offload =
+      (struct DWC_ETH_QOS_arp_offload *)req->ptr;
+   struct DWC_ETH_QOS_arp_offload l_arp_offload;
+   int ret = 0;
 
-	printk(KERN_ALERT "-->DWC_ETH_QOS_config_arp_offload\n");
+   printk(KERN_ALERT "-->DWC_ETH_QOS_config_arp_offload\n");
 
-	if (pdata->hw_feat.arp_offld_en == 0)
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
+   if (pdata->hw_feat.arp_offld_en == 0)
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
 
-	if (copy_from_user(&l_arp_offload, u_arp_offload,
-		sizeof(struct DWC_ETH_QOS_arp_offload)))
-		return -EFAULT;
+   if (copy_from_user(&l_arp_offload, u_arp_offload,
+      sizeof(struct DWC_ETH_QOS_arp_offload)))
+      return -EFAULT;
 
-	/* configure the L3 filters */
-	hw_if->config_arp_offload(req->flags);
-	hw_if->update_arp_offload_ip_addr(l_arp_offload.ip_addr, pdata->version);
-	pdata->arp_offload = req->flags;
+   /* configure the L3 filters */
+   hw_if->config_arp_offload(req->flags);
+   hw_if->update_arp_offload_ip_addr(l_arp_offload.ip_addr, pdata->version);
+   pdata->arp_offload = req->flags;
 
-	printk(KERN_ALERT "Successfully %s arp Offload\n",
-		(req->flags ? "ENABLED" : "DISABLED"));
+   printk(KERN_ALERT "Successfully %s arp Offload\n",
+      (req->flags ? "ENABLED" : "DISABLED"));
 
-	printk(KERN_ALERT "<--DWC_ETH_QOS_config_arp_offload\n");
+   printk(KERN_ALERT "<--DWC_ETH_QOS_config_arp_offload\n");
 
-	return ret;
+   return ret;
 }
 
-
 /*!
  * \details This function is invoked by ioctl function when user issues an
  * ioctl command to configure L2 destination addressing filtering mode. This
@@ -3998,43 +4017,43 @@ static int DWC_ETH_QOS_config_arp_offloa
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_confing_l2_da_filter(struct net_device *dev,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct DWC_ETH_QOS_l2_da_filter *u_l2_da_filter =
-	  (struct DWC_ETH_QOS_l2_da_filter *)req->ptr;
-	struct DWC_ETH_QOS_l2_da_filter l_l2_da_filter;
-	int ret = 0;
-
-	DBGPR_FILTER("-->DWC_ETH_QOS_confing_l2_da_filter\n");
-
-	if (copy_from_user(&l_l2_da_filter, u_l2_da_filter,
-	      sizeof(struct DWC_ETH_QOS_l2_da_filter)))
-		return - EFAULT;
-
-	if (l_l2_da_filter.perfect_hash) {
-		if (pdata->hw_feat.hash_tbl_sz > 0)
-			pdata->l2_filtering_mode = 1;
-		else
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-	} else {
-		if (pdata->max_addr_reg_cnt > 1)
-			pdata->l2_filtering_mode = 0;
-		else
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-	}
-
-	/* configure L2 DA perfect/inverse_matching */
-	hw_if->config_l2_da_perfect_inverse_match(l_l2_da_filter.perfect_inverse_match);
-
-	DBGPR_FILTER("Successfully selected L2 %s filtering and %s DA matching\n",
-		(l_l2_da_filter.perfect_hash ? "HASH" : "PERFECT"),
-		(l_l2_da_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"));
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_l2_da_filter *u_l2_da_filter =
+     (struct DWC_ETH_QOS_l2_da_filter *)req->ptr;
+   struct DWC_ETH_QOS_l2_da_filter l_l2_da_filter;
+   int ret = 0;
+
+   DBGPR_FILTER("-->DWC_ETH_QOS_confing_l2_da_filter\n");
+
+   if (copy_from_user(&l_l2_da_filter, u_l2_da_filter,
+         sizeof(struct DWC_ETH_QOS_l2_da_filter)))
+      return - EFAULT;
+
+   if (l_l2_da_filter.perfect_hash) {
+      if (pdata->hw_feat.hash_tbl_sz > 0)
+         pdata->l2_filtering_mode = 1;
+      else
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+   } else {
+      if (pdata->max_addr_reg_cnt > 1)
+         pdata->l2_filtering_mode = 0;
+      else
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+   }
+
+   /* configure L2 DA perfect/inverse_matching */
+   hw_if->config_l2_da_perfect_inverse_match(l_l2_da_filter.perfect_inverse_match);
 
-	DBGPR_FILTER("<--DWC_ETH_QOS_confing_l2_da_filter\n");
+   DBGPR_FILTER("Successfully selected L2 %s filtering and %s DA matching\n",
+      (l_l2_da_filter.perfect_hash ? "HASH" : "PERFECT"),
+      (l_l2_da_filter.perfect_inverse_match ? "INVERSE" : "PERFECT"));
 
-	return ret;
+   DBGPR_FILTER("<--DWC_ETH_QOS_confing_l2_da_filter\n");
+
+   return ret;
 }
 
 /*!
@@ -4050,84 +4069,84 @@ static int DWC_ETH_QOS_confing_l2_da_fil
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_mac_loopback_mode(struct net_device *dev,
-		unsigned int flags)
+      unsigned int flags)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	int ret = 0;
-
-	DBGPR("-->DWC_ETH_QOS_config_mac_loopback_mode\n");
-
-	if (flags && pdata->mac_loopback_mode) {
-		printk(KERN_ALERT
-			"MAC loopback mode is already enabled\n");
-		return -EINVAL;
-	}
-	if (!flags && !pdata->mac_loopback_mode) {
-		printk(KERN_ALERT
-			"MAC loopback mode is already disabled\n");
-		return -EINVAL;
-	}
-	pdata->mac_loopback_mode = !!flags;
-	hw_if->config_mac_loopback_mode(flags);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   int ret = 0;
+
+   DBGPR("-->DWC_ETH_QOS_config_mac_loopback_mode\n");
+
+   if (flags && pdata->mac_loopback_mode) {
+      printk(KERN_ALERT
+         "MAC loopback mode is already enabled\n");
+      return -EINVAL;
+   }
+   if (!flags && !pdata->mac_loopback_mode) {
+      printk(KERN_ALERT
+         "MAC loopback mode is already disabled\n");
+      return -EINVAL;
+   }
+   pdata->mac_loopback_mode = !!flags;
+   hw_if->config_mac_loopback_mode(flags);
 
-	printk(KERN_ALERT "Succesfully %s MAC loopback mode\n",
-		(flags ? "enabled" : "disabled"));
+   printk(KERN_ALERT "Succesfully %s MAC loopback mode\n",
+      (flags ? "enabled" : "disabled"));
 
-	DBGPR("<--DWC_ETH_QOS_config_mac_loopback_mode\n");
+   DBGPR("<--DWC_ETH_QOS_config_mac_loopback_mode\n");
 
-	return ret;
+   return ret;
 }
 
 #ifdef DWC_ETH_QOS_ENABLE_DVLAN
 static int config_tx_dvlan_processing_via_reg(struct DWC_ETH_QOS_prv_data *pdata,
-						uint32_t flags)
+                  uint32_t flags)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+   hw_interface_t *hw_if = &(pdata->hw_if);
 
-	printk(KERN_ALERT "--> config_tx_dvlan_processing_via_reg()\n");
+   printk(KERN_ALERT "--> config_tx_dvlan_processing_via_reg()\n");
 
-	if (pdata->in_out & DWC_ETH_QOS_DVLAN_OUTER)
-		hw_if->config_tx_outer_vlan(pdata->op_type,
-					pdata->outer_vlan_tag);
+   if (pdata->in_out & DWC_ETH_QOS_DVLAN_OUTER)
+      hw_if->config_tx_outer_vlan(pdata->op_type,
+               pdata->outer_vlan_tag);
 
-	if (pdata->in_out & DWC_ETH_QOS_DVLAN_INNER)
-		hw_if->config_tx_inner_vlan(pdata->op_type,
-					pdata->inner_vlan_tag);
+   if (pdata->in_out & DWC_ETH_QOS_DVLAN_INNER)
+      hw_if->config_tx_inner_vlan(pdata->op_type,
+               pdata->inner_vlan_tag);
 
-	if (flags == DWC_ETH_QOS_DVLAN_DISABLE)
-		hw_if->config_mac_for_vlan_pkt(); /* restore default configurations */
-	else
-		hw_if->config_dvlan(1);
+   if (flags == DWC_ETH_QOS_DVLAN_DISABLE)
+      hw_if->config_mac_for_vlan_pkt(); /* restore default configurations */
+   else
+      hw_if->config_dvlan(1);
 
-	printk(KERN_ALERT "<-- config_tx_dvlan_processing_via_reg()\n");
+   printk(KERN_ALERT "<-- config_tx_dvlan_processing_via_reg()\n");
 
-	return Y_SUCCESS;
+   return Y_SUCCESS;
 }
 
 static int config_tx_dvlan_processing_via_desc(struct DWC_ETH_QOS_prv_data *pdata,
-						uint32_t flags)
+                  uint32_t flags)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+   hw_interface_t *hw_if = &(pdata->hw_if);
 
-	printk(KERN_ALERT "-->config_tx_dvlan_processing_via_desc\n");
+   printk(KERN_ALERT "-->config_tx_dvlan_processing_via_desc\n");
 
-	if (flags == DWC_ETH_QOS_DVLAN_DISABLE) {
-		hw_if->config_mac_for_vlan_pkt(); /* restore default configurations */
-		pdata->via_reg_or_desc = 0;
-	} else {
-		hw_if->config_dvlan(1);
-	}
+   if (flags == DWC_ETH_QOS_DVLAN_DISABLE) {
+      hw_if->config_mac_for_vlan_pkt(); /* restore default configurations */
+      pdata->via_reg_or_desc = 0;
+   } else {
+      hw_if->config_dvlan(1);
+   }
 
-	if (pdata->in_out & DWC_ETH_QOS_DVLAN_INNER)
-			MAC_IVLANTIRR_VLTI_UdfWr(1);
+   if (pdata->in_out & DWC_ETH_QOS_DVLAN_INNER)
+         MAC_IVLANTIRR_VLTI_UdfWr(1);
 
-	if (pdata->in_out & DWC_ETH_QOS_DVLAN_OUTER)
-			MAC_VLANTIRR_VLTI_UdfWr(1);
+   if (pdata->in_out & DWC_ETH_QOS_DVLAN_OUTER)
+         MAC_VLANTIRR_VLTI_UdfWr(1);
 
-	printk(KERN_ALERT "<--config_tx_dvlan_processing_via_desc\n");
+   printk(KERN_ALERT "<--config_tx_dvlan_processing_via_desc\n");
 
-	return Y_SUCCESS;
+   return Y_SUCCESS;
 }
 
 /*!
@@ -4136,42 +4155,42 @@ static int config_tx_dvlan_processing_vi
  *
  * \param[in] pdata - pointer to private data structure.
  * \param[in] flags – Each bit in this variable carry some information related
- *		      double vlan processing.
+ *            double vlan processing.
  *
  * \return integer
  *
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_tx_dvlan_processing(
-		struct DWC_ETH_QOS_prv_data *pdata,
-		struct ifr_data_struct *req)
+      struct DWC_ETH_QOS_prv_data *pdata,
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_config_dvlan l_config_doubule_vlan,
-					  *u_config_doubule_vlan = req->ptr;
-	int ret = 0;
-
-	DBGPR("-->DWC_ETH_QOS_config_tx_dvlan_processing\n");
-
-	if(copy_from_user(&l_config_doubule_vlan, u_config_doubule_vlan,
-				sizeof(struct DWC_ETH_QOS_config_dvlan))) {
-		printk(KERN_ALERT "Failed to fetch Double vlan Struct info from user\n");
-		return DWC_ETH_QOS_CONFIG_FAIL;
-	}
-
-	pdata->inner_vlan_tag = l_config_doubule_vlan.inner_vlan_tag;
-	pdata->outer_vlan_tag = l_config_doubule_vlan.outer_vlan_tag;
-	pdata->op_type = l_config_doubule_vlan.op_type;
-	pdata->in_out = l_config_doubule_vlan.in_out;
-	pdata->via_reg_or_desc = l_config_doubule_vlan.via_reg_or_desc;
-
-	if (pdata->via_reg_or_desc == DWC_ETH_QOS_VIA_REG)
-		ret = config_tx_dvlan_processing_via_reg(pdata, req->flags);
-	else
-		ret = config_tx_dvlan_processing_via_desc(pdata, req->flags);
+   struct DWC_ETH_QOS_config_dvlan l_config_doubule_vlan,
+                 *u_config_doubule_vlan = req->ptr;
+   int ret = 0;
+
+   DBGPR("-->DWC_ETH_QOS_config_tx_dvlan_processing\n");
+
+   if(copy_from_user(&l_config_doubule_vlan, u_config_doubule_vlan,
+            sizeof(struct DWC_ETH_QOS_config_dvlan))) {
+      printk(KERN_ALERT "Failed to fetch Double vlan Struct info from user\n");
+      return DWC_ETH_QOS_CONFIG_FAIL;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_config_tx_dvlan_processing\n");
+   pdata->inner_vlan_tag = l_config_doubule_vlan.inner_vlan_tag;
+   pdata->outer_vlan_tag = l_config_doubule_vlan.outer_vlan_tag;
+   pdata->op_type = l_config_doubule_vlan.op_type;
+   pdata->in_out = l_config_doubule_vlan.in_out;
+   pdata->via_reg_or_desc = l_config_doubule_vlan.via_reg_or_desc;
+
+   if (pdata->via_reg_or_desc == DWC_ETH_QOS_VIA_REG)
+      ret = config_tx_dvlan_processing_via_reg(pdata, req->flags);
+   else
+      ret = config_tx_dvlan_processing_via_desc(pdata, req->flags);
 
-	return ret;
+   DBGPR("<--DWC_ETH_QOS_config_tx_dvlan_processing\n");
+
+   return ret;
 }
 
 /*!
@@ -4180,42 +4199,42 @@ static int DWC_ETH_QOS_config_tx_dvlan_p
  *
  * \param[in] pdata - pointer to private data structure.
  * \param[in] flags – Each bit in this variable carry some information related
- *		      double vlan processing.
+ *            double vlan processing.
  *
  * \return integer
  *
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_rx_dvlan_processing(
-		struct DWC_ETH_QOS_prv_data *pdata, unsigned int flags)
+      struct DWC_ETH_QOS_prv_data *pdata, unsigned int flags)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	int ret = 0;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   int ret = 0;
 
-	DBGPR("-->DWC_ETH_QOS_config_rx_dvlan_processing\n");
+   DBGPR("-->DWC_ETH_QOS_config_rx_dvlan_processing\n");
 
-	hw_if->config_dvlan(1);
-	if (flags == DWC_ETH_QOS_DVLAN_NONE) {
-		hw_if->config_dvlan(0);
-		hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
-		hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
-	} else if (flags == DWC_ETH_QOS_DVLAN_INNER) {
-		hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
-		hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
-	} else if (flags == DWC_ETH_QOS_DVLAN_OUTER) {
-		hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
-		hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
-	} else if (flags == DWC_ETH_QOS_DVLAN_BOTH) {
-		hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
-		hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
-	} else {
-		printk(KERN_ALERT "ERROR : double VLAN Rx configuration - Invalid argument");
-		ret = DWC_ETH_QOS_CONFIG_FAIL;
-	}
+   hw_if->config_dvlan(1);
+   if (flags == DWC_ETH_QOS_DVLAN_NONE) {
+      hw_if->config_dvlan(0);
+      hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
+      hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
+   } else if (flags == DWC_ETH_QOS_DVLAN_INNER) {
+      hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
+      hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
+   } else if (flags == DWC_ETH_QOS_DVLAN_OUTER) {
+      hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
+      hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_NO_VLAN_STRIP);
+   } else if (flags == DWC_ETH_QOS_DVLAN_BOTH) {
+      hw_if->config_rx_outer_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
+      hw_if->config_rx_inner_vlan_stripping(DWC_ETH_QOS_RX_VLAN_STRIP_ALWAYS);
+   } else {
+      printk(KERN_ALERT "ERROR : double VLAN Rx configuration - Invalid argument");
+      ret = DWC_ETH_QOS_CONFIG_FAIL;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_config_rx_dvlan_processing\n");
+   DBGPR("<--DWC_ETH_QOS_config_rx_dvlan_processing\n");
 
-	return ret;
+   return ret;
 }
 
 /*!
@@ -4224,65 +4243,65 @@ static int DWC_ETH_QOS_config_rx_dvlan_p
  *
  * \param[in] pdata - pointer to private data structure.
  * \param[in] flags – Each bit in this variable carry some information related
- *		      double vlan processing.
+ *            double vlan processing.
  *
  * \return integer
  *
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_svlan(struct DWC_ETH_QOS_prv_data *pdata,
-					unsigned int flags)
+               unsigned int flags)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	int ret = 0;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   int ret = 0;
 
-	DBGPR("-->DWC_ETH_QOS_config_svlan\n");
+   DBGPR("-->DWC_ETH_QOS_config_svlan\n");
 
-	ret = hw_if->config_svlan(flags);
-	if (ret == Y_FAILURE)
-		ret = DWC_ETH_QOS_CONFIG_FAIL;
+   ret = hw_if->config_svlan(flags);
+   if (ret == Y_FAILURE)
+      ret = DWC_ETH_QOS_CONFIG_FAIL;
 
-	DBGPR("<--DWC_ETH_QOS_config_svlan\n");
+   DBGPR("<--DWC_ETH_QOS_config_svlan\n");
 
-	return ret;
+   return ret;
 }
 #endif /* end of DWC_ETH_QOS_ENABLE_DVLAN */
 
 static void DWC_ETH_QOS_config_timer_registers(
-				struct DWC_ETH_QOS_prv_data *pdata)
+            struct DWC_ETH_QOS_prv_data *pdata)
 {
-		struct timespec now;
-		struct hw_if_struct *hw_if = &(pdata->hw_if);
-		uint64_t temp;
-
-		DBGPR("-->DWC_ETH_QOS_config_timer_registers\n");
-
-		/* program Sub Second Increment Reg */
-		hw_if->config_sub_second_increment(DWC_ETH_QOS_SYSCLOCK);
-
-		/* formula is :
-		 * addend = 2^32/freq_div_ratio;
-		 *
-		 * where, freq_div_ratio = DWC_ETH_QOS_SYSCLOCK/50MHz
-		 *
-		 * hence, addend = ((2^32) * 50MHz)/DWC_ETH_QOS_SYSCLOCK;
-		 *
-		 * NOTE: DWC_ETH_QOS_SYSCLOCK should be >= 50MHz to
-		 *       achive 20ns accuracy.
-		 *
-		 * 2^x * y == (y << x), hence
-		 * 2^32 * 50000000 ==> (50000000 << 32)
-		 * */
-		temp = (uint64_t)(50000000ULL << 32);
-		pdata->default_addend = div_u64(temp, 62500000);
-
-		hw_if->config_addend(pdata->default_addend);
-
-		/* initialize system time */
-		getnstimeofday(&now);
-		hw_if->init_systime(now.tv_sec, now.tv_nsec);
+      struct timespec now;
+      hw_interface_t *hw_if = &(pdata->hw_if);
+      uint64_t temp;
+
+      DBGPR("-->DWC_ETH_QOS_config_timer_registers\n");
+
+      /* program Sub Second Increment Reg */
+      hw_if->config_sub_second_increment(DWC_ETH_QOS_SYSCLOCK);
+
+      /* formula is :
+       * addend = 2^32/freq_div_ratio;
+       *
+       * where, freq_div_ratio = DWC_ETH_QOS_SYSCLOCK/50MHz
+       *
+       * hence, addend = ((2^32) * 50MHz)/DWC_ETH_QOS_SYSCLOCK;
+       *
+       * NOTE: DWC_ETH_QOS_SYSCLOCK should be >= 50MHz to
+       *       achive 20ns accuracy.
+       *
+       * 2^x * y == (y << x), hence
+       * 2^32 * 50000000 ==> (50000000 << 32)
+       * */
+      temp = (uint64_t)(50000000ULL << 32);
+      pdata->default_addend = div_u64(temp, 62500000);
+
+      hw_if->config_addend(pdata->default_addend);
+
+      /* initialize system time */
+      getnstimeofday(&now);
+      hw_if->init_systime(now.tv_sec, now.tv_nsec);
 
-		DBGPR("-->DWC_ETH_QOS_config_timer_registers\n");
+      DBGPR("-->DWC_ETH_QOS_config_timer_registers\n");
 }
 
 /*!
@@ -4291,85 +4310,84 @@ static void DWC_ETH_QOS_config_timer_reg
  *
  * \param[in] pdata - pointer to private data structure.
  * \param[in] flags – Each bit in this variable carry some information related
- *		      double vlan processing.
+ *            double vlan processing.
  *
  * \return integer
  *
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_ptpoffload(
-		struct DWC_ETH_QOS_prv_data *pdata,
-		struct DWC_ETH_QOS_config_ptpoffloading *u_conf_ptp)
+      struct DWC_ETH_QOS_prv_data *pdata,
+      struct DWC_ETH_QOS_config_ptpoffloading *u_conf_ptp)
 {
-	uint32_t pto_cntrl;
-	uint32_t varMAC_TCR;
-	struct DWC_ETH_QOS_config_ptpoffloading l_conf_ptp;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-
-
-	if(copy_from_user(&l_conf_ptp, u_conf_ptp,
-				sizeof(struct DWC_ETH_QOS_config_ptpoffloading))) {
-		printk(KERN_ALERT "Failed to fetch Double vlan Struct info from user\n");
-		return DWC_ETH_QOS_CONFIG_FAIL;
-	}
-
-	printk(KERN_ALERT"-->DWC_ETH_QOS_config_ptpoffload - %d\n",l_conf_ptp.mode);
-
-	pto_cntrl = MAC_PTOCR_PTOEN; /* enable ptp offloading */
-	varMAC_TCR = MAC_TCR_TSENA | MAC_TCR_TSIPENA | MAC_TCR_TSVER2ENA
-			| MAC_TCR_TSCFUPDT | MAC_TCR_TSCTRLSSR;
-	if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_ORDINARY_SLAVE) {
-
-		varMAC_TCR |= MAC_TCR_TSEVENTENA;
-		pdata->ptp_offloading_mode = DWC_ETH_QOS_PTP_ORDINARY_SLAVE;
-
-	} else if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_TRASPARENT_SLAVE) {
-
-		pto_cntrl |= MAC_PTOCR_APDREQEN;
-		varMAC_TCR |= MAC_TCR_TSEVENTENA;
-		varMAC_TCR |= MAC_TCR_SNAPTYPSEL_1;
-		pdata->ptp_offloading_mode =
-			DWC_ETH_QOS_PTP_TRASPARENT_SLAVE;
-
-	} else if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_ORDINARY_MASTER) {
-
-		pto_cntrl |= MAC_PTOCR_ASYNCEN;
-		varMAC_TCR |= MAC_TCR_TSEVENTENA;
-		varMAC_TCR |= MAC_TCR_TSMASTERENA;
-		pdata->ptp_offloading_mode = DWC_ETH_QOS_PTP_ORDINARY_MASTER;
-
-	} else if(l_conf_ptp.mode == DWC_ETH_QOS_PTP_TRASPARENT_MASTER) {
-
-		pto_cntrl |= MAC_PTOCR_ASYNCEN | MAC_PTOCR_APDREQEN;
-		varMAC_TCR |= MAC_TCR_SNAPTYPSEL_1;
-		varMAC_TCR |= MAC_TCR_TSEVENTENA;
-		varMAC_TCR |= MAC_TCR_TSMASTERENA;
-		pdata->ptp_offloading_mode =
-			DWC_ETH_QOS_PTP_TRASPARENT_MASTER;
-
-	} else if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_PEER_TO_PEER_TRANSPARENT) {
-
-		pto_cntrl |= MAC_PTOCR_APDREQEN;
-		varMAC_TCR |= MAC_TCR_SNAPTYPSEL_3;
-		pdata->ptp_offloading_mode =
-			DWC_ETH_QOS_PTP_PEER_TO_PEER_TRANSPARENT;
-	}
-
-	pdata->ptp_offload = 1;
-	if (l_conf_ptp.en_dis == DWC_ETH_QOS_PTP_OFFLOADING_DISABLE) {
-		pto_cntrl = 0;
-		varMAC_TCR = 0;
-		pdata->ptp_offload = 0;
-	}
-
-	pto_cntrl |= (l_conf_ptp.domain_num << 8);
-	hw_if->config_hw_time_stamping(varMAC_TCR);
-	DWC_ETH_QOS_config_timer_registers(pdata);
-	hw_if->config_ptpoffload_engine(pto_cntrl, l_conf_ptp.mc_uc);
+   uint32_t pto_cntrl;
+   uint32_t varMAC_TCR;
+   struct DWC_ETH_QOS_config_ptpoffloading l_conf_ptp;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+
+   if(copy_from_user(&l_conf_ptp, u_conf_ptp,
+            sizeof(struct DWC_ETH_QOS_config_ptpoffloading))) {
+      printk(KERN_ALERT "Failed to fetch Double vlan Struct info from user\n");
+      return DWC_ETH_QOS_CONFIG_FAIL;
+   }
+
+   printk(KERN_ALERT"-->DWC_ETH_QOS_config_ptpoffload - %d\n",l_conf_ptp.mode);
+
+   pto_cntrl = MAC_PTOCR_PTOEN; /* enable ptp offloading */
+   varMAC_TCR = MAC_TCR_TSENA | MAC_TCR_TSIPENA | MAC_TCR_TSVER2ENA
+         | MAC_TCR_TSCFUPDT | MAC_TCR_TSCTRLSSR;
+   if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_ORDINARY_SLAVE) {
+
+      varMAC_TCR |= MAC_TCR_TSEVENTENA;
+      pdata->ptp_offloading_mode = DWC_ETH_QOS_PTP_ORDINARY_SLAVE;
+
+   } else if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_TRASPARENT_SLAVE) {
+
+      pto_cntrl |= MAC_PTOCR_APDREQEN;
+      varMAC_TCR |= MAC_TCR_TSEVENTENA;
+      varMAC_TCR |= MAC_TCR_SNAPTYPSEL_1;
+      pdata->ptp_offloading_mode =
+         DWC_ETH_QOS_PTP_TRASPARENT_SLAVE;
+
+   } else if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_ORDINARY_MASTER) {
+
+      pto_cntrl |= MAC_PTOCR_ASYNCEN;
+      varMAC_TCR |= MAC_TCR_TSEVENTENA;
+      varMAC_TCR |= MAC_TCR_TSMASTERENA;
+      pdata->ptp_offloading_mode = DWC_ETH_QOS_PTP_ORDINARY_MASTER;
+
+   } else if(l_conf_ptp.mode == DWC_ETH_QOS_PTP_TRASPARENT_MASTER) {
+
+      pto_cntrl |= MAC_PTOCR_ASYNCEN | MAC_PTOCR_APDREQEN;
+      varMAC_TCR |= MAC_TCR_SNAPTYPSEL_1;
+      varMAC_TCR |= MAC_TCR_TSEVENTENA;
+      varMAC_TCR |= MAC_TCR_TSMASTERENA;
+      pdata->ptp_offloading_mode =
+         DWC_ETH_QOS_PTP_TRASPARENT_MASTER;
+
+   } else if (l_conf_ptp.mode == DWC_ETH_QOS_PTP_PEER_TO_PEER_TRANSPARENT) {
+
+      pto_cntrl |= MAC_PTOCR_APDREQEN;
+      varMAC_TCR |= MAC_TCR_SNAPTYPSEL_3;
+      pdata->ptp_offloading_mode =
+         DWC_ETH_QOS_PTP_PEER_TO_PEER_TRANSPARENT;
+   }
+
+   pdata->ptp_offload = 1;
+   if (l_conf_ptp.en_dis == DWC_ETH_QOS_PTP_OFFLOADING_DISABLE) {
+      pto_cntrl = 0;
+      varMAC_TCR = 0;
+      pdata->ptp_offload = 0;
+   }
+
+   pto_cntrl |= (l_conf_ptp.domain_num << 8);
+   hw_if->config_hw_time_stamping(varMAC_TCR);
+   DWC_ETH_QOS_config_timer_registers(pdata);
+   hw_if->config_ptpoffload_engine(pto_cntrl, l_conf_ptp.mc_uc);
 
-	printk(KERN_ALERT"<--DWC_ETH_QOS_config_ptpoffload\n");
+   printk(KERN_ALERT"<--DWC_ETH_QOS_config_ptpoffload\n");
 
-	return Y_SUCCESS;
+   return Y_SUCCESS;
 }
 
 /*!
@@ -4384,27 +4402,27 @@ static int DWC_ETH_QOS_config_ptpoffload
  * \retval zero on success and -ve number on failure.
  */
 static int DWC_ETH_QOS_config_pfc(struct net_device *dev,
-		unsigned int flags)
+      unsigned int flags)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	int ret = 0;
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   int ret = 0;
 
-	DBGPR("-->DWC_ETH_QOS_config_pfc\n");
+   DBGPR("-->DWC_ETH_QOS_config_pfc\n");
 
-	if (!pdata->hw_feat.dcb_en) {
-		printk(KERN_ALERT "PFC is not supported\n");
-		return DWC_ETH_QOS_NO_HW_SUPPORT;
-	}
+   if (!pdata->hw_feat.dcb_en) {
+      printk(KERN_ALERT "PFC is not supported\n");
+      return DWC_ETH_QOS_NO_HW_SUPPORT;
+   }
 
-	hw_if->config_pfc(flags);
+   hw_if->config_pfc(flags);
 
-	printk(KERN_ALERT "Succesfully %s PFC(Priority Based Flow Control)\n",
-		(flags ? "enabled" : "disabled"));
+   printk(KERN_ALERT "Succesfully %s PFC(Priority Based Flow Control)\n",
+      (flags ? "enabled" : "disabled"));
 
-	DBGPR("<--DWC_ETH_QOS_config_pfc\n");
+   DBGPR("<--DWC_ETH_QOS_config_pfc\n");
 
-	return ret;
+   return ret;
 }
 
 /*!
@@ -4424,386 +4442,383 @@ static int DWC_ETH_QOS_config_pfc(struct
  * \retval 0 - success
  * \retval negative - failure
  */
-
 static int DWC_ETH_QOS_handle_prv_ioctl(struct DWC_ETH_QOS_prv_data *pdata,
-					struct ifr_data_struct *req)
+               struct ifr_data_struct *req)
 {
-	unsigned int qInx = req->qInx;
-	struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data =
-	    GET_TX_WRAPPER_DESC(qInx);
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data =
-	    GET_RX_WRAPPER_DESC(qInx);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct net_device *dev = pdata->dev;
-	int ret = 0;
-
-	DBGPR("-->DWC_ETH_QOS_handle_prv_ioctl\n");
-
-	if (qInx > DWC_ETH_QOS_QUEUE_CNT) {
-		printk(KERN_ALERT "Queue number %d is invalid\n" \
-				"Hardware has only %d Tx/Rx Queues\n",
-				qInx, DWC_ETH_QOS_QUEUE_CNT);
-		ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		return ret;
-	}
-
-	switch (req->cmd) {
-	case DWC_ETH_QOS_POWERUP_MAGIC_CMD:
-		if (pdata->hw_feat.mgk_sel) {
-			ret = DWC_ETH_QOS_powerup(dev, DWC_ETH_QOS_IOCTL_CONTEXT);
-			if (ret == 0)
-				ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-			else
-				ret = DWC_ETH_QOS_CONFIG_FAIL;
-		} else {
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_POWERDOWN_MAGIC_CMD:
-		if (pdata->hw_feat.mgk_sel) {
-			ret =
-			  DWC_ETH_QOS_powerdown(dev,
-			    DWC_ETH_QOS_MAGIC_WAKEUP, DWC_ETH_QOS_IOCTL_CONTEXT);
-			if (ret == 0)
-				ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-			else
-				ret = DWC_ETH_QOS_CONFIG_FAIL;
-		} else {
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_POWERUP_REMOTE_WAKEUP_CMD:
-		if (pdata->hw_feat.rwk_sel) {
-			ret = DWC_ETH_QOS_powerup(dev, DWC_ETH_QOS_IOCTL_CONTEXT);
-			if (ret == 0)
-				ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-			else
-				ret = DWC_ETH_QOS_CONFIG_FAIL;
-		} else {
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_POWERDOWN_REMOTE_WAKEUP_CMD:
-		if (pdata->hw_feat.rwk_sel) {
-			ret = DWC_ETH_QOS_configure_remotewakeup(dev, req);
-			if (ret == 0)
-				ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-			else
-				ret = DWC_ETH_QOS_CONFIG_FAIL;
-		} else {
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_RX_THRESHOLD_CMD:
-		rx_desc_data->rx_threshold_val = req->flags;
-		hw_if->config_rx_threshold(qInx,
-					rx_desc_data->rx_threshold_val);
-		printk(KERN_ALERT "Configured Rx threshold with %d\n",
-		       rx_desc_data->rx_threshold_val);
-		break;
-
-	case DWC_ETH_QOS_TX_THRESHOLD_CMD:
-		tx_desc_data->tx_threshold_val = req->flags;
-		hw_if->config_tx_threshold(qInx,
-					tx_desc_data->tx_threshold_val);
-		printk(KERN_ALERT "Configured Tx threshold with %d\n",
-		       tx_desc_data->tx_threshold_val);
-		break;
-
-	case DWC_ETH_QOS_RSF_CMD:
-		rx_desc_data->rsf_on = req->flags;
-		hw_if->config_rsf_mode(qInx, rx_desc_data->rsf_on);
-		printk(KERN_ALERT "Receive store and forward mode %s\n",
-		       (rx_desc_data->rsf_on) ? "enabled" : "disabled");
-		break;
-
-	case DWC_ETH_QOS_TSF_CMD:
-		tx_desc_data->tsf_on = req->flags;
-		hw_if->config_tsf_mode(qInx, tx_desc_data->tsf_on);
-		printk(KERN_ALERT "Transmit store and forward mode %s\n",
-		       (tx_desc_data->tsf_on) ? "enabled" : "disabled");
-		break;
-
-	case DWC_ETH_QOS_OSF_CMD:
-		tx_desc_data->osf_on = req->flags;
-		hw_if->config_osf_mode(qInx, tx_desc_data->osf_on);
-		printk(KERN_ALERT "Transmit DMA OSF mode is %s\n",
-		       (tx_desc_data->osf_on) ? "enabled" : "disabled");
-		break;
-
-	case DWC_ETH_QOS_INCR_INCRX_CMD:
-		pdata->incr_incrx = req->flags;
-		hw_if->config_incr_incrx_mode(pdata->incr_incrx);
-		printk(KERN_ALERT "%s mode is enabled\n",
-		       (pdata->incr_incrx) ? "INCRX" : "INCR");
-		break;
-
-	case DWC_ETH_QOS_RX_PBL_CMD:
-		rx_desc_data->rx_pbl = req->flags;
-		DWC_ETH_QOS_config_rx_pbl(pdata, rx_desc_data->rx_pbl, qInx);
-		break;
-
-	case DWC_ETH_QOS_TX_PBL_CMD:
-		tx_desc_data->tx_pbl = req->flags;
-		DWC_ETH_QOS_config_tx_pbl(pdata, tx_desc_data->tx_pbl, qInx);
-		break;
+   unsigned int qInx = req->qInx;
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data =
+       GET_TX_WRAPPER_DESC(qInx);
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data =
+       GET_RX_WRAPPER_DESC(qInx);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct net_device *dev = pdata->dev;
+   int ret = 0;
+
+   DBGPR("-->DWC_ETH_QOS_handle_prv_ioctl\n");
+
+   if (qInx > DWC_ETH_QOS_QUEUE_CNT) {
+      printk(KERN_ALERT "Queue number %d is invalid\n" \
+            "Hardware has only %d Tx/Rx Queues\n",
+            qInx, DWC_ETH_QOS_QUEUE_CNT);
+      ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      return ret;
+   }
+
+   switch (req->cmd) {
+   case DWC_ETH_QOS_POWERUP_MAGIC_CMD:
+      if (pdata->hw_feat.mgk_sel) {
+         ret = DWC_ETH_QOS_powerup(dev);
+         if (ret == 0)
+            ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+         else
+            ret = DWC_ETH_QOS_CONFIG_FAIL;
+      } else {
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_POWERDOWN_MAGIC_CMD:
+      if (pdata->hw_feat.mgk_sel) {
+         ret =
+           DWC_ETH_QOS_powerdown(dev, DWC_ETH_QOS_MAGIC_WAKEUP);
+         if (ret == 0)
+            ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+         else
+            ret = DWC_ETH_QOS_CONFIG_FAIL;
+      } else {
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_POWERUP_REMOTE_WAKEUP_CMD:
+      if (pdata->hw_feat.rwk_sel) {
+         ret = DWC_ETH_QOS_powerup(dev);
+         if (ret == 0)
+            ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+         else
+            ret = DWC_ETH_QOS_CONFIG_FAIL;
+      } else {
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_POWERDOWN_REMOTE_WAKEUP_CMD:
+      if (pdata->hw_feat.rwk_sel) {
+         ret = DWC_ETH_QOS_configure_remotewakeup(dev, req);
+         if (ret == 0)
+            ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+         else
+            ret = DWC_ETH_QOS_CONFIG_FAIL;
+      } else {
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_RX_THRESHOLD_CMD:
+      rx_desc_data->rx_threshold_val = req->flags;
+      hw_if->config_rx_threshold(qInx,
+               rx_desc_data->rx_threshold_val);
+      printk(KERN_ALERT "Configured Rx threshold with %d\n",
+             rx_desc_data->rx_threshold_val);
+      break;
+
+   case DWC_ETH_QOS_TX_THRESHOLD_CMD:
+      tx_desc_data->tx_threshold_val = req->flags;
+      hw_if->config_tx_threshold(qInx,
+               tx_desc_data->tx_threshold_val);
+      printk(KERN_ALERT "Configured Tx threshold with %d\n",
+             tx_desc_data->tx_threshold_val);
+      break;
+
+   case DWC_ETH_QOS_RSF_CMD:
+      rx_desc_data->rsf_on = req->flags;
+      hw_if->config_rsf_mode(qInx, rx_desc_data->rsf_on);
+      printk(KERN_ALERT "Receive store and forward mode %s\n",
+             (rx_desc_data->rsf_on) ? "enabled" : "disabled");
+      break;
+
+   case DWC_ETH_QOS_TSF_CMD:
+      tx_desc_data->tsf_on = req->flags;
+      hw_if->config_tsf_mode(qInx, tx_desc_data->tsf_on);
+      printk(KERN_ALERT "Transmit store and forward mode %s\n",
+             (tx_desc_data->tsf_on) ? "enabled" : "disabled");
+      break;
+
+   case DWC_ETH_QOS_OSF_CMD:
+      tx_desc_data->osf_on = req->flags;
+      hw_if->config_osf_mode(qInx, tx_desc_data->osf_on);
+      printk(KERN_ALERT "Transmit DMA OSF mode is %s\n",
+             (tx_desc_data->osf_on) ? "enabled" : "disabled");
+      break;
+
+   case DWC_ETH_QOS_INCR_INCRX_CMD:
+      pdata->incr_incrx = req->flags;
+      hw_if->config_incr_incrx_mode(pdata->incr_incrx);
+      printk(KERN_ALERT "%s mode is enabled\n",
+             (pdata->incr_incrx) ? "INCRX" : "INCR");
+      break;
+
+   case DWC_ETH_QOS_RX_PBL_CMD:
+      rx_desc_data->rx_pbl = req->flags;
+      DWC_ETH_QOS_config_rx_pbl(pdata, rx_desc_data->rx_pbl, qInx);
+      break;
+
+   case DWC_ETH_QOS_TX_PBL_CMD:
+      tx_desc_data->tx_pbl = req->flags;
+      DWC_ETH_QOS_config_tx_pbl(pdata, tx_desc_data->tx_pbl, qInx);
+      break;
 
 #ifdef DWC_ETH_QOS_ENABLE_DVLAN
-	case DWC_ETH_QOS_DVLAN_TX_PROCESSING_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			ret = DWC_ETH_QOS_config_tx_dvlan_processing(pdata, req);
-		} else {
-			printk(KERN_ALERT "No HW support for Single/Double VLAN\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-	case DWC_ETH_QOS_DVLAN_RX_PROCESSING_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			ret = DWC_ETH_QOS_config_rx_dvlan_processing(pdata, req->flags);
-		} else {
-			printk(KERN_ALERT "No HW support for Single/Double VLAN\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-	case DWC_ETH_QOS_SVLAN_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			ret = DWC_ETH_QOS_config_svlan(pdata, req->flags);
-		} else {
-			printk(KERN_ALERT "No HW support for Single/Double VLAN\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
+   case DWC_ETH_QOS_DVLAN_TX_PROCESSING_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         ret = DWC_ETH_QOS_config_tx_dvlan_processing(pdata, req);
+      } else {
+         printk(KERN_ALERT "No HW support for Single/Double VLAN\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+   case DWC_ETH_QOS_DVLAN_RX_PROCESSING_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         ret = DWC_ETH_QOS_config_rx_dvlan_processing(pdata, req->flags);
+      } else {
+         printk(KERN_ALERT "No HW support for Single/Double VLAN\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+   case DWC_ETH_QOS_SVLAN_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         ret = DWC_ETH_QOS_config_svlan(pdata, req->flags);
+      } else {
+         printk(KERN_ALERT "No HW support for Single/Double VLAN\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
 #endif /* end of DWC_ETH_QOS_ENABLE_DVLAN */
-	case DWC_ETH_QOS_PTPOFFLOADING_CMD:
-		if (pdata->hw_feat.tsstssel) {
-			ret = DWC_ETH_QOS_config_ptpoffload(pdata,
-					req->ptr);
-		} else {
-			printk(KERN_ALERT "No HW support for PTP\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_SA0_DESC_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			pdata->tx_sa_ctrl_via_desc = req->flags;
-			pdata->tx_sa_ctrl_via_reg = DWC_ETH_QOS_SA0_NONE;
-			if (req->flags == DWC_ETH_QOS_SA0_NONE) {
-				memcpy(pdata->mac_addr, pdata->dev->dev_addr,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			} else {
-				memcpy(pdata->mac_addr, mac_addr0,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			}
-			hw_if->update_mac_addr(0, pdata->mac_addr);
-			hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
-			printk(KERN_ALERT
-			       "SA will use MAC0 with descriptor for configuration %d\n",
-			       pdata->tx_sa_ctrl_via_desc);
-		} else {
-			printk(KERN_ALERT
-			       "Device doesn't supports SA Insertion/Replacement\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_SA1_DESC_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			pdata->tx_sa_ctrl_via_desc = req->flags;
-			pdata->tx_sa_ctrl_via_reg = DWC_ETH_QOS_SA1_NONE;
-			if (req->flags == DWC_ETH_QOS_SA1_NONE) {
-				memcpy(pdata->mac_addr, pdata->dev->dev_addr,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			} else {
-				memcpy(pdata->mac_addr, mac_addr1,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			}
-			hw_if->update_mac_addr(1, pdata->mac_addr);
-			hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
-			printk(KERN_ALERT
-			       "SA will use MAC1 with descriptor for configuration %d\n",
-			       pdata->tx_sa_ctrl_via_desc);
-		} else {
-			printk(KERN_ALERT
-			       "Device doesn't supports SA Insertion/Replacement\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_SA0_REG_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			pdata->tx_sa_ctrl_via_reg = req->flags;
-			pdata->tx_sa_ctrl_via_desc = DWC_ETH_QOS_SA0_NONE;
-			if (req->flags == DWC_ETH_QOS_SA0_NONE) {
-				memcpy(pdata->mac_addr, pdata->dev->dev_addr,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			} else {
-				memcpy(pdata->mac_addr, mac_addr0,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			}
-			hw_if->update_mac_addr(0, pdata->mac_addr);
-			hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
-			printk(KERN_ALERT
-			       "SA will use MAC0 with register for configuration %d\n",
-			       pdata->tx_sa_ctrl_via_desc);
-		} else {
-			printk(KERN_ALERT
-			       "Device doesn't supports SA Insertion/Replacement\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_SA1_REG_CMD:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			pdata->tx_sa_ctrl_via_reg = req->flags;
-			pdata->tx_sa_ctrl_via_desc = DWC_ETH_QOS_SA1_NONE;
-			if (req->flags == DWC_ETH_QOS_SA1_NONE) {
-				memcpy(pdata->mac_addr, pdata->dev->dev_addr,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			} else {
-				memcpy(pdata->mac_addr, mac_addr1,
-				       DWC_ETH_QOS_MAC_ADDR_LEN);
-			}
-			hw_if->update_mac_addr(1, pdata->mac_addr);
-			hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
-			printk(KERN_ALERT
-			       "SA will use MAC1 with register for configuration %d\n",
-			       pdata->tx_sa_ctrl_via_desc);
-		} else {
-			printk(KERN_ALERT
-			       "Device doesn't supports SA Insertion/Replacement\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_SETUP_CONTEXT_DESCRIPTOR:
-		if (pdata->hw_feat.sa_vlan_ins) {
-			tx_desc_data->context_setup = req->context_setup;
-			if (tx_desc_data->context_setup == 1) {
-				printk(KERN_ALERT "Context descriptor will be transmitted"\
-						" with every normal descriptor on %d DMA Channel\n",
-						qInx);
-			}
-			else {
-				printk(KERN_ALERT "Context descriptor will be setup"\
-						" only if VLAN id changes %d\n", qInx);
-			}
-		}
-		else {
-			printk(KERN_ALERT
-			       "Device doesn't support VLAN operations\n");
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-
-	case DWC_ETH_QOS_GET_RX_QCNT:
-		req->qInx = DWC_ETH_QOS_RX_QUEUE_CNT;
-		break;
-
-	case DWC_ETH_QOS_GET_TX_QCNT:
-		req->qInx = DWC_ETH_QOS_TX_QUEUE_CNT;
-		break;
-
-	case DWC_ETH_QOS_GET_CONNECTED_SPEED:
-		req->connected_speed = pdata->speed;
-		break;
-
-	case DWC_ETH_QOS_DCB_ALGORITHM:
-		DWC_ETH_QOS_program_dcb_algorithm(pdata, req);
-		break;
-
-	case DWC_ETH_QOS_AVB_ALGORITHM:
-		DWC_ETH_QOS_program_avb_algorithm(pdata, req);
-		break;
-
-	case DWC_ETH_QOS_RX_SPLIT_HDR_CMD:
-		if (pdata->hw_feat.sph_en) {
-			ret = DWC_ETH_QOS_config_rx_split_hdr_mode(dev, req->flags);
-			if (ret == 0)
-				ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-			else
-				ret = DWC_ETH_QOS_CONFIG_FAIL;
-		} else {
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-	case DWC_ETH_QOS_L3_L4_FILTER_CMD:
-		if (pdata->hw_feat.l3l4_filter_num > 0) {
-			ret = DWC_ETH_QOS_config_l3_l4_filtering(dev, req->flags);
-			if (ret == 0)
-				ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-			else
-				ret = DWC_ETH_QOS_CONFIG_FAIL;
-		} else {
-			ret = DWC_ETH_QOS_NO_HW_SUPPORT;
-		}
-		break;
-	case DWC_ETH_QOS_IPV4_FILTERING_CMD:
-		ret = DWC_ETH_QOS_config_ip4_filters(dev, req);
-		break;
-	case DWC_ETH_QOS_IPV6_FILTERING_CMD:
-		ret = DWC_ETH_QOS_config_ip6_filters(dev, req);
-		break;
-	case DWC_ETH_QOS_UDP_FILTERING_CMD:
-		ret = DWC_ETH_QOS_config_tcp_udp_filters(dev, req, 1);
-		break;
-	case DWC_ETH_QOS_TCP_FILTERING_CMD:
-		ret = DWC_ETH_QOS_config_tcp_udp_filters(dev, req, 0);
-		break;
-	case DWC_ETH_QOS_VLAN_FILTERING_CMD:
-		ret = DWC_ETH_QOS_config_vlan_filter(dev, req);
-		break;
-	case DWC_ETH_QOS_L2_DA_FILTERING_CMD:
-		ret = DWC_ETH_QOS_confing_l2_da_filter(dev, req);
-		break;
-	case DWC_ETH_QOS_ARP_OFFLOAD_CMD:
-		ret = DWC_ETH_QOS_config_arp_offload(dev, req);
-		break;
-	case DWC_ETH_QOS_AXI_PBL_CMD:
-		pdata->axi_pbl = req->flags;
-		hw_if->config_axi_pbl_val(pdata->axi_pbl);
-		printk(KERN_ALERT "AXI PBL value: %d\n", pdata->axi_pbl);
-		break;
-	case DWC_ETH_QOS_AXI_WORL_CMD:
-		pdata->axi_worl = req->flags;
-		hw_if->config_axi_worl_val(pdata->axi_worl);
-		printk(KERN_ALERT "AXI WORL value: %d\n", pdata->axi_worl);
-		break;
-	case DWC_ETH_QOS_AXI_RORL_CMD:
-		pdata->axi_rorl = req->flags;
-		hw_if->config_axi_rorl_val(pdata->axi_rorl);
-		printk(KERN_ALERT "AXI RORL value: %d\n", pdata->axi_rorl);
-		break;
-	case DWC_ETH_QOS_MAC_LOOPBACK_MODE_CMD:
-		ret = DWC_ETH_QOS_config_mac_loopback_mode(dev, req->flags);
-		if (ret == 0)
-			ret = DWC_ETH_QOS_CONFIG_SUCCESS;
-		else
-			ret = DWC_ETH_QOS_CONFIG_FAIL;
-		break;
-	case DWC_ETH_QOS_PFC_CMD:
-		ret = DWC_ETH_QOS_config_pfc(dev, req->flags);
-		break;
+   case DWC_ETH_QOS_PTPOFFLOADING_CMD:
+      if (pdata->hw_feat.tsstssel) {
+         ret = DWC_ETH_QOS_config_ptpoffload(pdata,
+               req->ptr);
+      } else {
+         printk(KERN_ALERT "No HW support for PTP\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_SA0_DESC_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         pdata->tx_sa_ctrl_via_desc = req->flags;
+         pdata->tx_sa_ctrl_via_reg = DWC_ETH_QOS_SA0_NONE;
+         if (req->flags == DWC_ETH_QOS_SA0_NONE) {
+            memcpy(pdata->mac_addr, pdata->dev->dev_addr,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         } else {
+            memcpy(pdata->mac_addr, mac_addr0,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         }
+         hw_if->update_mac_addr(0, pdata->mac_addr);
+         hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
+         printk(KERN_ALERT
+                "SA will use MAC0 with descriptor for configuration %d\n",
+                pdata->tx_sa_ctrl_via_desc);
+      } else {
+         printk(KERN_ALERT
+                "Device doesn't supports SA Insertion/Replacement\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_SA1_DESC_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         pdata->tx_sa_ctrl_via_desc = req->flags;
+         pdata->tx_sa_ctrl_via_reg = DWC_ETH_QOS_SA1_NONE;
+         if (req->flags == DWC_ETH_QOS_SA1_NONE) {
+            memcpy(pdata->mac_addr, pdata->dev->dev_addr,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         } else {
+            memcpy(pdata->mac_addr, mac_addr1,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         }
+         hw_if->update_mac_addr(1, pdata->mac_addr);
+         hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
+         printk(KERN_ALERT
+                "SA will use MAC1 with descriptor for configuration %d\n",
+                pdata->tx_sa_ctrl_via_desc);
+      } else {
+         printk(KERN_ALERT
+                "Device doesn't supports SA Insertion/Replacement\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_SA0_REG_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         pdata->tx_sa_ctrl_via_reg = req->flags;
+         pdata->tx_sa_ctrl_via_desc = DWC_ETH_QOS_SA0_NONE;
+         if (req->flags == DWC_ETH_QOS_SA0_NONE) {
+            memcpy(pdata->mac_addr, pdata->dev->dev_addr,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         } else {
+            memcpy(pdata->mac_addr, mac_addr0,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         }
+         hw_if->update_mac_addr(0, pdata->mac_addr);
+         hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
+         printk(KERN_ALERT
+                "SA will use MAC0 with register for configuration %d\n",
+                pdata->tx_sa_ctrl_via_desc);
+      } else {
+         printk(KERN_ALERT
+                "Device doesn't supports SA Insertion/Replacement\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_SA1_REG_CMD:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         pdata->tx_sa_ctrl_via_reg = req->flags;
+         pdata->tx_sa_ctrl_via_desc = DWC_ETH_QOS_SA1_NONE;
+         if (req->flags == DWC_ETH_QOS_SA1_NONE) {
+            memcpy(pdata->mac_addr, pdata->dev->dev_addr,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         } else {
+            memcpy(pdata->mac_addr, mac_addr1,
+                   DWC_ETH_QOS_MAC_ADDR_LEN);
+         }
+         hw_if->update_mac_addr(1, pdata->mac_addr);
+         hw_if->configure_sa_via_reg(pdata->tx_sa_ctrl_via_reg);
+         printk(KERN_ALERT
+                "SA will use MAC1 with register for configuration %d\n",
+                pdata->tx_sa_ctrl_via_desc);
+      } else {
+         printk(KERN_ALERT
+                "Device doesn't supports SA Insertion/Replacement\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_SETUP_CONTEXT_DESCRIPTOR:
+      if (pdata->hw_feat.sa_vlan_ins) {
+         tx_desc_data->context_setup = req->context_setup;
+         if (tx_desc_data->context_setup == 1) {
+            printk(KERN_ALERT "Context descriptor will be transmitted"\
+                  " with every normal descriptor on %d DMA Channel\n",
+                  qInx);
+         }
+         else {
+            printk(KERN_ALERT "Context descriptor will be setup"\
+                  " only if VLAN id changes %d\n", qInx);
+         }
+      }
+      else {
+         printk(KERN_ALERT
+                "Device doesn't support VLAN operations\n");
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+
+   case DWC_ETH_QOS_GET_RX_QCNT:
+      req->qInx = DWC_ETH_QOS_RX_QUEUE_CNT;
+      break;
+
+   case DWC_ETH_QOS_GET_TX_QCNT:
+      req->qInx = DWC_ETH_QOS_TX_QUEUE_CNT;
+      break;
+
+   case DWC_ETH_QOS_GET_CONNECTED_SPEED:
+      req->connected_speed = pdata->speed;
+      break;
+
+   case DWC_ETH_QOS_DCB_ALGORITHM:
+      DWC_ETH_QOS_program_dcb_algorithm(pdata, req);
+      break;
+
+   case DWC_ETH_QOS_AVB_ALGORITHM:
+      DWC_ETH_QOS_program_avb_algorithm(pdata, req);
+      break;
+
+   case DWC_ETH_QOS_RX_SPLIT_HDR_CMD:
+      if (pdata->hw_feat.sph_en) {
+         ret = DWC_ETH_QOS_config_rx_split_hdr_mode(dev, req->flags);
+         if (ret == 0)
+            ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+         else
+            ret = DWC_ETH_QOS_CONFIG_FAIL;
+      } else {
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+   case DWC_ETH_QOS_L3_L4_FILTER_CMD:
+      if (pdata->hw_feat.l3l4_filter_num > 0) {
+         ret = DWC_ETH_QOS_config_l3_l4_filtering(dev, req->flags);
+         if (ret == 0)
+            ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+         else
+            ret = DWC_ETH_QOS_CONFIG_FAIL;
+      } else {
+         ret = DWC_ETH_QOS_NO_HW_SUPPORT;
+      }
+      break;
+   case DWC_ETH_QOS_IPV4_FILTERING_CMD:
+      ret = DWC_ETH_QOS_config_ip4_filters(dev, req);
+      break;
+   case DWC_ETH_QOS_IPV6_FILTERING_CMD:
+      ret = DWC_ETH_QOS_config_ip6_filters(dev, req);
+      break;
+   case DWC_ETH_QOS_UDP_FILTERING_CMD:
+      ret = DWC_ETH_QOS_config_tcp_udp_filters(dev, req, 1);
+      break;
+   case DWC_ETH_QOS_TCP_FILTERING_CMD:
+      ret = DWC_ETH_QOS_config_tcp_udp_filters(dev, req, 0);
+      break;
+   case DWC_ETH_QOS_VLAN_FILTERING_CMD:
+      ret = DWC_ETH_QOS_config_vlan_filter(dev, req);
+      break;
+   case DWC_ETH_QOS_L2_DA_FILTERING_CMD:
+      ret = DWC_ETH_QOS_confing_l2_da_filter(dev, req);
+      break;
+   case DWC_ETH_QOS_ARP_OFFLOAD_CMD:
+      ret = DWC_ETH_QOS_config_arp_offload(dev, req);
+      break;
+   case DWC_ETH_QOS_AXI_PBL_CMD:
+      pdata->axi_pbl = req->flags;
+      hw_if->config_axi_pbl_val(pdata->axi_pbl);
+      printk(KERN_ALERT "AXI PBL value: %d\n", pdata->axi_pbl);
+      break;
+   case DWC_ETH_QOS_AXI_WORL_CMD:
+      pdata->axi_worl = req->flags;
+      hw_if->config_axi_worl_val(pdata->axi_worl);
+      printk(KERN_ALERT "AXI WORL value: %d\n", pdata->axi_worl);
+      break;
+   case DWC_ETH_QOS_AXI_RORL_CMD:
+      pdata->axi_rorl = req->flags;
+      hw_if->config_axi_rorl_val(pdata->axi_rorl);
+      printk(KERN_ALERT "AXI RORL value: %d\n", pdata->axi_rorl);
+      break;
+   case DWC_ETH_QOS_MAC_LOOPBACK_MODE_CMD:
+      ret = DWC_ETH_QOS_config_mac_loopback_mode(dev, req->flags);
+      if (ret == 0)
+         ret = DWC_ETH_QOS_CONFIG_SUCCESS;
+      else
+         ret = DWC_ETH_QOS_CONFIG_FAIL;
+      break;
+   case DWC_ETH_QOS_PFC_CMD:
+      ret = DWC_ETH_QOS_config_pfc(dev, req->flags);
+      break;
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	case DWC_ETH_QOS_PG_TEST:
-		ret = DWC_ETH_QOS_handle_pg_ioctl(pdata, (void *)req);
-		break;
+   case DWC_ETH_QOS_PG_TEST:
+      ret = DWC_ETH_QOS_handle_pg_ioctl(pdata, (void *)req);
+      break;
 #endif /* end of DWC_ETH_QOS_CONFIG_PGTEST */
-	default:
-		ret = -EOPNOTSUPP;
-		printk(KERN_ALERT "Unsupported command call\n");
-	}
+   default:
+      ret = -EOPNOTSUPP;
+      printk(KERN_ALERT "Unsupported command call\n");
+   }
 
-	DBGPR("<--DWC_ETH_QOS_handle_prv_ioctl\n");
+   DBGPR("<--DWC_ETH_QOS_handle_prv_ioctl\n");
 
-	return ret;
+   return ret;
 }
 
-
 /*!
  * \brief control hw timestamping.
  *
@@ -4818,223 +4833,222 @@ static int DWC_ETH_QOS_handle_prv_ioctl(
  * \retval 0 - success
  * \retval negative - failure
  */
-
 static int DWC_ETH_QOS_handle_hwtstamp_ioctl(struct DWC_ETH_QOS_prv_data *pdata,
-	struct ifreq *ifr)
+   struct ifreq *ifr)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	struct hwtstamp_config config;
-	uint32_t ptp_v2 = 0;
-	uint32_t tstamp_all = 0;
-	uint32_t ptp_over_ipv4_udp = 0;
-	uint32_t ptp_over_ipv6_udp = 0;
-	uint32_t ptp_over_ethernet = 0;
-	uint32_t snap_type_sel = 0;
-	uint32_t ts_master_en = 0;
-	uint32_t ts_event_en = 0;
-	uint32_t av_8021asm_en = 0;
-	uint32_t varMAC_TCR = 0;
-	uint64_t temp = 0;
-	struct timespec now;
-
-	DBGPR_PTP("-->DWC_ETH_QOS_handle_hwtstamp_ioctl\n");
-
-	if (!pdata->hw_feat.tsstssel) {
-		printk(KERN_ALERT "No hw timestamping is available in this core\n");
-		return -EOPNOTSUPP;
-	}
-
-	if (copy_from_user(&config, ifr->ifr_data,
-		sizeof(struct hwtstamp_config)))
-		return -EFAULT;
-
-	DBGPR_PTP("config.flags = %#x, tx_type = %#x, rx_filter = %#x\n",
-		config.flags, config.tx_type, config.rx_filter);
-
-	/* reserved for future extensions */
-	if (config.flags)
-		return -EINVAL;
-
-	switch (config.tx_type) {
-	case HWTSTAMP_TX_OFF:
-		pdata->hwts_tx_en = 0;
-		break;
-	case HWTSTAMP_TX_ON:
-		pdata->hwts_tx_en = 1;
-		break;
-	default:
-		return -ERANGE;
-	}
-
-	switch (config.rx_filter) {
-	/* time stamp no incoming packet at all */
-	case HWTSTAMP_FILTER_NONE:
-		config.rx_filter = HWTSTAMP_FILTER_NONE;
-		break;
-
-	/* PTP v1, UDP, any kind of event packet */
-	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_EVENT;
-		/* take time stamp for all event messages */
-		snap_type_sel = MAC_TCR_SNAPTYPSEL_1;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		break;
-
-	/* PTP v1, UDP, Sync packet */
-	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_SYNC;
-		/* take time stamp for SYNC messages only */
-		ts_event_en = MAC_TCR_TSEVENTENA;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		break;
-
-	/* PTP v1, UDP, Delay_req packet */
-	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ;
-		/* take time stamp for Delay_Req messages only */
-		ts_master_en = MAC_TCR_TSMASTERENA;
-		ts_event_en = MAC_TCR_TSEVENTENA;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		break;
-
-	/* PTP v2, UDP, any kind of event packet */
-	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_EVENT;
-		ptp_v2 = MAC_TCR_TSVER2ENA;
-		/* take time stamp for all event messages */
-		snap_type_sel = MAC_TCR_SNAPTYPSEL_1;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		break;
-
-	/* PTP v2, UDP, Sync packet */
-	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_SYNC;
-		ptp_v2 = MAC_TCR_TSVER2ENA;
-		/* take time stamp for SYNC messages only */
-		ts_event_en = MAC_TCR_TSEVENTENA;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		break;
-
-	/* PTP v2, UDP, Delay_req packet */
-	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ;
-		ptp_v2 = MAC_TCR_TSVER2ENA;
-		/* take time stamp for Delay_Req messages only */
-		ts_master_en = MAC_TCR_TSMASTERENA;
-		ts_event_en = MAC_TCR_TSEVENTENA;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		break;
-
-	/* PTP v2/802.AS1, any layer, any kind of event packet */
-	case HWTSTAMP_FILTER_PTP_V2_EVENT:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_EVENT;
-		ptp_v2 = MAC_TCR_TSVER2ENA;
-		/* take time stamp for all event messages */
-		snap_type_sel = MAC_TCR_SNAPTYPSEL_1;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		ptp_over_ethernet = MAC_TCR_TSIPENA;
-		av_8021asm_en = MAC_TCR_AV8021ASMEN;
-		break;
-
-	/* PTP v2/802.AS1, any layer, Sync packet */
-	case HWTSTAMP_FILTER_PTP_V2_SYNC:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_SYNC;
-		ptp_v2 = MAC_TCR_TSVER2ENA;
-		/* take time stamp for SYNC messages only */
-		ts_event_en = MAC_TCR_TSEVENTENA;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		ptp_over_ethernet = MAC_TCR_TSIPENA;
-		av_8021asm_en = MAC_TCR_AV8021ASMEN;
-		break;
-
-	/* PTP v2/802.AS1, any layer, Delay_req packet */
-	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
-		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_DELAY_REQ;
-		ptp_v2 = MAC_TCR_TSVER2ENA;
-		/* take time stamp for Delay_Req messages only */
-		ts_master_en = MAC_TCR_TSMASTERENA;
-		ts_event_en = MAC_TCR_TSEVENTENA;
-
-		ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
-		ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
-		ptp_over_ethernet = MAC_TCR_TSIPENA;
-		av_8021asm_en = MAC_TCR_AV8021ASMEN;
-		break;
-
-	/* time stamp any incoming packet */
-	case HWTSTAMP_FILTER_ALL:
-		config.rx_filter = HWTSTAMP_FILTER_ALL;
-		tstamp_all = MAC_TCR_TSENALL;
-		break;
-
-	default:
-		return -ERANGE;
-	}
-	pdata->hwts_rx_en = ((config.rx_filter == HWTSTAMP_FILTER_NONE) ? 0 : 1);
-
-	if (!pdata->hwts_tx_en && !pdata->hwts_rx_en) {
-		/* disable hw time stamping */
-		hw_if->config_hw_time_stamping(varMAC_TCR);
-	} else {
-		varMAC_TCR = (MAC_TCR_TSENA | MAC_TCR_TSCFUPDT | MAC_TCR_TSCTRLSSR |
-				tstamp_all | ptp_v2 | ptp_over_ethernet | ptp_over_ipv6_udp |
-				ptp_over_ipv4_udp | ts_event_en | ts_master_en |
-				snap_type_sel | av_8021asm_en);
-
-		if (!pdata->one_nsec_accuracy)
-			varMAC_TCR &= ~MAC_TCR_TSCTRLSSR;
-
-		hw_if->config_hw_time_stamping(varMAC_TCR);
-
-		/* program Sub Second Increment Reg */
-		hw_if->config_sub_second_increment(DWC_ETH_QOS_SYSCLOCK);
-
-		/* formula is :
-		 * addend = 2^32/freq_div_ratio;
-		 *
-		 * where, freq_div_ratio = DWC_ETH_QOS_SYSCLOCK/50MHz
-		 *
-		 * hence, addend = ((2^32) * 50MHz)/DWC_ETH_QOS_SYSCLOCK;
-		 *
-		 * NOTE: DWC_ETH_QOS_SYSCLOCK should be >= 50MHz to
-		 *       achive 20ns accuracy.
-		 *
-		 * 2^x * y == (y << x), hence
-		 * 2^32 * 50000000 ==> (50000000 << 32)
-		 * */
-		temp = (uint64_t)(50000000ULL << 32);
-		pdata->default_addend = div_u64(temp, 62500000);
-
-		hw_if->config_addend(pdata->default_addend);
-
-		/* initialize system time */
-		getnstimeofday(&now);
-		hw_if->init_systime(now.tv_sec, now.tv_nsec);
-	}
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   struct hwtstamp_config config;
+   uint32_t ptp_v2 = 0;
+   uint32_t tstamp_all = 0;
+   uint32_t ptp_over_ipv4_udp = 0;
+   uint32_t ptp_over_ipv6_udp = 0;
+   uint32_t ptp_over_ethernet = 0;
+   uint32_t snap_type_sel = 0;
+   uint32_t ts_master_en = 0;
+   uint32_t ts_event_en = 0;
+   uint32_t av_8021asm_en = 0;
+   uint32_t varMAC_TCR = 0;
+   uint64_t temp = 0;
+   struct timespec now;
+
+   DBGPR_PTP("-->DWC_ETH_QOS_handle_hwtstamp_ioctl\n");
+
+   if (!pdata->hw_feat.tsstssel) {
+      printk(KERN_ALERT "No hw timestamping is available in this core\n");
+      return -EOPNOTSUPP;
+   }
+
+   if (copy_from_user(&config, ifr->ifr_data,
+      sizeof(struct hwtstamp_config)))
+      return -EFAULT;
+
+   DBGPR_PTP("config.flags = %#x, tx_type = %#x, rx_filter = %#x\n",
+      config.flags, config.tx_type, config.rx_filter);
+
+   /* reserved for future extensions */
+   if (config.flags)
+      return -EINVAL;
+
+   switch (config.tx_type) {
+   case HWTSTAMP_TX_OFF:
+      pdata->hwts_tx_en = 0;
+      break;
+   case HWTSTAMP_TX_ON:
+      pdata->hwts_tx_en = 1;
+      break;
+   default:
+      return -ERANGE;
+   }
 
-	DBGPR_PTP("config.flags = %#x, tx_type = %#x, rx_filter = %#x\n",
-		config.flags, config.tx_type, config.rx_filter);
+   switch (config.rx_filter) {
+   /* time stamp no incoming packet at all */
+   case HWTSTAMP_FILTER_NONE:
+      config.rx_filter = HWTSTAMP_FILTER_NONE;
+      break;
+
+   /* PTP v1, UDP, any kind of event packet */
+   case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_EVENT;
+      /* take time stamp for all event messages */
+      snap_type_sel = MAC_TCR_SNAPTYPSEL_1;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      break;
+
+   /* PTP v1, UDP, Sync packet */
+   case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_SYNC;
+      /* take time stamp for SYNC messages only */
+      ts_event_en = MAC_TCR_TSEVENTENA;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      break;
+
+   /* PTP v1, UDP, Delay_req packet */
+   case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ;
+      /* take time stamp for Delay_Req messages only */
+      ts_master_en = MAC_TCR_TSMASTERENA;
+      ts_event_en = MAC_TCR_TSEVENTENA;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      break;
+
+   /* PTP v2, UDP, any kind of event packet */
+   case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_EVENT;
+      ptp_v2 = MAC_TCR_TSVER2ENA;
+      /* take time stamp for all event messages */
+      snap_type_sel = MAC_TCR_SNAPTYPSEL_1;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      break;
+
+   /* PTP v2, UDP, Sync packet */
+   case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_SYNC;
+      ptp_v2 = MAC_TCR_TSVER2ENA;
+      /* take time stamp for SYNC messages only */
+      ts_event_en = MAC_TCR_TSEVENTENA;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      break;
+
+   /* PTP v2, UDP, Delay_req packet */
+   case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ;
+      ptp_v2 = MAC_TCR_TSVER2ENA;
+      /* take time stamp for Delay_Req messages only */
+      ts_master_en = MAC_TCR_TSMASTERENA;
+      ts_event_en = MAC_TCR_TSEVENTENA;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      break;
+
+   /* PTP v2/802.AS1, any layer, any kind of event packet */
+   case HWTSTAMP_FILTER_PTP_V2_EVENT:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V2_EVENT;
+      ptp_v2 = MAC_TCR_TSVER2ENA;
+      /* take time stamp for all event messages */
+      snap_type_sel = MAC_TCR_SNAPTYPSEL_1;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      ptp_over_ethernet = MAC_TCR_TSIPENA;
+      av_8021asm_en = MAC_TCR_AV8021ASMEN;
+      break;
+
+   /* PTP v2/802.AS1, any layer, Sync packet */
+   case HWTSTAMP_FILTER_PTP_V2_SYNC:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V2_SYNC;
+      ptp_v2 = MAC_TCR_TSVER2ENA;
+      /* take time stamp for SYNC messages only */
+      ts_event_en = MAC_TCR_TSEVENTENA;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      ptp_over_ethernet = MAC_TCR_TSIPENA;
+      av_8021asm_en = MAC_TCR_AV8021ASMEN;
+      break;
+
+   /* PTP v2/802.AS1, any layer, Delay_req packet */
+   case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+      config.rx_filter = HWTSTAMP_FILTER_PTP_V2_DELAY_REQ;
+      ptp_v2 = MAC_TCR_TSVER2ENA;
+      /* take time stamp for Delay_Req messages only */
+      ts_master_en = MAC_TCR_TSMASTERENA;
+      ts_event_en = MAC_TCR_TSEVENTENA;
+
+      ptp_over_ipv4_udp = MAC_TCR_TSIPV4ENA;
+      ptp_over_ipv6_udp = MAC_TCR_TSIPV6ENA;
+      ptp_over_ethernet = MAC_TCR_TSIPENA;
+      av_8021asm_en = MAC_TCR_AV8021ASMEN;
+      break;
+
+   /* time stamp any incoming packet */
+   case HWTSTAMP_FILTER_ALL:
+      config.rx_filter = HWTSTAMP_FILTER_ALL;
+      tstamp_all = MAC_TCR_TSENALL;
+      break;
 
-	DBGPR_PTP("<--DWC_ETH_QOS_handle_hwtstamp_ioctl\n");
+   default:
+      return -ERANGE;
+   }
+   pdata->hwts_rx_en = ((config.rx_filter == HWTSTAMP_FILTER_NONE) ? 0 : 1);
+
+   if (!pdata->hwts_tx_en && !pdata->hwts_rx_en) {
+      /* disable hw time stamping */
+      hw_if->config_hw_time_stamping(varMAC_TCR);
+   } else {
+      varMAC_TCR = (MAC_TCR_TSENA | MAC_TCR_TSCFUPDT | MAC_TCR_TSCTRLSSR |
+            tstamp_all | ptp_v2 | ptp_over_ethernet | ptp_over_ipv6_udp |
+            ptp_over_ipv4_udp | ts_event_en | ts_master_en |
+            snap_type_sel | av_8021asm_en);
+
+      if (!pdata->one_nsec_accuracy)
+         varMAC_TCR &= ~MAC_TCR_TSCTRLSSR;
+
+      hw_if->config_hw_time_stamping(varMAC_TCR);
+
+      /* program Sub Second Increment Reg */
+      hw_if->config_sub_second_increment(DWC_ETH_QOS_SYSCLOCK);
+
+      /* formula is :
+       * addend = 2^32/freq_div_ratio;
+       *
+       * where, freq_div_ratio = DWC_ETH_QOS_SYSCLOCK/50MHz
+       *
+       * hence, addend = ((2^32) * 50MHz)/DWC_ETH_QOS_SYSCLOCK;
+       *
+       * NOTE: DWC_ETH_QOS_SYSCLOCK should be >= 50MHz to
+       *       achive 20ns accuracy.
+       *
+       * 2^x * y == (y << x), hence
+       * 2^32 * 50000000 ==> (50000000 << 32)
+       * */
+      temp = (uint64_t)(50000000ULL << 32);
+      pdata->default_addend = div_u64(temp, 62500000);
+
+      hw_if->config_addend(pdata->default_addend);
+
+      /* initialize system time */
+      getnstimeofday(&now);
+      hw_if->init_systime(now.tv_sec, now.tv_nsec);
+   }
+
+   DBGPR_PTP("config.flags = %#x, tx_type = %#x, rx_filter = %#x\n",
+      config.flags, config.tx_type, config.rx_filter);
 
-	return (copy_to_user(ifr->ifr_data, &config,
-		sizeof(struct hwtstamp_config))) ? -EFAULT : 0;
+   DBGPR_PTP("<--DWC_ETH_QOS_handle_hwtstamp_ioctl\n");
+
+   return (copy_to_user(ifr->ifr_data, &config,
+      sizeof(struct hwtstamp_config))) ? -EFAULT : 0;
 }
 
 /*!
@@ -5059,62 +5073,62 @@ static int DWC_ETH_QOS_handle_hwtstamp_i
  */
 static int DWC_ETH_QOS_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct ifr_data_struct *req = ifr->ifr_ifru.ifru_data;
-	struct mii_ioctl_data *data = if_mii(ifr);
-	unsigned int reg_val = 0;
-	int ret = 0;
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   struct ifr_data_struct *req = ifr->ifr_ifru.ifru_data;
+   struct mii_ioctl_data *data = if_mii(ifr);
+   unsigned int reg_val = 0;
+   int ret = 0;
 
-	DBGPR("-->DWC_ETH_QOS_ioctl\n");
+   DBGPR("-->DWC_ETH_QOS_ioctl\n");
 
 #ifndef DWC_ETH_QOS_CONFIG_PGTEST
-	if ((!netif_running(dev)) || (!pdata->phydev)) {
-		DBGPR("<--DWC_ETH_QOS_ioctl - error\n");
-		return -EINVAL;
-	}
-#endif
-
-	spin_lock(&pdata->lock);
-	switch (cmd) {
-	case SIOCGMIIPHY:
-		data->phy_id = pdata->phyaddr;
-		printk(KERN_ALERT "PHY ID: SIOCGMIIPHY\n");
-		break;
-
-	case SIOCGMIIREG:
-		ret =
-		    DWC_ETH_QOS_mdio_read_direct(pdata, pdata->phyaddr,
-				(data->reg_num & 0x1F), &reg_val);
-		if (ret)
-			ret = -EIO;
-
-		data->val_out = reg_val;
-		printk(KERN_ALERT "PHY ID: SIOCGMIIREG reg:%#x reg_val:%#x\n",
-		       (data->reg_num & 0x1F), reg_val);
-		break;
-
-	case SIOCSMIIREG:
-		printk(KERN_ALERT "PHY ID: SIOCSMIIPHY\n");
-		break;
-
-	case DWC_ETH_QOS_PRV_IOCTL:
-		ret = DWC_ETH_QOS_handle_prv_ioctl(pdata, req);
-		req->command_error = ret;
-		break;
-
-	case SIOCSHWTSTAMP:
-		ret = DWC_ETH_QOS_handle_hwtstamp_ioctl(pdata, ifr);
-		break;
-
-	default:
-		ret = -EOPNOTSUPP;
-		printk(KERN_ALERT "Unsupported IOCTL call\n");
-	}
-	spin_unlock(&pdata->lock);
+   if ((!netif_running(dev)) || (!pdata->phydev)) {
+      DBGPR("<--DWC_ETH_QOS_ioctl - error\n");
+      return -EINVAL;
+   }
+#endif
 
-	DBGPR("<--DWC_ETH_QOS_ioctl\n");
+   spin_lock(&pdata->lock);
+   switch (cmd) {
+   case SIOCGMIIPHY:
+      data->phy_id = pdata->phyaddr;
+      printk(KERN_ALERT "PHY ID: SIOCGMIIPHY\n");
+      break;
+
+   case SIOCGMIIREG:
+      ret =
+          DWC_ETH_QOS_mdio_read_direct(pdata, pdata->phyaddr,
+            (data->reg_num & 0x1F), &reg_val);
+      if (ret)
+         ret = -EIO;
+
+      data->val_out = reg_val;
+      printk(KERN_ALERT "PHY ID: SIOCGMIIREG reg:%#x reg_val:%#x\n",
+             (data->reg_num & 0x1F), reg_val);
+      break;
+
+   case SIOCSMIIREG:
+      printk(KERN_ALERT "PHY ID: SIOCSMIIPHY\n");
+      break;
+
+   case DWC_ETH_QOS_PRV_IOCTL:
+      ret = DWC_ETH_QOS_handle_prv_ioctl(pdata, req);
+      req->command_error = ret;
+      break;
+
+   case SIOCSHWTSTAMP:
+      ret = DWC_ETH_QOS_handle_hwtstamp_ioctl(pdata, ifr);
+      break;
+
+   default:
+      ret = -EOPNOTSUPP;
+      printk(KERN_ALERT "Unsupported IOCTL call\n");
+   }
+   spin_unlock(&pdata->lock);
+
+   DBGPR("<--DWC_ETH_QOS_ioctl\n");
 
-	return ret;
+   return ret;
 }
 
 /*!
@@ -5135,90 +5149,87 @@ static int DWC_ETH_QOS_ioctl(struct net_
 */
 static int DWC_ETH_QOS_change_mtu(struct net_device *dev, int new_mtu)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	int max_frame = (new_mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   int max_frame = (new_mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);
+   gbe_power_state_t state;
 
-	DBGPR("-->DWC_ETH_QOS_change_mtu: new_mtu:%d\n", new_mtu);
+   CFG_PRINT("-->DWC_ETH_QOS_change_mtu: new_mtu:%d\n", new_mtu);
 
 #ifdef DWC_ETH_QOS_CONFIG_PGTEST
-	printk(KERN_ALERT "jumbo frames not supported with PG test\n");
-	return -EOPNOTSUPP;
+   printk(KERN_ALERT "jumbo frames not supported with PG test\n");
+   return -EOPNOTSUPP;
 #endif
-	if (dev->mtu == new_mtu) {
-		printk(KERN_ALERT "%s: is already configured to %d mtu\n",
-		       dev->name, new_mtu);
-		return 0;
-	}
-
-	/* Supported frame sizes */
-	if ((new_mtu < DWC_ETH_QOS_MIN_SUPPORTED_MTU) ||
-	    (max_frame > DWC_ETH_QOS_MAX_SUPPORTED_MTU)) {
-		printk(KERN_ALERT
-		       "%s: invalid MTU, min %d and max %d MTU are supported\n",
-		       dev->name, DWC_ETH_QOS_MIN_SUPPORTED_MTU,
-		       DWC_ETH_QOS_MAX_SUPPORTED_MTU);
-		return -EINVAL;
-	}
-
-	printk(KERN_ALERT "changing MTU from %d to %d\n", dev->mtu, new_mtu);
-
-	DWC_ETH_QOS_stop_dev(pdata);
-
-	if (max_frame <= 2048)
-		pdata->rx_buffer_len = 2048;
-	else
-		pdata->rx_buffer_len = PAGE_SIZE; /* in case of JUMBO frame,
-						max buffer allocated is
-						PAGE_SIZE */
-
-	if ((max_frame == ETH_FRAME_LEN + ETH_FCS_LEN) ||
-	    (max_frame == ETH_FRAME_LEN + ETH_FCS_LEN + VLAN_HLEN))
-		pdata->rx_buffer_len =
-		    DWC_ETH_QOS_ETH_FRAME_LEN;
+   if (dev->mtu == new_mtu) {
+      printk(KERN_ALERT "%s: is already configured to %d mtu\n",
+             dev->name, new_mtu);
+      return 0;
+   }
 
-	dev->mtu = new_mtu;
+   /* Supported frame sizes */
+   if ((new_mtu < DWC_ETH_QOS_MIN_SUPPORTED_MTU) ||
+       (max_frame > DWC_ETH_QOS_MAX_SUPPORTED_MTU)) {
+      printk(KERN_ALERT
+             "%s: invalid MTU, min %d and max %d MTU are supported\n",
+             dev->name, DWC_ETH_QOS_MIN_SUPPORTED_MTU,
+             DWC_ETH_QOS_MAX_SUPPORTED_MTU);
+      return -EINVAL;
+   }
 
-	DWC_ETH_QOS_start_dev(pdata);
+   state = DWC_ETH_QOS_stop_dev(pdata);
 
-	DBGPR("<--DWC_ETH_QOS_change_mtu\n");
+   CFG_PRINT("Changing MTU from %d to %d\n", dev->mtu, new_mtu);
+   /* For JUMBO frame, max buffer allocated is PAGE_SIZE */
+   pdata->rx_buffer_len = (max_frame <= 2048)? 2048 : PAGE_SIZE;
+   if ((max_frame == ETH_FRAME_LEN + ETH_FCS_LEN) ||
+       (max_frame == ETH_FRAME_LEN + ETH_FCS_LEN + VLAN_HLEN))
+      pdata->rx_buffer_len = DWC_ETH_QOS_ETH_FRAME_LEN;
+   dev->mtu = new_mtu;
+
+   if (state == GBE_RUN_STATE) {
+      DWC_ETH_QOS_start_dev(pdata);
+   } else if (state == GBE_STANDBY_STATE) {
+      /* Save request to apply it when device is powered up */
+      pdata->power_state |= DWC_ETH_QOS_NETIP_MTU_REQ;
+   }
 
-	return 0;
+   CFG_PRINT("<--DWC_ETH_QOS_change_mtu\n");
+   return 0;
 }
 
 #ifdef DWC_ETH_QOS_QUEUE_SELECT_ALGO
-u16	DWC_ETH_QOS_select_queue(struct net_device *dev,
-			struct sk_buff *skb)
+u16   DWC_ETH_QOS_select_queue(struct net_device *dev,
+         struct sk_buff *skb)
 {
-	static u16 txqueue_select = 0;
+   static u16 txqueue_select = 0;
 
-	DBGPR("-->DWC_ETH_QOS_select_queue\n");
+   DBGPR("-->DWC_ETH_QOS_select_queue\n");
 
-	txqueue_select = skb_tx_hash(dev, skb);
+   txqueue_select = skb_tx_hash(dev, skb);
 
-	DBGPR("<--DWC_ETH_QOS_select_queue txqueue-select:%d\n",
-		txqueue_select);
+   DBGPR("<--DWC_ETH_QOS_select_queue txqueue-select:%d\n",
+      txqueue_select);
 
-	return txqueue_select;
+   return txqueue_select;
 }
 #endif
 
 unsigned int crc32_snps_le(unsigned int initval, unsigned char *data, unsigned int size)
 {
-	unsigned int crc = initval;
-	unsigned int poly = 0x04c11db7;
-	unsigned int temp = 0;
-	unsigned char my_data = 0;
-	int bit_count;
-	for(bit_count = 0; bit_count < size; bit_count++) {
-		if((bit_count % 8) == 0) my_data = data[bit_count/8];
-		DBGPR_FILTER("%s my_data = %x crc=%x\n", __func__, my_data,crc);
-		temp = ((crc >> 31) ^  my_data) &  0x1;
-		crc <<= 1;
-		if(temp != 0) crc ^= poly;
-		my_data >>=1;
-	}
-		DBGPR_FILTER("%s my_data = %x crc=%x\n", __func__, my_data,crc);
-	return ~crc;
+   unsigned int crc = initval;
+   unsigned int poly = 0x04c11db7;
+   unsigned int temp = 0;
+   unsigned char my_data = 0;
+   int bit_count;
+   for(bit_count = 0; bit_count < size; bit_count++) {
+      if((bit_count % 8) == 0) my_data = data[bit_count/8];
+      DBGPR_FILTER("%s my_data = %x crc=%x\n", __func__, my_data,crc);
+      temp = ((crc >> 31) ^  my_data) &  0x1;
+      crc <<= 1;
+      if(temp != 0) crc ^= poly;
+      my_data >>=1;
+   }
+      DBGPR_FILTER("%s my_data = %x crc=%x\n", __func__, my_data,crc);
+   return ~crc;
 }
 
 /*!
@@ -5237,37 +5248,37 @@ unsigned int crc32_snps_le(unsigned int
 static int DWC_ETH_QOS_vlan_rx_kill_vid(struct net_device *dev,
       __always_unused __be16 proto, u16 vid)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	unsigned short new_index, old_index;
-	int crc32_val = 0;
-	unsigned int enb_12bit_vhash;
-
-	DBGPR("-->DWC_ETH_QOS_vlan_rx_kill_vid: vid = %d\n", vid);
-
-	if (pdata->vlan_hash_filtering) {
-		crc32_val = (bitrev32(~crc32_le(~0, (unsigned char *)&vid, 2)) >> 28);
-
-		enb_12bit_vhash = hw_if->get_vlan_tag_comparison();
-		if (enb_12bit_vhash) {
-			/* neget 4-bit crc value for 12-bit VLAN hash comparison */
-			new_index = (1 << (~crc32_val & 0xF));
-		} else {
-			new_index = (1 << (crc32_val & 0xF));
-		}
-
-		old_index = hw_if->get_vlan_hash_table_reg();
-		old_index &= ~new_index;
-		hw_if->update_vlan_hash_table_reg(old_index);
-		pdata->vlan_ht_or_id = old_index;
-	} else {
-		/* By default, receive only VLAN pkt with VID = 1
-		 * becasue writting 0 will pass all VLAN pkt */
-		hw_if->update_vlan_id(1);
-		pdata->vlan_ht_or_id = 1;
-	}
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   unsigned short new_index, old_index;
+   int crc32_val = 0;
+   unsigned int enb_12bit_vhash;
+
+   DBGPR("-->DWC_ETH_QOS_vlan_rx_kill_vid: vid = %d\n", vid);
+
+   if (pdata->vlan_hash_filtering) {
+      crc32_val = (bitrev32(~crc32_le(~0, (unsigned char *)&vid, 2)) >> 28);
+
+      enb_12bit_vhash = hw_if->get_vlan_tag_comparison();
+      if (enb_12bit_vhash) {
+         /* neget 4-bit crc value for 12-bit VLAN hash comparison */
+         new_index = (1 << (~crc32_val & 0xF));
+      } else {
+         new_index = (1 << (crc32_val & 0xF));
+      }
+
+      old_index = hw_if->get_vlan_hash_table_reg();
+      old_index &= ~new_index;
+      hw_if->update_vlan_hash_table_reg(old_index);
+      pdata->vlan_ht_or_id = old_index;
+   } else {
+      /* By default, receive only VLAN pkt with VID = 1
+       * becasue writting 0 will pass all VLAN pkt */
+      hw_if->update_vlan_id(1);
+      pdata->vlan_ht_or_id = 1;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_vlan_rx_kill_vid\n");
+   DBGPR("<--DWC_ETH_QOS_vlan_rx_kill_vid\n");
    return 0;
 }
 
@@ -5289,43 +5300,85 @@ static int DWC_ETH_QOS_vlan_rx_kill_vid(
 static int DWC_ETH_QOS_vlan_rx_add_vid(struct net_device *dev,
       __always_unused __be16 proto, u16 vid)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	unsigned short new_index, old_index;
-	int crc32_val = 0;
-	unsigned int enb_12bit_vhash;
-
-	CFG_PRINT("-->DWC_ETH_QOS_vlan_rx_add_vid: vid = %d hash = %d\n",
-			vid, pdata->vlan_hash_filtering);
-
-	if (pdata->vlan_hash_filtering) {
-		/* The upper 4 bits of the calculated CRC are used to
-		 * index the content of the VLAN Hash Table Reg.
-		 * */
-		crc32_val = (bitrev32(~crc32_le(~0, (unsigned char *)&vid, 2)) >> 28);
-
-		/* These 4(0xF) bits determines the bit within the
-		 * VLAN Hash Table Reg 0
-		 * */
-		enb_12bit_vhash = hw_if->get_vlan_tag_comparison();
-		if (enb_12bit_vhash) {
-			/* neget 4-bit crc value for 12-bit VLAN hash comparison */
-			new_index = (1 << (~crc32_val & 0xF));
-		} else {
-			new_index = (1 << (crc32_val & 0xF));
-		}
-
-		old_index = hw_if->get_vlan_hash_table_reg();
-		old_index |= new_index;
-		hw_if->update_vlan_hash_table_reg(old_index);
-		pdata->vlan_ht_or_id = old_index;
-	} else {
-		hw_if->update_vlan_id(vid);
-		pdata->vlan_ht_or_id = vid;
-	}
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   unsigned short new_index, old_index;
+   int crc32_val = 0;
+   unsigned int enb_12bit_vhash;
+
+   CFG_PRINT("-->DWC_ETH_QOS_vlan_rx_add_vid: vid = %d hash = %d\n",
+         vid, pdata->vlan_hash_filtering);
+
+   if (pdata->vlan_hash_filtering) {
+      /* The upper 4 bits of the calculated CRC are used to
+       * index the content of the VLAN Hash Table Reg.
+       * */
+      crc32_val = (bitrev32(~crc32_le(~0, (unsigned char *)&vid, 2)) >> 28);
+
+      /* These 4(0xF) bits determines the bit within the
+       * VLAN Hash Table Reg 0
+       * */
+      enb_12bit_vhash = hw_if->get_vlan_tag_comparison();
+      if (enb_12bit_vhash) {
+         /* neget 4-bit crc value for 12-bit VLAN hash comparison */
+         new_index = (1 << (~crc32_val & 0xF));
+      } else {
+         new_index = (1 << (crc32_val & 0xF));
+      }
 
-	CFG_PRINT("<--DWC_ETH_QOS_vlan_rx_add_vid\n");
-	return 0;
+      old_index = hw_if->get_vlan_hash_table_reg();
+      old_index |= new_index;
+      hw_if->update_vlan_hash_table_reg(old_index);
+      pdata->vlan_ht_or_id = old_index;
+   } else {
+      hw_if->update_vlan_id(vid);
+      pdata->vlan_ht_or_id = vid;
+   }
+
+   CFG_PRINT("<--DWC_ETH_QOS_vlan_rx_add_vid\n");
+   return 0;
+}
+
+void gbe_enter_standby(struct DWC_ETH_QOS_prv_data *pdata)
+{
+   uint32_t qInx;
+   hw_interface_t *hw_if = &pdata->hw_if;
+#ifdef GBE_POLLING
+   hrtimer_cancel(&pdata->gbe_timer);
+#endif
+   /* Stop MAC */
+   hw_if->stop_mac_tx_rx();
+   /* Disable interrupts in NetSS */
+   netss_interrupt_disable(NETSS_INTERUPT_GBE);
+   /* Disable MAC interrupts */
+   DWC_REG_WR(MAC_IER, 0);
+   /* Disable DMA interrupts */
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
+      DWC_REG_WR(DMA_IER(qInx), 0);
+   }
+   /* Indicate ISR that NetIP is powering down */
+   pdata->power_state = DWC_ETH_QOS_NETIP_PWRDWN;
+   /* Process and clear any pending interrupt */
+   DWC_ETH_QOS_ISR(pdata->irq_number, pdata);
+   /* Update power down status variable */
+   pdata->power_state = DWC_ETH_QOS_NETIP_WAKEUP;
+}
+
+void gbe_exit_standby(struct DWC_ETH_QOS_prv_data *pdata)
+{
+   uint32_t qInx;
+   hw_config_t *hw_cfg = &pdata->hw_cfg;
+   /* Enable interrupts in NetSS */
+   netss_interrupt_enable(NETSS_INTERUPT_GBE);
+   /* Enable MAC interrupts */
+   DWC_REG_WR(MAC_IER, hw_cfg->mac_ier);
+   /* Enable DMA interrupts */
+   for (qInx = 0; qInx < DWC_ETH_QOS_TX_QUEUE_CNT; qInx++) {
+      DWC_REG_WR(DMA_IER(qInx), hw_cfg->dma_ier);
+   }
+#ifdef GBE_POLLING
+   hrtimer_start(&pdata->gbe_timer, ktime_set(0, 100000), HRTIMER_MODE_REL);
+#endif
 }
 
 /*!
@@ -5342,61 +5395,81 @@ static int DWC_ETH_QOS_vlan_rx_add_vid(s
  *
  * \param[in] dev – pointer to net device structure.
  * \param[in] wakeup_type – remote wake-on-lan or magic packet.
- * \param[in] caller – netif_detach gets called conditionally based
- *                     on caller, IOCTL or DRIVER-suspend
  *
  * \return int
  *
  * \retval zero on success and -ve number on failure.
  */
-int DWC_ETH_QOS_powerdown(struct net_device *dev, uint32_t wakeup_type,
-		uint32_t caller)
+int DWC_ETH_QOS_powerdown(struct net_device *dev, uint32_t wakeup_type)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	unsigned long flags;
-
-	DBGPR(KERN_ALERT "-->DWC_ETH_QOS_powerdown\n");
-
-	if (!dev || !netif_running(dev) ||
-	    (caller == DWC_ETH_QOS_IOCTL_CONTEXT && pdata->power_down)) {
-		printk(KERN_ALERT
-		       "Device is already powered down and will powerup for %s\n",
-		       DWC_ETH_QOS_POWER_DOWN_TYPE(pdata));
-		DBGPR("<--DWC_ETH_QOS_powerdown\n");
-		return -EINVAL;
-	}
-
-	if (pdata->phydev)
-		phy_stop(pdata->phydev);
-
-	spin_lock_irqsave(&pdata->pmt_lock, flags);
+   struct DWC_ETH_QOS_prv_data *pdata = NULL;
+   hw_interface_t *hw_if = NULL;
+   unsigned long flags;
+   int ret = 0;
+
+   CFG_PRINT("-->DWC_ETH_QOS_powerdown (0x%08x)\n", wakeup_type);
+
+   if (!dev) {
+      ERR_PRINT("Invalid parameter!\n");
+      ret = -EINVAL;
+      goto exit_pwr_dwn_err;
+   }
 
-	if (caller == DWC_ETH_QOS_DRIVER_CONTEXT)
-		netif_device_detach(dev);
+   pdata = netdev_priv(dev);
+   hw_if = &(pdata->hw_if);
 
-	netif_tx_disable(dev);
-	DWC_ETH_QOS_napi_disable(pdata);
+   spin_lock_irqsave(&pdata->pmt_lock, flags);
 
-	/* stop DMA TX/RX */
-	DWC_ETH_QOS_stop_all_ch_tx_dma(pdata);
-	DWC_ETH_QOS_stop_all_ch_rx_dma(pdata);
+   if (!netif_running(dev)) {
+      WRN_PRINT("Device is not running!\n");
+      pdata->power_state = (wakeup_type & DWC_ETH_QOS_NETIP_WAKEUP);
+      goto exit_pwr_dwn;
+   }
 
-	/* enable power down mode by programming the PMT regs */
-	if (wakeup_type & DWC_ETH_QOS_REMOTE_WAKEUP)
-		hw_if->enable_remote_pmt();
-	if (wakeup_type & DWC_ETH_QOS_MAGIC_WAKEUP)
-		hw_if->enable_magic_pmt();
-	pdata->power_down_type = wakeup_type;
+   if (pdata->power_state &&
+     !(pdata->power_state & DWC_ETH_QOS_NETIP_WAKEUP) &&
+      (wakeup_type & DWC_ETH_QOS_NETIP_WAKEUP)) {
+      /* Process request started by the NetIP SS driver. */
+      gbe_enter_standby(pdata);
+      goto exit_pwr_dwn;
+   } else if (pdata->power_state) {
+      WRN_PRINT("Device is already powered down!\n");
+      goto exit_pwr_dwn;
+   }
 
-	if (caller == DWC_ETH_QOS_IOCTL_CONTEXT)
-		pdata->power_down = 1;
+   /* Stop PHY */
+   if (pdata->phydev)
+      phy_stop(pdata->phydev);
+   /* Detach net device */
+   netif_device_detach(dev);
+   /* Disable NAPI */
+   DWC_ETH_QOS_napi_disable(pdata);
+   /* Stop DMA TX/RX */
+   DWC_ETH_QOS_stop_all_ch_tx_dma(pdata);
+   DWC_ETH_QOS_stop_all_ch_rx_dma(pdata);
+
+   if (wakeup_type & DWC_ETH_QOS_NETIP_WAKEUP) {
+      /* Process request started by NetIP SS driver first */
+      gbe_enter_standby(pdata);
+   } else {
+      /* Enable PMT Interrupt */
+      VAR32_SET_BIT(pdata->hw_cfg.mac_ier, MAC_IER_PMTIE, 0x1);
+      DWC_REG_WR(MAC_IER, pdata->hw_cfg.mac_ier);
+      /* Enable power down mode by programming the PMT regs */
+      if (wakeup_type & DWC_ETH_QOS_REMOTE_WAKEUP)
+         hw_if->enable_remote_pmt();
+      if (wakeup_type & DWC_ETH_QOS_MAGIC_WAKEUP)
+         hw_if->enable_magic_pmt();
+      pdata->power_state = wakeup_type;
+   }
 
-	spin_unlock_irqrestore(&pdata->pmt_lock, flags);
+exit_pwr_dwn:
+   spin_unlock_irqrestore(&pdata->pmt_lock, flags);
 
-	DBGPR("<--DWC_ETH_QOS_powerdown\n");
+exit_pwr_dwn_err:
+   CFG_PRINT("<--DWC_ETH_QOS_powerdown\n");
 
-	return 0;
+   return ret;
 }
 
 /*!
@@ -5412,64 +5485,98 @@ int DWC_ETH_QOS_powerdown(struct net_dev
  * - Starts the queue.
  *
  * \param[in] dev – pointer to net device structure.
- * \param[in] caller – netif_attach gets called conditionally based
- *                     on caller, IOCTL or DRIVER-suspend
  *
  * \return int
  *
  * \retval zero on success and -ve number on failure.
  */
-int DWC_ETH_QOS_powerup(struct net_device *dev, uint32_t caller)
+int DWC_ETH_QOS_powerup(struct net_device *dev)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	unsigned long flags;
-
-	DBGPR("-->DWC_ETH_QOS_powerup\n");
-
-	if (!dev || !netif_running(dev) ||
-	    (caller == DWC_ETH_QOS_IOCTL_CONTEXT && !pdata->power_down)) {
-		printk(KERN_ALERT "Device is already powered up\n");
-		DBGPR(KERN_ALERT "<--DWC_ETH_QOS_powerup\n");
-		return -EINVAL;
-	}
-
-	spin_lock_irqsave(&pdata->pmt_lock, flags);
-
-	if (pdata->power_down_type & DWC_ETH_QOS_MAGIC_WAKEUP) {
-		hw_if->disable_magic_pmt();
-		pdata->power_down_type &= ~DWC_ETH_QOS_MAGIC_WAKEUP;
-	}
-
-	if (pdata->power_down_type & DWC_ETH_QOS_REMOTE_WAKEUP) {
-		hw_if->disable_remote_pmt();
-		pdata->power_down_type &= ~DWC_ETH_QOS_REMOTE_WAKEUP;
-	}
-
-	pdata->power_down = 0;
-
-	if (pdata->phydev)
-		phy_start(pdata->phydev);
+   struct DWC_ETH_QOS_prv_data *pdata = NULL;
+   hw_interface_t *hw_if = NULL;
+   unsigned long flags;
+   int ret = 0;
+
+   CFG_PRINT("-->DWC_ETH_QOS_powerup\n");
+
+   if (!dev) {
+      ERR_PRINT("Invalid parameter!\n");
+      ret = -EINVAL;
+      goto exit_pwr_up_err;
+   }
 
-	/* enable MAC TX/RX */
-	hw_if->start_mac_tx_rx();
+   pdata = netdev_priv(dev);
+   hw_if = &(pdata->hw_if);
 
-	/* enable DMA TX/RX */
-	DWC_ETH_QOS_start_all_ch_tx_dma(pdata);
-	DWC_ETH_QOS_start_all_ch_rx_dma(pdata);
+   spin_lock_irqsave(&pdata->pmt_lock, flags);
 
-	if (caller == DWC_ETH_QOS_DRIVER_CONTEXT)
-		netif_device_attach(dev);
+   if(!pdata->power_state) {
+      WRN_PRINT("Device has not been powered down!\n");
+      goto exit_pwr_up;
+   } else if (!netif_running(dev)) {
+      /* Process request if it was started by the NetIP SS driver. */
+      pdata->power_state &= ~DWC_ETH_QOS_NETIP_WAKEUP;
+      WRN_PRINT("Device is not running!\n");
+      goto exit_pwr_up;
+   }
 
-	DWC_ETH_QOS_napi_enable(pdata);
+   if (pdata->power_state & DWC_ETH_QOS_NETIP_WAKEUP) {
+      gbe_exit_standby(pdata);
+      pdata->power_state &= ~DWC_ETH_QOS_NETIP_WAKEUP;
+   }
+   if (pdata->power_state & DWC_ETH_QOS_MAGIC_WAKEUP) {
+      hw_if->disable_magic_pmt();
+      pdata->power_state &= ~DWC_ETH_QOS_MAGIC_WAKEUP;
+   }
+   if (pdata->power_state & DWC_ETH_QOS_REMOTE_WAKEUP) {
+      hw_if->disable_remote_pmt();
+      pdata->power_state &= ~DWC_ETH_QOS_REMOTE_WAKEUP;
+   }
 
-	netif_tx_start_all_queues(dev);
+   /* Configure pending requests (e.g. Split header or MTU changes) */
+   if (pdata->power_state &
+      (DWC_ETH_QOS_NETIP_SPLHDR_REQ | DWC_ETH_QOS_NETIP_MTU_REQ)) {
+      /* Indicate controller is powering up from StandBy */
+      pdata->power_state |= DWC_ETH_QOS_NETIP_PWRUP;
+      DWC_ETH_QOS_stop_dev(pdata);
+      if (pdata->power_state & DWC_ETH_QOS_NETIP_SPLHDR_REQ) {
+         uint32_t qInx;
+         hw_if->config_header_size(DWC_ETH_QOS_MAX_HDR_SIZE);
+         /* Enable/disable split header for all RX DMA channel */
+         for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++)
+            hw_if->config_split_header_mode(qInx, pdata->rx_split_hdr);
+         }
+      /* Attach net device */
+      netif_device_attach(dev);
+      DWC_ETH_QOS_start_dev(pdata);
+   } else {
+      /* Start PHY */
+      if (pdata->phydev)
+         phy_start(pdata->phydev);
+      /* Enable MAC TX/RX */
+      hw_if->start_mac_tx_rx();
+      /* Enable DMA TX/RX */
+      DWC_ETH_QOS_start_all_ch_tx_dma(pdata);
+      DWC_ETH_QOS_start_all_ch_rx_dma(pdata);
+      /* Attach net device */
+      netif_device_attach(dev);
+      /* Enable NAPI */
+      DWC_ETH_QOS_napi_enable(pdata);
+      /* Start Tx queues */
+      netif_tx_start_all_queues(dev);
+      /* Disable PMT Interrupt */
+      VAR32_SET_BIT(pdata->hw_cfg.mac_ier, MAC_IER_PMTIE, 0x0);
+      DWC_REG_WR(MAC_IER, pdata->hw_cfg.mac_ier);
+   }
+   pdata->power_state = DWC_ETH_QOS_POWER_ON;
 
-	spin_unlock_irqrestore(&pdata->pmt_lock, flags);
+exit_pwr_up:
+   spin_unlock_irqrestore(&pdata->pmt_lock, flags);
 
-	DBGPR("<--DWC_ETH_QOS_powerup\n");
+exit_pwr_up_err:
+   CFG_PRINT("<--DWC_ETH_QOS_powerup\n");
 
-	return 0;
+   return ret;
 }
 
 /*!
@@ -5486,26 +5593,22 @@ int DWC_ETH_QOS_powerup(struct net_devic
  * \retval zero on success and -ve number on failure.
  */
 int DWC_ETH_QOS_configure_remotewakeup(struct net_device *dev,
-				       struct ifr_data_struct *req)
+                   struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+   struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
+   hw_interface_t *hw_if = &(pdata->hw_if);
 
-	if (!dev || !netif_running(dev) || !pdata->hw_feat.rwk_sel
-	    || pdata->power_down) {
-		printk(KERN_ALERT
-		       "Device is already powered down and will powerup for %s\n",
-		       DWC_ETH_QOS_POWER_DOWN_TYPE(pdata));
-		return -EINVAL;
-	}
+   if (!dev || !netif_running(dev) ||
+       !pdata->hw_feat.rwk_sel || pdata->power_state) {
+      ERR_PRINT("Device is already powered down!\n");
+      return -EINVAL;
+   }
 
-	hw_if->configure_rwk_filter(req->rwk_filter_values,
-				    req->rwk_filter_length);
+   hw_if->configure_rwk_filter(req->rwk_filter_values, req->rwk_filter_length);
 
-	DWC_ETH_QOS_powerdown(dev, DWC_ETH_QOS_REMOTE_WAKEUP,
-			DWC_ETH_QOS_IOCTL_CONTEXT);
+   DWC_ETH_QOS_powerdown(dev, DWC_ETH_QOS_REMOTE_WAKEUP);
 
-	return 0;
+   return 0;
 }
 
 /*!
@@ -5520,51 +5623,50 @@ int DWC_ETH_QOS_configure_remotewakeup(s
  *
  * \retval none
  */
-
 static void DWC_ETH_QOS_config_rx_pbl(struct DWC_ETH_QOS_prv_data *pdata,
-				      uint32_t rx_pbl,
-				      uint32_t qInx)
+                  uint32_t rx_pbl,
+                  uint32_t qInx)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t pblx8_val = 0;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t pblx8_val = 0;
+
+   DBGPR("-->DWC_ETH_QOS_config_rx_pbl: %d\n", rx_pbl);
 
-	DBGPR("-->DWC_ETH_QOS_config_rx_pbl: %d\n", rx_pbl);
+   switch (rx_pbl) {
+   case DWC_ETH_QOS_PBL_1:
+   case DWC_ETH_QOS_PBL_2:
+   case DWC_ETH_QOS_PBL_4:
+   case DWC_ETH_QOS_PBL_8:
+   case DWC_ETH_QOS_PBL_16:
+   case DWC_ETH_QOS_PBL_32:
+      hw_if->config_rx_pbl_val(qInx, rx_pbl);
+      hw_if->config_pblx8(qInx, 0);
+      break;
+   case DWC_ETH_QOS_PBL_64:
+   case DWC_ETH_QOS_PBL_128:
+   case DWC_ETH_QOS_PBL_256:
+      hw_if->config_rx_pbl_val(qInx, rx_pbl / 8);
+      hw_if->config_pblx8(qInx, 1);
+      pblx8_val = 1;
+      break;
+   }
 
-	switch (rx_pbl) {
-	case DWC_ETH_QOS_PBL_1:
-	case DWC_ETH_QOS_PBL_2:
-	case DWC_ETH_QOS_PBL_4:
-	case DWC_ETH_QOS_PBL_8:
-	case DWC_ETH_QOS_PBL_16:
-	case DWC_ETH_QOS_PBL_32:
-		hw_if->config_rx_pbl_val(qInx, rx_pbl);
-		hw_if->config_pblx8(qInx, 0);
-		break;
-	case DWC_ETH_QOS_PBL_64:
-	case DWC_ETH_QOS_PBL_128:
-	case DWC_ETH_QOS_PBL_256:
-		hw_if->config_rx_pbl_val(qInx, rx_pbl / 8);
-		hw_if->config_pblx8(qInx, 1);
-		pblx8_val = 1;
-		break;
-	}
-
-	switch (pblx8_val) {
-		case 0:
-			printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
-					qInx, hw_if->get_tx_pbl_val(qInx));
-			printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
-					qInx, hw_if->get_rx_pbl_val(qInx));
-			break;
-		case 1:
-			printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
-					qInx, (hw_if->get_tx_pbl_val(qInx) * 8));
-			printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
-					qInx, (hw_if->get_rx_pbl_val(qInx) * 8));
-			break;
-	}
+   switch (pblx8_val) {
+      case 0:
+         printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
+               qInx, hw_if->get_tx_pbl_val(qInx));
+         printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
+               qInx, hw_if->get_rx_pbl_val(qInx));
+         break;
+      case 1:
+         printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
+               qInx, (hw_if->get_tx_pbl_val(qInx) * 8));
+         printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
+               qInx, (hw_if->get_rx_pbl_val(qInx) * 8));
+         break;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_config_rx_pbl\n");
+   DBGPR("<--DWC_ETH_QOS_config_rx_pbl\n");
 }
 
 /*!
@@ -5579,53 +5681,51 @@ static void DWC_ETH_QOS_config_rx_pbl(st
  *
  * \retval none
  */
-
 static void DWC_ETH_QOS_config_tx_pbl(struct DWC_ETH_QOS_prv_data *pdata,
-				      uint32_t tx_pbl,
-				      uint32_t qInx)
+                  uint32_t tx_pbl,
+                  uint32_t qInx)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
-	uint32_t pblx8_val = 0;
+   hw_interface_t *hw_if = &(pdata->hw_if);
+   uint32_t pblx8_val = 0;
 
-	DBGPR("-->DWC_ETH_QOS_config_tx_pbl: %d\n", tx_pbl);
+   DBGPR("-->DWC_ETH_QOS_config_tx_pbl: %d\n", tx_pbl);
 
-	switch (tx_pbl) {
-	case DWC_ETH_QOS_PBL_1:
-	case DWC_ETH_QOS_PBL_2:
-	case DWC_ETH_QOS_PBL_4:
-	case DWC_ETH_QOS_PBL_8:
-	case DWC_ETH_QOS_PBL_16:
-	case DWC_ETH_QOS_PBL_32:
-		hw_if->config_tx_pbl_val(qInx, tx_pbl);
-		hw_if->config_pblx8(qInx, 0);
-		break;
-	case DWC_ETH_QOS_PBL_64:
-	case DWC_ETH_QOS_PBL_128:
-	case DWC_ETH_QOS_PBL_256:
-		hw_if->config_tx_pbl_val(qInx, tx_pbl / 8);
-		hw_if->config_pblx8(qInx, 1);
-		pblx8_val = 1;
-		break;
-	}
-
-	switch (pblx8_val) {
-		case 0:
-			printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
-					qInx, hw_if->get_tx_pbl_val(qInx));
-			printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
-					qInx, hw_if->get_rx_pbl_val(qInx));
-			break;
-		case 1:
-			printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
-					qInx, (hw_if->get_tx_pbl_val(qInx) * 8));
-			printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
-					qInx, (hw_if->get_rx_pbl_val(qInx) * 8));
-			break;
-	}
+   switch (tx_pbl) {
+   case DWC_ETH_QOS_PBL_1:
+   case DWC_ETH_QOS_PBL_2:
+   case DWC_ETH_QOS_PBL_4:
+   case DWC_ETH_QOS_PBL_8:
+   case DWC_ETH_QOS_PBL_16:
+   case DWC_ETH_QOS_PBL_32:
+      hw_if->config_tx_pbl_val(qInx, tx_pbl);
+      hw_if->config_pblx8(qInx, 0);
+      break;
+   case DWC_ETH_QOS_PBL_64:
+   case DWC_ETH_QOS_PBL_128:
+   case DWC_ETH_QOS_PBL_256:
+      hw_if->config_tx_pbl_val(qInx, tx_pbl / 8);
+      hw_if->config_pblx8(qInx, 1);
+      pblx8_val = 1;
+      break;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_config_tx_pbl\n");
-}
+   switch (pblx8_val) {
+      case 0:
+         printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
+               qInx, hw_if->get_tx_pbl_val(qInx));
+         printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
+               qInx, hw_if->get_rx_pbl_val(qInx));
+         break;
+      case 1:
+         printk(KERN_ALERT "Tx PBL[%d] value: %d\n",
+               qInx, (hw_if->get_tx_pbl_val(qInx) * 8));
+         printk(KERN_ALERT "Rx PBL[%d] value: %d\n",
+               qInx, (hw_if->get_rx_pbl_val(qInx) * 8));
+         break;
+   }
 
+   DBGPR("<--DWC_ETH_QOS_config_tx_pbl\n");
+}
 
 /*!
  * \details This function is invoked by ioctl function when the user issues an
@@ -5638,31 +5738,29 @@ static void DWC_ETH_QOS_config_tx_pbl(st
  *
  * \retval none
  */
-
 static void DWC_ETH_QOS_program_dcb_algorithm(struct DWC_ETH_QOS_prv_data *pdata,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_dcb_algorithm l_dcb_struct, *u_dcb_struct =
-		(struct DWC_ETH_QOS_dcb_algorithm *)req->ptr;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
+   struct DWC_ETH_QOS_dcb_algorithm l_dcb_struct, *u_dcb_struct =
+      (struct DWC_ETH_QOS_dcb_algorithm *)req->ptr;
+   hw_interface_t *hw_if = &pdata->hw_if;
 
-	DBGPR("-->DWC_ETH_QOS_program_dcb_algorithm\n");
+   DBGPR("-->DWC_ETH_QOS_program_dcb_algorithm\n");
 
-	if(copy_from_user(&l_dcb_struct, u_dcb_struct,
-				sizeof(struct DWC_ETH_QOS_dcb_algorithm)))
-		printk(KERN_ALERT "Failed to fetch DCB Struct info from user\n");
+   if(copy_from_user(&l_dcb_struct, u_dcb_struct,
+            sizeof(struct DWC_ETH_QOS_dcb_algorithm)))
+      printk(KERN_ALERT "Failed to fetch DCB Struct info from user\n");
 
-	hw_if->set_tx_queue_operating_mode(l_dcb_struct.qInx,
-		(uint32_t)l_dcb_struct.op_mode);
-	hw_if->set_dcb_algorithm(l_dcb_struct.algorithm);
-	hw_if->set_dcb_queue_weight(l_dcb_struct.qInx, l_dcb_struct.weight);
+   hw_if->set_tx_queue_operating_mode(l_dcb_struct.qInx,
+      (uint32_t)l_dcb_struct.op_mode);
+   hw_if->set_dcb_algorithm(l_dcb_struct.algorithm);
+   hw_if->set_dcb_queue_weight(l_dcb_struct.qInx, l_dcb_struct.weight);
 
-	DBGPR("<--DWC_ETH_QOS_program_dcb_algorithm\n");
+   DBGPR("<--DWC_ETH_QOS_program_dcb_algorithm\n");
 
-	return;
+   return;
 }
 
-
 /*!
  * \details This function is invoked by ioctl function when the user issues an
  * ioctl command to select the AVB algorithm. This function also configures other
@@ -5675,32 +5773,31 @@ static void DWC_ETH_QOS_program_dcb_algo
  *
  * \retval none
  */
-
 static void DWC_ETH_QOS_program_avb_algorithm(struct DWC_ETH_QOS_prv_data *pdata,
-		struct ifr_data_struct *req)
+      struct ifr_data_struct *req)
 {
-	struct DWC_ETH_QOS_avb_algorithm l_avb_struct, *u_avb_struct =
-		(struct DWC_ETH_QOS_avb_algorithm *)req->ptr;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
-
-	DBGPR("-->DWC_ETH_QOS_program_avb_algorithm\n");
-
-	if(copy_from_user(&l_avb_struct, u_avb_struct,
-				sizeof(struct DWC_ETH_QOS_avb_algorithm)))
-		printk(KERN_ALERT "Failed to fetch AVB Struct info from user\n");
-
-	hw_if->set_tx_queue_operating_mode(l_avb_struct.qInx,
-		(uint32_t)l_avb_struct.op_mode);
-	hw_if->set_avb_algorithm(l_avb_struct.qInx, l_avb_struct.algorithm);
-	hw_if->config_credit_control(l_avb_struct.qInx, l_avb_struct.cc);
-	hw_if->config_send_slope(l_avb_struct.qInx, l_avb_struct.send_slope);
-	hw_if->config_idle_slope(l_avb_struct.qInx, l_avb_struct.idle_slope);
-	hw_if->config_high_credit(l_avb_struct.qInx, l_avb_struct.hi_credit);
-	hw_if->config_low_credit(l_avb_struct.qInx, l_avb_struct.low_credit);
+   struct DWC_ETH_QOS_avb_algorithm l_avb_struct, *u_avb_struct =
+      (struct DWC_ETH_QOS_avb_algorithm *)req->ptr;
+   hw_interface_t *hw_if = &pdata->hw_if;
+
+   DBGPR("-->DWC_ETH_QOS_program_avb_algorithm\n");
+
+   if(copy_from_user(&l_avb_struct, u_avb_struct,
+            sizeof(struct DWC_ETH_QOS_avb_algorithm)))
+      printk(KERN_ALERT "Failed to fetch AVB Struct info from user\n");
+
+   hw_if->set_tx_queue_operating_mode(l_avb_struct.qInx,
+      (uint32_t)l_avb_struct.op_mode);
+   hw_if->set_avb_algorithm(l_avb_struct.qInx, l_avb_struct.algorithm);
+   hw_if->config_credit_control(l_avb_struct.qInx, l_avb_struct.cc);
+   hw_if->config_send_slope(l_avb_struct.qInx, l_avb_struct.send_slope);
+   hw_if->config_idle_slope(l_avb_struct.qInx, l_avb_struct.idle_slope);
+   hw_if->config_high_credit(l_avb_struct.qInx, l_avb_struct.hi_credit);
+   hw_if->config_low_credit(l_avb_struct.qInx, l_avb_struct.low_credit);
 
-	DBGPR("<--DWC_ETH_QOS_program_avb_algorithm\n");
+   DBGPR("<--DWC_ETH_QOS_program_avb_algorithm\n");
 
-	return;
+   return;
 }
 
 /*!
@@ -5713,13 +5810,12 @@ static void DWC_ETH_QOS_program_avb_algo
  *
  * \return void
  */
-
 void dump_rx_desc(uint32_t qInx, rx_descriptor_t *desc, int desc_idx)
 {
-	printk(KERN_ALERT "\nRX_NORMAL_DESC[%02d %4p %03d RECEIVED FROM DEVICE]"\
-		" = %#x:%#x:%#x:%#x",
-		qInx, desc, desc_idx, desc->RDES0, desc->RDES1,
-		desc->RDES2, desc->RDES3);
+   printk(KERN_ALERT "\nRX_NORMAL_DESC[%02d %4p %03d RECEIVED FROM DEVICE]"\
+      " = %#x:%#x:%#x:%#x",
+      qInx, desc, desc_idx, desc->RDES0, desc->RDES1,
+      desc->RDES2, desc->RDES3);
 }
 
 /*!
@@ -5733,21 +5829,21 @@ void dump_rx_desc(uint32_t qInx, rx_desc
  */
 void DWC_ETH_QOS_init_rx_coalesce(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data = NULL;
-	uint32_t i;
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data = NULL;
+   uint32_t i;
 
-	DBGPR("-->DWC_ETH_QOS_init_rx_coalesce\n");
+   DBGPR("-->DWC_ETH_QOS_init_rx_coalesce\n");
 
-	for (i = 0; i < DWC_ETH_QOS_RX_QUEUE_CNT; i++) {
-		rx_desc_data = GET_RX_WRAPPER_DESC(i);
+   for (i = 0; i < DWC_ETH_QOS_RX_QUEUE_CNT; i++) {
+      rx_desc_data = GET_RX_WRAPPER_DESC(i);
 
-		rx_desc_data->use_riwt = 1;
-		rx_desc_data->rx_coal_frames = DWC_ETH_QOS_RX_MAX_FRAMES;
-		rx_desc_data->rx_riwt =
-			DWC_ETH_QOS_usec2riwt(DWC_ETH_QOS_OPTIMAL_DMA_RIWT_USEC, pdata);
-	}
+      rx_desc_data->use_riwt = 1;
+      rx_desc_data->rx_coal_frames = DWC_ETH_QOS_RX_MAX_FRAMES;
+      rx_desc_data->rx_riwt =
+         DWC_ETH_QOS_usec2riwt(DWC_ETH_QOS_OPTIMAL_DMA_RIWT_USEC, pdata);
+   }
 
-	DBGPR("<--DWC_ETH_QOS_init_rx_coalesce\n");
+   DBGPR("<--DWC_ETH_QOS_init_rx_coalesce\n");
 }
 
 /*!
@@ -5760,14 +5856,14 @@ void DWC_ETH_QOS_init_rx_coalesce(struct
  */
 static void DWC_ETH_QOS_mmc_setup(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	DBGPR("-->DWC_ETH_QOS_mmc_setup\n");
+   DBGPR("-->DWC_ETH_QOS_mmc_setup\n");
 
-	if (pdata->hw_feat.mmc_sel) {
-		memset(&pdata->mmc, 0, sizeof(struct DWC_ETH_QOS_mmc_counters));
-	} else
-		printk(KERN_ALERT "No MMC/RMON module available in the HW\n");
+   if (pdata->hw_feat.mmc_sel) {
+      memset(&pdata->mmc, 0, sizeof(struct DWC_ETH_QOS_mmc_counters));
+   } else
+      printk(KERN_ALERT "No MMC/RMON module available in the HW\n");
 
-	DBGPR("<--DWC_ETH_QOS_mmc_setup\n");
+   DBGPR("<--DWC_ETH_QOS_mmc_setup\n");
 }
 
 /*!
@@ -5787,144 +5883,166 @@ static void DWC_ETH_QOS_mmc_setup(struct
  */
 void DWC_ETH_QOS_mmc_read(struct DWC_ETH_QOS_mmc_counters *mmc)
 {
-	DBGPR("-->DWC_ETH_QOS_mmc_read\n");
+   DBGPR("-->DWC_ETH_QOS_mmc_read\n");
 
-	/* MMC TX counter registers */
-	mmc->mmc_tx_octetcount_gb += DWC_REG_RD(MMC_TX_OCTETS);
-	mmc->mmc_tx_framecount_gb += DWC_REG_RD(MMC_TX_PKTS);
-	mmc->mmc_tx_broadcastframe_g += DWC_REG_RD(MMC_TX_BROADCAST_GOOD);
-	mmc->mmc_tx_multicastframe_g += DWC_REG_RD(MMC_TX_MULTICAST_GOOD);
-	mmc->mmc_tx_64_octets_gb += DWC_REG_RD(MMC_TX_64_OCTETS);
-	mmc->mmc_tx_65_to_127_octets_gb += DWC_REG_RD(MMC_TX_65TO127_OCTETS);
-	mmc->mmc_tx_128_to_255_octets_gb += DWC_REG_RD(MMC_TX_128TO255_OCTETS);
-	mmc->mmc_tx_256_to_511_octets_gb += DWC_REG_RD(MMC_TX_256TO511_OCTETS);
-	mmc->mmc_tx_512_to_1023_octets_gb += DWC_REG_RD(MMC_TX_512TO1023_OCTETS);
-	mmc->mmc_tx_1024_to_max_octets_gb += DWC_REG_RD(MMC_TX_1024TOMAX_OCTETS);
-	mmc->mmc_tx_unicast_gb += DWC_REG_RD(MMC_TX_UNICAST);
-	mmc->mmc_tx_multicast_gb += DWC_REG_RD(MMC_TX_MULTICAST);
-	mmc->mmc_tx_broadcast_gb += DWC_REG_RD(MMC_TX_BROADCAST);
-	mmc->mmc_tx_underflow_error += DWC_REG_RD(MMC_TX_UNDERFLOW_ERROR);
-	mmc->mmc_tx_singlecol_g += DWC_REG_RD(MMC_TX_SINGLE_COLLISION);
-	mmc->mmc_tx_multicol_g += DWC_REG_RD(MMC_TX_MULTI_COLLISION);
-	mmc->mmc_tx_deferred += DWC_REG_RD(MMC_TX_DEFERRED);
-	mmc->mmc_tx_latecol += DWC_REG_RD(MMC_TX_LATE_COLLISION);
-	mmc->mmc_tx_exesscol += DWC_REG_RD(MMC_TX_EXCESS_COLLISION);
-	mmc->mmc_tx_carrier_error += DWC_REG_RD(MMC_TX_CARRIER_ERROR);
-	mmc->mmc_tx_octetcount_g += DWC_REG_RD(MMC_TX_OCTETS_GOOD);
-	mmc->mmc_tx_framecount_g += DWC_REG_RD(MMC_TX_PKTS_GOOD);
-	mmc->mmc_tx_excessdef += DWC_REG_RD(MMC_TX_EXCESS_DEFERRAL_ERR);
-	mmc->mmc_tx_pause_frame += DWC_REG_RD(MMC_TX_PAUSE_PKTS);
-	mmc->mmc_tx_vlan_frame_g += DWC_REG_RD(MMC_TX_VLAN_PKTS);
-	mmc->mmc_tx_osize_frame_g += DWC_REG_RD(MMC_TX_OVERSIZE_PKTS);
-
-	/* MMC RX counter registers */
-	mmc->mmc_rx_framecount_gb += DWC_REG_RD(MMC_RX_PKTS);
-	mmc->mmc_rx_octetcount_gb += DWC_REG_RD(MMC_RX_OCTETS);
-	mmc->mmc_rx_octetcount_g += DWC_REG_RD(MMC_RX_OCTET_GOOD);
-	mmc->mmc_rx_broadcastframe_g += DWC_REG_RD(MMC_RX_BROADCAST_GOOD);
-	mmc->mmc_rx_multicastframe_g += DWC_REG_RD(MMC_RX_MULTICAST_GOOD);
-	mmc->mmc_rx_crc_errror += DWC_REG_RD(MMC_RX_CRC_ERROR);
-	mmc->mmc_rx_align_error += DWC_REG_RD(MMC_RX_ALIGN_ERROR);
-	mmc->mmc_rx_run_error += DWC_REG_RD(MMC_RX_RUNT_ERROR);
-	mmc->mmc_rx_jabber_error += DWC_REG_RD(MMC_RX_JABBER_ERROR);
-	mmc->mmc_rx_undersize_g += DWC_REG_RD(MMC_RX_UNDERSIZE_PKTS);
-	mmc->mmc_rx_oversize_g += DWC_REG_RD(MMC_RX_OVERSIZE_PKTS);
-	mmc->mmc_rx_64_octets_gb += DWC_REG_RD(MMC_RX_64_OCTETS);
-	mmc->mmc_rx_65_to_127_octets_gb += DWC_REG_RD(MMC_RX_65TO127_OCTETS);
-	mmc->mmc_rx_128_to_255_octets_gb += DWC_REG_RD(MMC_RX_128TO255_OCTETS);
-	mmc->mmc_rx_256_to_511_octets_gb += DWC_REG_RD(MMC_RX_256TO511_OCTETS);
-	mmc->mmc_rx_512_to_1023_octets_gb += DWC_REG_RD(MMC_RX_512TO1023_OCTETS);
-	mmc->mmc_rx_1024_to_max_octets_gb += DWC_REG_RD(MMC_RX_1024TOMAX_OCTETS);
-	mmc->mmc_rx_unicast_g += DWC_REG_RD(MMC_RX_UNICAST_GOOD);
-	mmc->mmc_rx_length_error += DWC_REG_RD(MMC_RX_LENGTH_ERROR);
-	mmc->mmc_rx_outofrangetype += DWC_REG_RD(MMC_RX_OUT_RANGE_TYPE);
-	mmc->mmc_rx_pause_frames += DWC_REG_RD(MMC_RX_PAUSE_PKTS);
-	mmc->mmc_rx_fifo_overflow += DWC_REG_RD(MMC_RX_FIFO_OVERFLOW);
-	mmc->mmc_rx_vlan_frames_gb += DWC_REG_RD(MMC_RX_VLAN_PKTS);
-	mmc->mmc_rx_watchdog_error += DWC_REG_RD(MMC_RX_WATCHDOG_ERROR);
-	mmc->mmc_rx_receive_error += DWC_REG_RD(MMC_RX_RECEIVE_ERROR);
-	mmc->mmc_rx_ctrl_frames_g += DWC_REG_RD(MMC_RX_CONTROL_PKTS);
-
-	/* IPC */
-	mmc->mmc_rx_ipc_intr_mask = DWC_REG_RD(MMC_IPC_RX_IMR);
-	mmc->mmc_rx_ipc_intr = DWC_REG_RD(MMC_IPC_RX_IR);
-
-	/* IPv4 */
-	mmc->mmc_rx_ipv4_gd += DWC_REG_RD(MMC_RX_IPV4_GOOD_PKTS);
-	mmc->mmc_rx_ipv4_hderr += DWC_REG_RD(MMC_RX_IPV4_HEADER_ERROR_PKTS);
-	mmc->mmc_rx_ipv4_nopay += DWC_REG_RD(MMC_RX_IPV4_NO_PAYLOAD_PKTS);
-	mmc->mmc_rx_ipv4_frag += DWC_REG_RD(MMC_RX_IPV4_FRAGMENTED_PKTS);
-	mmc->mmc_rx_ipv4_udsbl += DWC_REG_RD(MMC_RX_IPV4_UDP_CSUM_DIS_PKTS);
-
-	/* IPV6 */
-	mmc->mmc_rx_ipv6_gd += DWC_REG_RD(MMC_RX_IPV6_GOOD_PKTS);
-	mmc->mmc_rx_ipv6_hderr += DWC_REG_RD(MMC_RX_IPV6_HEADER_ERROR_PKTS);
-	mmc->mmc_rx_ipv6_nopay += DWC_REG_RD(MMC_RX_IPV6_NO_PAYLOAD_PKTS);
-
-	/* Protocols */
-	mmc->mmc_rx_udp_gd += DWC_REG_RD(MMC_RX_UDP_GOOD_PKTS);
-	mmc->mmc_rx_udp_err += DWC_REG_RD(MMC_RX_UDP_ERROR_PKTS);
-	mmc->mmc_rx_tcp_gd += DWC_REG_RD(MMC_RX_TCP_GOOD_PKTS);
-	mmc->mmc_rx_tcp_err += DWC_REG_RD(MMC_RX_TCP_ERROR_PKTS);
-	mmc->mmc_rx_icmp_gd += DWC_REG_RD(MMC_RX_ICMP_GOOD_PKTS);
-	mmc->mmc_rx_icmp_err += DWC_REG_RD(MMC_RX_ICMP_ERROR_PKTS);
-
-	/* IPv4 */
-	mmc->mmc_rx_ipv4_gd_octets += DWC_REG_RD(MMC_RX_IPV4_GOOD_OCTETS);
-	mmc->mmc_rx_ipv4_hderr_octets += DWC_REG_RD(MMC_RX_IPV4_ERROR_OCTETS);
-	mmc->mmc_rx_ipv4_nopay_octets += DWC_REG_RD(MMC_RX_IPV4_NO_PAYLOAD_OCTETS);
-	mmc->mmc_rx_ipv4_frag_octets += DWC_REG_RD(MMC_RX_IPV4_FRAGMENTED_OCTETS);
-	mmc->mmc_rx_ipv4_udsbl_octets += DWC_REG_RD(MMC_RX_IPV4_UDP_CSUM_DIS_OCTETS);
-
-	/* IPV6 */
-	mmc->mmc_rx_ipv6_gd_octets += DWC_REG_RD(MMC_RX_IPV6_GOOD_OCTETS);
-	mmc->mmc_rx_ipv6_hderr_octets += DWC_REG_RD(MMC_RX_IPV6_HEADER_ERROR_OCTETS);
-	mmc->mmc_rx_ipv6_nopay_octets += DWC_REG_RD(MMC_RX_IPV6_NO_PAYLOAD_OCTETS);
-
-	/* Protocols */
-	mmc->mmc_rx_udp_gd_octets += DWC_REG_RD(MMC_RX_UDP_GOOD_OCTETS);
-	mmc->mmc_rx_udp_err_octets += DWC_REG_RD(MMC_RX_UDP_ERROR_OCTETS);
-	mmc->mmc_rx_tcp_gd_octets += DWC_REG_RD(MMC_RX_TCP_GOOD_OCTETS);
-	mmc->mmc_rx_tcp_err_octets += DWC_REG_RD(MMC_RX_TCP_ERROR_OCTETS);
-	mmc->mmc_rx_icmp_gd_octets += DWC_REG_RD(MMC_RX_ICMP_GOOD_OCTETS);
-	mmc->mmc_rx_icmp_err_octets += DWC_REG_RD(MMC_RX_ICMP_ERROR_OCTETS);
+   /* MMC TX counter registers */
+   mmc->mmc_tx_octetcount_gb += DWC_REG_RD(MMC_TX_OCTETS);
+   mmc->mmc_tx_framecount_gb += DWC_REG_RD(MMC_TX_PKTS);
+   mmc->mmc_tx_broadcastframe_g += DWC_REG_RD(MMC_TX_BROADCAST_GOOD);
+   mmc->mmc_tx_multicastframe_g += DWC_REG_RD(MMC_TX_MULTICAST_GOOD);
+   mmc->mmc_tx_64_octets_gb += DWC_REG_RD(MMC_TX_64_OCTETS);
+   mmc->mmc_tx_65_to_127_octets_gb += DWC_REG_RD(MMC_TX_65TO127_OCTETS);
+   mmc->mmc_tx_128_to_255_octets_gb += DWC_REG_RD(MMC_TX_128TO255_OCTETS);
+   mmc->mmc_tx_256_to_511_octets_gb += DWC_REG_RD(MMC_TX_256TO511_OCTETS);
+   mmc->mmc_tx_512_to_1023_octets_gb += DWC_REG_RD(MMC_TX_512TO1023_OCTETS);
+   mmc->mmc_tx_1024_to_max_octets_gb += DWC_REG_RD(MMC_TX_1024TOMAX_OCTETS);
+   mmc->mmc_tx_unicast_gb += DWC_REG_RD(MMC_TX_UNICAST);
+   mmc->mmc_tx_multicast_gb += DWC_REG_RD(MMC_TX_MULTICAST);
+   mmc->mmc_tx_broadcast_gb += DWC_REG_RD(MMC_TX_BROADCAST);
+   mmc->mmc_tx_underflow_error += DWC_REG_RD(MMC_TX_UNDERFLOW_ERROR);
+   mmc->mmc_tx_singlecol_g += DWC_REG_RD(MMC_TX_SINGLE_COLLISION);
+   mmc->mmc_tx_multicol_g += DWC_REG_RD(MMC_TX_MULTI_COLLISION);
+   mmc->mmc_tx_deferred += DWC_REG_RD(MMC_TX_DEFERRED);
+   mmc->mmc_tx_latecol += DWC_REG_RD(MMC_TX_LATE_COLLISION);
+   mmc->mmc_tx_exesscol += DWC_REG_RD(MMC_TX_EXCESS_COLLISION);
+   mmc->mmc_tx_carrier_error += DWC_REG_RD(MMC_TX_CARRIER_ERROR);
+   mmc->mmc_tx_octetcount_g += DWC_REG_RD(MMC_TX_OCTETS_GOOD);
+   mmc->mmc_tx_framecount_g += DWC_REG_RD(MMC_TX_PKTS_GOOD);
+   mmc->mmc_tx_excessdef += DWC_REG_RD(MMC_TX_EXCESS_DEFERRAL_ERR);
+   mmc->mmc_tx_pause_frame += DWC_REG_RD(MMC_TX_PAUSE_PKTS);
+   mmc->mmc_tx_vlan_frame_g += DWC_REG_RD(MMC_TX_VLAN_PKTS);
+   mmc->mmc_tx_osize_frame_g += DWC_REG_RD(MMC_TX_OVERSIZE_PKTS);
+
+   /* MMC RX counter registers */
+   mmc->mmc_rx_framecount_gb += DWC_REG_RD(MMC_RX_PKTS);
+   mmc->mmc_rx_octetcount_gb += DWC_REG_RD(MMC_RX_OCTETS);
+   mmc->mmc_rx_octetcount_g += DWC_REG_RD(MMC_RX_OCTET_GOOD);
+   mmc->mmc_rx_broadcastframe_g += DWC_REG_RD(MMC_RX_BROADCAST_GOOD);
+   mmc->mmc_rx_multicastframe_g += DWC_REG_RD(MMC_RX_MULTICAST_GOOD);
+   mmc->mmc_rx_crc_errror += DWC_REG_RD(MMC_RX_CRC_ERROR);
+   mmc->mmc_rx_align_error += DWC_REG_RD(MMC_RX_ALIGN_ERROR);
+   mmc->mmc_rx_run_error += DWC_REG_RD(MMC_RX_RUNT_ERROR);
+   mmc->mmc_rx_jabber_error += DWC_REG_RD(MMC_RX_JABBER_ERROR);
+   mmc->mmc_rx_undersize_g += DWC_REG_RD(MMC_RX_UNDERSIZE_PKTS);
+   mmc->mmc_rx_oversize_g += DWC_REG_RD(MMC_RX_OVERSIZE_PKTS);
+   mmc->mmc_rx_64_octets_gb += DWC_REG_RD(MMC_RX_64_OCTETS);
+   mmc->mmc_rx_65_to_127_octets_gb += DWC_REG_RD(MMC_RX_65TO127_OCTETS);
+   mmc->mmc_rx_128_to_255_octets_gb += DWC_REG_RD(MMC_RX_128TO255_OCTETS);
+   mmc->mmc_rx_256_to_511_octets_gb += DWC_REG_RD(MMC_RX_256TO511_OCTETS);
+   mmc->mmc_rx_512_to_1023_octets_gb += DWC_REG_RD(MMC_RX_512TO1023_OCTETS);
+   mmc->mmc_rx_1024_to_max_octets_gb += DWC_REG_RD(MMC_RX_1024TOMAX_OCTETS);
+   mmc->mmc_rx_unicast_g += DWC_REG_RD(MMC_RX_UNICAST_GOOD);
+   mmc->mmc_rx_length_error += DWC_REG_RD(MMC_RX_LENGTH_ERROR);
+   mmc->mmc_rx_outofrangetype += DWC_REG_RD(MMC_RX_OUT_RANGE_TYPE);
+   mmc->mmc_rx_pause_frames += DWC_REG_RD(MMC_RX_PAUSE_PKTS);
+   mmc->mmc_rx_fifo_overflow += DWC_REG_RD(MMC_RX_FIFO_OVERFLOW);
+   mmc->mmc_rx_vlan_frames_gb += DWC_REG_RD(MMC_RX_VLAN_PKTS);
+   mmc->mmc_rx_watchdog_error += DWC_REG_RD(MMC_RX_WATCHDOG_ERROR);
+   mmc->mmc_rx_receive_error += DWC_REG_RD(MMC_RX_RECEIVE_ERROR);
+   mmc->mmc_rx_ctrl_frames_g += DWC_REG_RD(MMC_RX_CONTROL_PKTS);
+
+   /* IPC */
+   mmc->mmc_rx_ipc_intr_mask = DWC_REG_RD(MMC_IPC_RX_IMR);
+   mmc->mmc_rx_ipc_intr = DWC_REG_RD(MMC_IPC_RX_IR);
+
+   /* IPv4 */
+   mmc->mmc_rx_ipv4_gd += DWC_REG_RD(MMC_RX_IPV4_GOOD_PKTS);
+   mmc->mmc_rx_ipv4_hderr += DWC_REG_RD(MMC_RX_IPV4_HEADER_ERROR_PKTS);
+   mmc->mmc_rx_ipv4_nopay += DWC_REG_RD(MMC_RX_IPV4_NO_PAYLOAD_PKTS);
+   mmc->mmc_rx_ipv4_frag += DWC_REG_RD(MMC_RX_IPV4_FRAGMENTED_PKTS);
+   mmc->mmc_rx_ipv4_udsbl += DWC_REG_RD(MMC_RX_IPV4_UDP_CSUM_DIS_PKTS);
+
+   /* IPV6 */
+   mmc->mmc_rx_ipv6_gd += DWC_REG_RD(MMC_RX_IPV6_GOOD_PKTS);
+   mmc->mmc_rx_ipv6_hderr += DWC_REG_RD(MMC_RX_IPV6_HEADER_ERROR_PKTS);
+   mmc->mmc_rx_ipv6_nopay += DWC_REG_RD(MMC_RX_IPV6_NO_PAYLOAD_PKTS);
+
+   /* Protocols */
+   mmc->mmc_rx_udp_gd += DWC_REG_RD(MMC_RX_UDP_GOOD_PKTS);
+   mmc->mmc_rx_udp_err += DWC_REG_RD(MMC_RX_UDP_ERROR_PKTS);
+   mmc->mmc_rx_tcp_gd += DWC_REG_RD(MMC_RX_TCP_GOOD_PKTS);
+   mmc->mmc_rx_tcp_err += DWC_REG_RD(MMC_RX_TCP_ERROR_PKTS);
+   mmc->mmc_rx_icmp_gd += DWC_REG_RD(MMC_RX_ICMP_GOOD_PKTS);
+   mmc->mmc_rx_icmp_err += DWC_REG_RD(MMC_RX_ICMP_ERROR_PKTS);
+
+   /* IPv4 */
+   mmc->mmc_rx_ipv4_gd_octets += DWC_REG_RD(MMC_RX_IPV4_GOOD_OCTETS);
+   mmc->mmc_rx_ipv4_hderr_octets += DWC_REG_RD(MMC_RX_IPV4_ERROR_OCTETS);
+   mmc->mmc_rx_ipv4_nopay_octets += DWC_REG_RD(MMC_RX_IPV4_NO_PAYLOAD_OCTETS);
+   mmc->mmc_rx_ipv4_frag_octets += DWC_REG_RD(MMC_RX_IPV4_FRAGMENTED_OCTETS);
+   mmc->mmc_rx_ipv4_udsbl_octets += DWC_REG_RD(MMC_RX_IPV4_UDP_CSUM_DIS_OCTETS);
+
+   /* IPV6 */
+   mmc->mmc_rx_ipv6_gd_octets += DWC_REG_RD(MMC_RX_IPV6_GOOD_OCTETS);
+   mmc->mmc_rx_ipv6_hderr_octets += DWC_REG_RD(MMC_RX_IPV6_HEADER_ERROR_OCTETS);
+   mmc->mmc_rx_ipv6_nopay_octets += DWC_REG_RD(MMC_RX_IPV6_NO_PAYLOAD_OCTETS);
+
+   /* Protocols */
+   mmc->mmc_rx_udp_gd_octets += DWC_REG_RD(MMC_RX_UDP_GOOD_OCTETS);
+   mmc->mmc_rx_udp_err_octets += DWC_REG_RD(MMC_RX_UDP_ERROR_OCTETS);
+   mmc->mmc_rx_tcp_gd_octets += DWC_REG_RD(MMC_RX_TCP_GOOD_OCTETS);
+   mmc->mmc_rx_tcp_err_octets += DWC_REG_RD(MMC_RX_TCP_ERROR_OCTETS);
+   mmc->mmc_rx_icmp_gd_octets += DWC_REG_RD(MMC_RX_ICMP_GOOD_OCTETS);
+   mmc->mmc_rx_icmp_err_octets += DWC_REG_RD(MMC_RX_ICMP_ERROR_OCTETS);
 
-	DBGPR("<--DWC_ETH_QOS_mmc_read\n");
+   DBGPR("<--DWC_ETH_QOS_mmc_read\n");
 }
 
 phy_interface_t DWC_ETH_QOS_get_phy_interface(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	phy_interface_t ret = PHY_INTERFACE_MODE_MII;
+   phy_interface_t ret = PHY_INTERFACE_MODE_MII;
 
-	DBGPR("-->DWC_ETH_QOS_get_phy_interface\n");
+   DBGPR("-->DWC_ETH_QOS_get_phy_interface\n");
 
-	if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_GMII_MII) {
-		if (pdata->hw_feat.gmii_sel)
-			ret = PHY_INTERFACE_MODE_GMII;
-		else if (pdata->hw_feat.mii_sel)
-			ret = PHY_INTERFACE_MODE_MII;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RGMII) {
-		ret = PHY_INTERFACE_MODE_RGMII;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_SGMII) {
-		ret = PHY_INTERFACE_MODE_SGMII;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_TBI) {
-		ret = PHY_INTERFACE_MODE_TBI;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RMII) {
-		ret = PHY_INTERFACE_MODE_RMII;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RTBI) {
-		ret = PHY_INTERFACE_MODE_RTBI;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_SMII) {
-		ret = PHY_INTERFACE_MODE_SMII;
-	} else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RevMII) {
-		//what to return ?
-	} else {
-		printk(KERN_ALERT "Missing interface support between"\
-		    "PHY and MAC\n\n");
-		ret = PHY_INTERFACE_MODE_NA;
-	}
+   if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_GMII_MII) {
+      if (pdata->hw_feat.gmii_sel)
+         ret = PHY_INTERFACE_MODE_GMII;
+      else if (pdata->hw_feat.mii_sel)
+         ret = PHY_INTERFACE_MODE_MII;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RGMII) {
+      ret = PHY_INTERFACE_MODE_RGMII;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_SGMII) {
+      ret = PHY_INTERFACE_MODE_SGMII;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_TBI) {
+      ret = PHY_INTERFACE_MODE_TBI;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RMII) {
+      ret = PHY_INTERFACE_MODE_RMII;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RTBI) {
+      ret = PHY_INTERFACE_MODE_RTBI;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_SMII) {
+      ret = PHY_INTERFACE_MODE_SMII;
+   } else if (pdata->hw_feat.act_phy_sel == DWC_ETH_QOS_RevMII) {
+      //what to return ?
+   } else {
+      printk(KERN_ALERT "Missing interface support between"\
+          "PHY and MAC\n\n");
+      ret = PHY_INTERFACE_MODE_NA;
+   }
 
-	DBGPR("<--DWC_ETH_QOS_get_phy_interface\n");
+   DBGPR("<--DWC_ETH_QOS_get_phy_interface\n");
 
-	return ret;
+   return ret;
 }
 
+uint32_t gbe_config_to_speed(uint32_t config)
+{
+   uint32_t speed = 0;
+   switch(config) {
+      case GBE_GCR5_PHY_SPEED_10M:
+         speed = 10;
+      break;
+      case GBE_GCR5_PHY_SPEED_100M:
+         speed = 100;
+      break;
+      case GBE_GCR5_PHY_SPEED_1G:
+         speed = 1000;
+      break;
+      case GBE_GCR5_PHY_SPEED_2_5G:
+         speed = 2500;
+      break;
+      case GBE_GCR5_PHY_SPEED_5G:
+         speed = 5000;
+      break;
+   }
+   return speed;
+}
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_eee.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_eee.c
@@ -60,7 +60,7 @@
 void DWC_ETH_QOS_enable_eee_mode(struct DWC_ETH_QOS_prv_data *pdata)
 {
 	struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data = NULL;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	int tx_idle = 0, qInx;
 
 	DBGPR_EEE("-->DWC_ETH_QOS_enable_eee_mode\n");
@@ -85,7 +85,7 @@ void DWC_ETH_QOS_enable_eee_mode(struct
 
 void DWC_ETH_QOS_disable_eee_mode(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 
 	DBGPR_EEE("-->DWC_ETH_QOS_disable_eee_mode\n");
 
@@ -373,7 +373,7 @@ static int DWC_ETH_QOS_phy_init_eee(stru
 */
 bool DWC_ETH_QOS_eee_init(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	bool ret = false;
 
 	DBGPR_EEE("-->DWC_ETH_QOS_eee_init\n");
@@ -427,7 +427,7 @@ bool DWC_ETH_QOS_eee_init(struct DWC_ETH
 #define MAC_LPS_RLPIEX 0x00000008
 void DWC_ETH_QOS_handle_eee_interrupt(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	uint32_t lpi_status;
 
 	DBGPR_EEE("-->DWC_ETH_QOS_handle_eee_interrupt\n");
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_ethtool.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_ethtool.c
@@ -293,7 +293,7 @@ static void DWC_ETH_QOS_get_pauseparam(s
 				       struct ethtool_pauseparam *pause)
 {
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	struct phy_device *phydev = pdata->phydev;
 	unsigned int data;
 
@@ -343,7 +343,7 @@ static int DWC_ETH_QOS_set_pauseparam(st
 				      struct ethtool_pauseparam *pause)
 {
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	struct phy_device *phydev = pdata->phydev;
 	int new_pause = DWC_ETH_QOS_FLOW_CTRL_OFF;
 	unsigned int data;
@@ -393,7 +393,7 @@ static int DWC_ETH_QOS_set_pauseparam(st
 
 void DWC_ETH_QOS_configure_flow_ctrl(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	uint32_t qInx;
 
 	DBGPR("-->DWC_ETH_QOS_configure_flow_ctrl\n");
@@ -440,7 +440,7 @@ static int DWC_ETH_QOS_getsettings(struc
 				   struct ethtool_cmd *cmd)
 {
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned int pause, duplex;
 	unsigned int lp_pause, lp_duplex;
 	int ret = 0;
@@ -545,7 +545,7 @@ static int DWC_ETH_QOS_setsettings(struc
 				   struct ethtool_cmd *cmd)
 {
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned int speed;
 	//unsigned int pause, duplex, speed;
 	//unsigned int lp_pause, lp_duplex;
@@ -765,7 +765,7 @@ static int DWC_ETH_QOS_set_coalesce(stru
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
 	struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data =
 	    GET_RX_WRAPPER_DESC(0);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned int rx_riwt, rx_usec, local_use_riwt, qInx;
 
 	DBGPR("-->DWC_ETH_QOS_set_coalesce\n");
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_mdio.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_mdio.c
@@ -55,7 +55,7 @@
 int DWC_ETH_QOS_mdio_read_direct(struct DWC_ETH_QOS_prv_data *pdata,
 				 int phyaddr, int phyreg, int *phydata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	int phy_reg_read_status;
 
 	DBGPR_MDIO("--> DWC_ETH_QOS_mdio_read_direct\n");
@@ -95,7 +95,7 @@ int DWC_ETH_QOS_mdio_read_direct(struct
 int DWC_ETH_QOS_mdio_write_direct(struct DWC_ETH_QOS_prv_data *pdata,
 				  int phyaddr, int phyreg, int phydata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	int phy_reg_write_status;
 
 	DBGPR_MDIO("--> DWC_ETH_QOS_mdio_write_direct\n");
@@ -133,7 +133,7 @@ static int DWC_ETH_QOS_mdio_read(struct
 {
 	struct net_device *dev = bus->priv;
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	int phydata;
 
 	DBGPR_MDIO("--> DWC_ETH_QOS_mdio_read: phyaddr = %d, phyreg = %d\n",
@@ -171,7 +171,7 @@ static int DWC_ETH_QOS_mdio_write(struct
 {
 	struct net_device *dev = bus->priv;
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	int ret = Y_SUCCESS;
 
 	DBGPR_MDIO("--> DWC_ETH_QOS_mdio_write\n");
@@ -205,7 +205,7 @@ static int DWC_ETH_QOS_mdio_reset(struct
 {
 	struct net_device *dev = bus->priv;
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	uint32_t phydata;
 
 	DBGPR_MDIO("-->DWC_ETH_QOS_mdio_reset: phyaddr : %d\n", pdata->phyaddr);
@@ -342,7 +342,7 @@ void dump_phy_registers(struct DWC_ETH_Q
 static void DWC_ETH_QOS_adjust_link(struct net_device *dev)
 {
 	struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	struct phy_device *phydev = pdata->phydev;
 	unsigned long flags;
 	int new_state = 0;
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_pci.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_pci.c
@@ -33,10 +33,9 @@
  * @brief: Driver functions.
  */
 
+#include <linux/netip_subsystem.h>
 #include "DWC_ETH_QOS_yheader.h"
-#include "DWC_ETH_QOS_pci.h"
 #include "DWC_ETH_QOS_yregacc.h"
-#include <linux/netip_subsystem.h>
 
 /**
    Assuming an average packet size of 1522 bytes the total
@@ -84,9 +83,71 @@ MODULE_PARM_DESC(metadata_on_crc, "Test
 module_param(mss_for_tso, uint, S_IRUGO);
 MODULE_PARM_DESC(mss_for_tso, "MSS value to test TSO");
 
+static ssize_t gbe_dbg_show(struct device *dev,
+   struct device_attribute *attr, char *buf)
+{
+   struct DWC_ETH_QOS_prv_data *pdata = NULL;
+   struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data = NULL;
+   struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data = NULL;
+   tx_descriptor_t *tx_desc = NULL;
+   rx_descriptor_t *rx_desc = NULL;
+   uint32_t qInx = 0, i, j;
+   char *st = buf;
+   pdata = container_of(attr, struct DWC_ETH_QOS_prv_data, debug_attr);
+   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
+      buf += sprintf(buf, "---------------------------------------\n");
+      tx_desc_data = GET_TX_WRAPPER_DESC(qInx);
+      buf += sprintf(buf, "[%u]cur_tx        = %d\n", qInx, tx_desc_data->cur_tx);
+      buf += sprintf(buf, "[%u]dirty_tx      = %d\n", qInx, tx_desc_data->dirty_tx);
+      buf += sprintf(buf, "[%u]free_desc_cnt = %d\n", qInx, tx_desc_data->free_desc_cnt);
+      buf += sprintf(buf, "[%u]queue_stopped = %d\n", qInx, tx_desc_data->queue_stopped);
+      buf += sprintf(buf, "[%u]tx_pkt_queued = %d\n", qInx, tx_desc_data->tx_pkt_queued);
+      i = tx_desc_data->dirty_tx;
+      DECR_TX_DESC_INDEX(i);
+      j = tx_desc_data->cur_tx;
+      INCR_TX_DESC_INDEX(j, 1);
+      while(i != j) {
+         tx_desc = GET_TX_DESC_PTR(qInx, i);
+         buf += sprintf(buf, "[%u:%u] 0x%08x:0x%08x:0x%08x:0x%08x\n", qInx, i,
+            tx_desc->TDES0, tx_desc->TDES1, tx_desc->TDES2, tx_desc->TDES3);
+         INCR_TX_DESC_INDEX(i, 1);
+      }
+      buf += sprintf(buf, "---------------------------------------\n");
+      rx_desc_data = GET_RX_WRAPPER_DESC(qInx);
+      buf += sprintf(buf, "[%u]cur_rx        = %d\n", qInx, rx_desc_data->cur_rx);
+      buf += sprintf(buf, "[%u]dirty_rx      = %d\n", qInx, rx_desc_data->dirty_rx);
+      buf += sprintf(buf, "[%u]pkt_received = %d\n", qInx, rx_desc_data->pkt_received);
+
+      i = rx_desc_data->cur_rx;
+      rx_desc = GET_RX_DESC_PTR(qInx, i);
+      while(!(rx_desc->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
+         buf += sprintf(buf, "[%u:%u] 0x%08x:0x%08x:0x%08x:0x%08x\n", qInx, i,
+            rx_desc->RDES0, rx_desc->RDES1, rx_desc->RDES2, rx_desc->RDES3);
+         INCR_RX_DESC_INDEX(i, 1);
+         rx_desc = GET_RX_DESC_PTR(qInx, i);
+      }
+      buf += sprintf(buf, "---------------------------------------\n");
+   }
+   return (buf - st);
+}
+
+static int gbe_handle_suspend_resume(void *args, netss_power_state_t state);
+
+static ssize_t gbe_suspend_store(struct device *dev,
+   struct device_attribute *attr, const char *buf, size_t count)
+{
+   unsigned int value = simple_strtoul(buf, NULL, 0);
+   struct DWC_ETH_QOS_prv_data *pdata = NULL;
+   pdata = container_of(attr, struct DWC_ETH_QOS_prv_data, suspend_attr);
+   gbe_handle_suspend_resume(pdata, (value)?
+                                    NETSS_NETIP_POWER_STATE_OFF :
+                                    NETSS_NETIP_POWER_STATE_ACTIVE);
+   return count;
+}
+
 #endif //GBE_DEBUG
 
-static int DWC_ETH_QOS_init_general_gbe(void __iomem **gbe_base,
+static int gbe_init_top_registers(void __iomem **gbe_base,
    unsigned int *mux_cfg)
 {
    int ret = 1;
@@ -167,7 +228,7 @@ static int DWC_ETH_QOS_init_general_gbe(
    return ret;
 }
 
-static void DWC_ETH_QOS_configure_IC(void __iomem *reg_base)
+static void gbe_configure_IC(void __iomem *reg_base)
 {
    // Enable GMAC5 hardware interrupts
    GBE_REG_WR_BIT(GBE_ATOM_HIE, GBE_ATOM_INTC, 0x1);
@@ -175,38 +236,15 @@ static void DWC_ETH_QOS_configure_IC(voi
    GBE_REG_WR_BIT(GBE_ATOM_ELS, GBE_ATOM_INTC, 0x1);
 }
 
-static uint32_t DWC_ETH_QOS_gbe_config_to_speed(uint32_t config)
-{
-   uint32_t speed = 0;
-   switch(config) {
-      case GBE_GCR5_PHY_SPEED_10M:
-         speed = 10;
-      break;
-      case GBE_GCR5_PHY_SPEED_100M:
-         speed = 100;
-      break;
-      case GBE_GCR5_PHY_SPEED_1G:
-         speed = 1000;
-      break;
-      case GBE_GCR5_PHY_SPEED_2_5G:
-         speed = 2500;
-      break;
-      case GBE_GCR5_PHY_SPEED_5G:
-         speed = 5000;
-      break;
-   }
-   return speed;
-}
-
-static ssize_t DWC_ETH_QOS_gbe_speed_show(struct device *dev,
+static ssize_t gbe_speed_show(struct device *dev,
    struct device_attribute *attr, char *buf)
 {
    struct DWC_ETH_QOS_prv_data *pdata = NULL;
    pdata = container_of(attr, struct DWC_ETH_QOS_prv_data, rate_attr);
-   return sprintf(buf, "%d\n", DWC_ETH_QOS_gbe_config_to_speed(pdata->rate));
+   return sprintf(buf, "%d\n", gbe_config_to_speed(pdata->rate));
 }
 
-static ssize_t DWC_ETH_QOS_gbe_speed_store(struct device *dev,
+static ssize_t gbe_speed_store(struct device *dev,
    struct device_attribute *attr, const char *buf, size_t count)
 {
    unsigned int value = simple_strtoul(buf, NULL, 0);
@@ -216,7 +254,7 @@ static ssize_t DWC_ETH_QOS_gbe_speed_sto
    return count;
 }
 
-static ssize_t DWC_ETH_QOS_gbe_stats_show(struct device *dev,
+static ssize_t gbe_stats_show(struct device *dev,
    struct device_attribute *attr, char *buf)
 {
    struct DWC_ETH_QOS_prv_data *pdata = NULL;
@@ -234,59 +272,7 @@ static ssize_t DWC_ETH_QOS_gbe_stats_sho
    return (buf - st);
 }
 
-#ifdef GBE_DEBUG
-
-static ssize_t DWC_ETH_QOS_gbe_dbg_show(struct device *dev,
-   struct device_attribute *attr, char *buf)
-{
-   struct DWC_ETH_QOS_prv_data *pdata = NULL;
-   struct DWC_ETH_QOS_tx_wrapper_descriptor *tx_desc_data = NULL;
-   struct DWC_ETH_QOS_rx_wrapper_descriptor *rx_desc_data = NULL;
-   tx_descriptor_t *tx_desc = NULL;
-   rx_descriptor_t *rx_desc = NULL;
-   uint32_t qInx = 0, i, j;
-   char *st = buf;
-   pdata = container_of(attr, struct DWC_ETH_QOS_prv_data, debug_attr);
-   for (qInx = 0; qInx < DWC_ETH_QOS_RX_QUEUE_CNT; qInx++) {
-      buf += sprintf(buf, "---------------------------------------\n");
-      tx_desc_data = GET_TX_WRAPPER_DESC(qInx);
-      buf += sprintf(buf, "[%u]cur_tx        = %d\n", qInx, tx_desc_data->cur_tx);
-      buf += sprintf(buf, "[%u]dirty_tx      = %d\n", qInx, tx_desc_data->dirty_tx);
-      buf += sprintf(buf, "[%u]free_desc_cnt = %d\n", qInx, tx_desc_data->free_desc_cnt);
-      buf += sprintf(buf, "[%u]queue_stopped = %d\n", qInx, tx_desc_data->queue_stopped);
-      buf += sprintf(buf, "[%u]tx_pkt_queued = %d\n", qInx, tx_desc_data->tx_pkt_queued);
-      i = tx_desc_data->dirty_tx;
-      DECR_TX_DESC_INDEX(i);
-      j = tx_desc_data->cur_tx;
-      INCR_TX_DESC_INDEX(j, 1);
-      while(i != j) {
-         tx_desc = GET_TX_DESC_PTR(qInx, i);
-         buf += sprintf(buf, "[%u:%u] 0x%08x:0x%08x:0x%08x:0x%08x\n", qInx, i,
-            tx_desc->TDES0, tx_desc->TDES1, tx_desc->TDES2, tx_desc->TDES3);
-         INCR_TX_DESC_INDEX(i, 1);
-      }
-      buf += sprintf(buf, "---------------------------------------\n");
-      rx_desc_data = GET_RX_WRAPPER_DESC(qInx);
-      buf += sprintf(buf, "[%u]cur_rx        = %d\n", qInx, rx_desc_data->cur_rx);
-      buf += sprintf(buf, "[%u]dirty_rx      = %d\n", qInx, rx_desc_data->dirty_rx);
-      buf += sprintf(buf, "[%u]pkt_received = %d\n", qInx, rx_desc_data->pkt_received);
-
-      i = rx_desc_data->cur_rx;
-      rx_desc = GET_RX_DESC_PTR(qInx, i);
-      while(!(rx_desc->RDES3 & DWC_ETH_QOS_RDESC3_OWN)) {
-         buf += sprintf(buf, "[%u:%u] 0x%08x:0x%08x:0x%08x:0x%08x\n", qInx, i,
-            rx_desc->RDES0, rx_desc->RDES1, rx_desc->RDES2, rx_desc->RDES3);
-         INCR_RX_DESC_INDEX(i, 1);
-         rx_desc = GET_RX_DESC_PTR(qInx, i);
-      }
-      buf += sprintf(buf, "---------------------------------------\n");
-   }
-   return (buf - st);
-}
-
-#endif
-
-static ssize_t DWC_ETH_QOS_gbe_itr_ips_show(struct device *dev,
+static ssize_t gbe_itr_ips_show(struct device *dev,
    struct device_attribute *attr, char *buf)
 {
    struct DWC_ETH_QOS_prv_data *pdata = NULL;
@@ -294,7 +280,7 @@ static ssize_t DWC_ETH_QOS_gbe_itr_ips_s
    return sprintf(buf, "%d\n", ONE_SEC_TO_NS/pdata->itr_latency);
 }
 
-static ssize_t DWC_ETH_QOS_gbe_itr_ips_store(struct device *dev,
+static ssize_t gbe_itr_ips_store(struct device *dev,
    struct device_attribute *attr, const char *buf, size_t count)
 {
    uint32_t max_ips = simple_strtoul(buf, NULL, 0);
@@ -304,12 +290,12 @@ static ssize_t DWC_ETH_QOS_gbe_itr_ips_s
    return count;
 }
 
-static void DWC_ETH_QOS_create_gbe_sysfs(struct DWC_ETH_QOS_prv_data *pdata)
+static void create_gbe_sysfs(struct DWC_ETH_QOS_prv_data *pdata)
 {
    struct device_attribute *dev_attr = &pdata->rate_attr;
    sysfs_attr_init(&dev_attr->attr);
-   dev_attr->show = DWC_ETH_QOS_gbe_speed_show;
-   dev_attr->store = DWC_ETH_QOS_gbe_speed_store;
+   dev_attr->show = gbe_speed_show;
+   dev_attr->store = gbe_speed_store;
    dev_attr->attr.mode = S_IRUGO | S_IWUSR;
    dev_attr->attr.name = "rate";
    if (device_create_file(&pdata->dev->dev, dev_attr)) {
@@ -317,7 +303,7 @@ static void DWC_ETH_QOS_create_gbe_sysfs
    }
    dev_attr = &pdata->stats_attr;
    sysfs_attr_init(&dev_attr->attr);
-   dev_attr->show = DWC_ETH_QOS_gbe_stats_show;
+   dev_attr->show = gbe_stats_show;
    dev_attr->store = NULL;
    dev_attr->attr.mode = S_IRUGO;
    dev_attr->attr.name = "stats";
@@ -327,18 +313,27 @@ static void DWC_ETH_QOS_create_gbe_sysfs
 #ifdef GBE_DEBUG
    dev_attr = &pdata->debug_attr;
    sysfs_attr_init(&dev_attr->attr);
-   dev_attr->show = DWC_ETH_QOS_gbe_dbg_show;
+   dev_attr->show = gbe_dbg_show;
    dev_attr->store = NULL;
    dev_attr->attr.mode = S_IRUGO;
    dev_attr->attr.name = "debug";
    if (device_create_file(&pdata->dev->dev, dev_attr)) {
       printk(KERN_ALERT "[GBE] Error creating debug sysfs attribute!\n");
    }
+   dev_attr = &pdata->suspend_attr;
+   sysfs_attr_init(&dev_attr->attr);
+   dev_attr->show = NULL;
+   dev_attr->store = gbe_suspend_store;
+   dev_attr->attr.mode = S_IWUSR;
+   dev_attr->attr.name = "suspend";
+   if (device_create_file(&pdata->dev->dev, dev_attr)) {
+      printk(KERN_ALERT "[GBE] Error creating suspend sysfs attribute!\n");
+   }
 #endif
    dev_attr = &pdata->itr_lat_attr;
    sysfs_attr_init(&dev_attr->attr);
-   dev_attr->show = DWC_ETH_QOS_gbe_itr_ips_show;
-   dev_attr->store = DWC_ETH_QOS_gbe_itr_ips_store;
+   dev_attr->show = gbe_itr_ips_show;
+   dev_attr->store = gbe_itr_ips_store;
    dev_attr->attr.mode = S_IRUGO | S_IWUSR;
    dev_attr->attr.name = "itr_max_ips";
    if (device_create_file(&pdata->dev->dev, dev_attr)) {
@@ -346,13 +341,25 @@ static void DWC_ETH_QOS_create_gbe_sysfs
    }
 }
 
-void DWC_ETH_QOS_gbe_core_version(struct DWC_ETH_QOS_prv_data *pdata)
+void gbe_core_version(struct DWC_ETH_QOS_prv_data *pdata)
 {
-   //Enable Interrupt Controller if version is 4.00
+   // Enable Interrupt Controller if version is 4.00
    pdata->version = DWC_REG_RD_FIELD(MAC_VR, MAC_VR_SNPSVER);
    CFG_PRINT("[GBE] Core version = 0x%02x\n", pdata->version);
    if (pdata->version == MAC_VER_4_00)
-      DWC_ETH_QOS_configure_IC(pdata->gbe_base);
+      gbe_configure_IC(pdata->gbe_base);
+}
+
+static int gbe_handle_suspend_resume(void *args, netss_power_state_t state)
+{
+   struct DWC_ETH_QOS_prv_data *pdata = (struct DWC_ETH_QOS_prv_data *)args;
+   int ret = -EINVAL;
+   if (state == NETSS_NETIP_POWER_STATE_OFF) {
+      ret = DWC_ETH_QOS_powerdown(pdata->dev, DWC_ETH_QOS_NETIP_WAKEUP);
+   } else if (state == NETSS_NETIP_POWER_STATE_ACTIVE) {
+      ret = DWC_ETH_QOS_powerup(pdata->dev);
+   }
+   return ret;
 }
 
 void DWC_ETH_QOS_init_all_fptrs(struct DWC_ETH_QOS_prv_data *pdata)
@@ -387,18 +394,19 @@ int DWC_ETH_QOS_probe(struct pci_dev *pd
    struct DWC_ETH_QOS_prv_data *pdata = NULL;
    struct net_device *dev = NULL;
    int i, ret = 0;
-   struct hw_if_struct *hw_if = NULL;
+   hw_interface_t *hw_if = NULL;
    struct desc_if_struct *desc_if = NULL;
    uint8_t tx_q_count = 0, rx_q_count = 0;
    void __iomem *gbe_base;
    unsigned int gbe_mux_cfg;
+   netss_power_state_callback_info_t pm_callback_info;
 #ifdef GBE_DEBUG
    char dbg_str[]="_DEBUG";
 #else
    char dbg_str[]="";
 #endif
 
-   DBGPR("--> DWC_ETH_QOS_probe\n");
+   CFG_PRINT("--> DWC_ETH_QOS_probe\n");
 
    if ((ret = pci_enable_device(pdev)) != 0) {
       printk(KERN_ALERT "%s:Unable to enable device\n", DEV_NAME);
@@ -413,7 +421,7 @@ int DWC_ETH_QOS_probe(struct pci_dev *pd
       }
    }
 
-   if (DWC_ETH_QOS_init_general_gbe(&gbe_base, &gbe_mux_cfg)) {
+   if (gbe_init_top_registers(&gbe_base, &gbe_mux_cfg)) {
       ret = -ENODEV;
       goto err_out_req_reg_failed;
    }
@@ -472,6 +480,8 @@ int DWC_ETH_QOS_probe(struct pci_dev *pd
    hw_if = &(pdata->hw_if);
    desc_if = &(pdata->desc_if);
 
+   /* Initialize HW configuration variables */
+   memset(&pdata->hw_cfg, 0, sizeof(hw_config_t));
    pci_set_drvdata(pdev, dev);
    pdata->pdev = pdev;
    pdata->dev = dev;
@@ -484,10 +494,10 @@ int DWC_ETH_QOS_probe(struct pci_dev *pd
    pdata->itr_latency = ONE_SEC_TO_NS/DEFAULT_NUM_IPS;
 
    /* Verify GMAC core version */
-   DWC_ETH_QOS_gbe_core_version(pdata);
+   gbe_core_version(pdata);
 
    /* issue software reset to device */
-   hw_if->exit();
+   hw_if->sw_reset();
    dev->irq = pdev->irq;
 
    DWC_ETH_QOS_get_all_hw_features(pdata);
@@ -614,16 +624,28 @@ int DWC_ETH_QOS_probe(struct pci_dev *pd
    create_debug_files();
 #endif
 
-   DWC_ETH_QOS_create_gbe_sysfs(pdata);
+   create_gbe_sysfs(pdata);
+
+   /* Register PM callback with NetSS driver */
+   pm_callback_info.func = gbe_handle_suspend_resume;
+   pm_callback_info.args = pdata;
+   if (netss_power_state_change_callback_register(NETSS_DEV_GBE, &pm_callback_info)) {
+      ERR_PRINT("Failed to register PM callback with NetSS!\n");
+      // TODO:
+      // - Determine if it's ok to continue or return error code
+      // - What if NetIP is in DSBY already?
+   }
 
    if (pdata->hw_feat.pcs_sel) {
       netif_carrier_off(dev);
       CFG_PRINT("Carrier off till LINK is up\n");
    }
 
-   printk(KERN_INFO "Synopsys DWC_QOS_ETH%s driver built on %s @ %s\n", dbg_str, __DATE__, __TIME__);
+   printk(KERN_INFO "Synopsys DWC_ETH_QOS%s driver built on %s @ %s\n",
+          dbg_str, __DATE__, __TIME__);
+
+   CFG_PRINT("<-- DWC_ETH_QOS_probe\n");
 
-   DBGPR("<-- DWC_ETH_QOS_probe\n");
    return 0;
 
  err_out_netdev_failed:
@@ -681,9 +703,15 @@ void DWC_ETH_QOS_remove(struct pci_dev *
    struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
    struct desc_if_struct *desc_if = &(pdata->desc_if);
    void __iomem *reg_base = pdata->gbe_base;
+   netss_power_state_callback_info_t pm_callback_info;
 
    DBGPR("--> DWC_ETH_QOS_remove\n");
 
+   /* Deregister PM callback with NetSS driver */
+   pm_callback_info.func = NULL;
+   pm_callback_info.args = NULL;
+   netss_power_state_change_callback_register(NETSS_DEV_GBE, &pm_callback_info);
+
    if (pdata->irq_number != 0) {
       free_irq(pdata->irq_number, pdata);
       pdata->irq_number = 0;
@@ -727,6 +755,11 @@ void DWC_ETH_QOS_remove(struct pci_dev *
    return;
 }
 
+#ifdef CONFIG_PM
+static int DWC_ETH_QOS_suspend(struct pci_dev *, pm_message_t);
+static int DWC_ETH_QOS_resume(struct pci_dev *);
+#endif
+
 static struct pci_device_id DWC_ETH_QOS_id[] = {
    {PCI_DEVICE(VENDOR_ID, DEVICE_ID)},
    {0}, /* terminate list */
@@ -739,9 +772,6 @@ static struct pci_driver DWC_ETH_QOS_pci
    .id_table = DWC_ETH_QOS_id,
    .probe = DWC_ETH_QOS_probe,
    .remove = DWC_ETH_QOS_remove,
-   .shutdown = DWC_ETH_QOS_shutdown,
-   .suspend_late = DWC_ETH_QOS_suspend_late,
-   .resume_early = DWC_ETH_QOS_resume_early,
 #ifdef CONFIG_PM
    .suspend = DWC_ETH_QOS_suspend,
    .resume = DWC_ETH_QOS_resume,
@@ -752,33 +782,6 @@ static struct pci_driver DWC_ETH_QOS_pci
    },
 };
 
-static void DWC_ETH_QOS_shutdown(struct pci_dev *pdev)
-{
-   printk(KERN_ALERT "-->DWC_ETH_QOS_shutdown\n");
-   printk(KERN_ALERT "Handle the shutdown\n");
-   printk(KERN_ALERT ">--DWC_ETH_QOS_shutdown\n");
-
-   return;
-}
-
-static int DWC_ETH_QOS_suspend_late(struct pci_dev *pdev, pm_message_t state)
-{
-   printk(KERN_ALERT "-->DWC_ETH_QOS_suspend_late\n");
-   printk(KERN_ALERT "Handle the suspend_late\n");
-   printk(KERN_ALERT "<--DWC_ETH_QOS_suspend_late\n");
-
-   return 0;
-}
-
-static int DWC_ETH_QOS_resume_early(struct pci_dev *pdev)
-{
-   printk(KERN_ALERT "-->DWC_ETH_QOS_resume_early\n");
-   printk(KERN_ALERT "Handle the resume_early\n");
-   printk(KERN_ALERT "<--DWC_ETH_QOS_resume_early\n");
-
-   return 0;
-}
-
 #ifdef CONFIG_PM
 
 /*!
@@ -802,12 +805,11 @@ static int DWC_ETH_QOS_resume_early(stru
  *
  * \retval 0
  */
-
 static int DWC_ETH_QOS_suspend(struct pci_dev *pdev, pm_message_t state)
 {
    struct net_device *dev = pci_get_drvdata(pdev);
    struct DWC_ETH_QOS_prv_data *pdata = netdev_priv(dev);
-   struct hw_if_struct *hw_if = &(pdata->hw_if);
+   hw_interface_t *hw_if = &(pdata->hw_if);
    int ret, pmt_flags = 0;
    unsigned int rwk_filter_values[] = {
       /* for filter 0 CRC is computed on 0 - 7 bytes from offset */
@@ -854,7 +856,7 @@ static int DWC_ETH_QOS_suspend(struct pc
    if (pdata->hw_feat.mgk_sel && (pdata->wolopts & WAKE_MAGIC))
       pmt_flags |= DWC_ETH_QOS_MAGIC_WAKEUP;
 
-   ret = DWC_ETH_QOS_powerdown(dev, pmt_flags, DWC_ETH_QOS_DRIVER_CONTEXT);
+   ret = DWC_ETH_QOS_powerdown(dev, pmt_flags);
    pci_save_state(pdev);
    pci_set_power_state(pdev, pci_choose_state(pdev, state));
 
@@ -884,7 +886,6 @@ static int DWC_ETH_QOS_suspend(struct pc
  *
  * \retval 0
  */
-
 static int DWC_ETH_QOS_resume(struct pci_dev *pdev)
 {
    struct net_device *dev = pci_get_drvdata(pdev);
@@ -900,7 +901,7 @@ static int DWC_ETH_QOS_resume(struct pci
    pci_set_power_state(pdev, PCI_D0);
    pci_restore_state(pdev);
 
-   ret = DWC_ETH_QOS_powerup(dev, DWC_ETH_QOS_DRIVER_CONTEXT);
+   ret = DWC_ETH_QOS_powerup(dev);
 
    DBGPR("<--DWC_ETH_QOS_resume\n");
 
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_pci.h
+++ /dev/null
@@ -1,52 +0,0 @@
-/* =========================================================================
- * The Synopsys DWC ETHER QOS Software Driver and documentation (hereinafter
- * "Software") is an unsupported proprietary work of Synopsys, Inc. unless
- * otherwise expressly agreed to in writing between Synopsys and you.
- *
- * The Software IS NOT an item of Licensed Software or Licensed Product under
- * any End User Software License Agreement or Agreement for Licensed Product
- * with Synopsys or any supplement thereto.  Permission is hereby granted,
- * free of charge, to any person obtaining a copy of this software annotated
- * with this license and the Software, to deal in the Software without
- * restriction, including without limitation the rights to use, copy, modify,
- * merge, publish, distribute, sublicense, and/or sell copies of the Software,
- * and to permit persons to whom the Software is furnished to do so, subject
- * to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included in
- * all copies or substantial portions of the Software.
- *
- * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
- * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
- * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
- * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
- * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
- * DAMAGE.
- * ========================================================================= */
-
-#ifndef __DWC_ETH_QOS__PCI_H__
-
-#define __DWC_ETH_QOS__PCI_H__
-
-int DWC_ETH_QOS_probe(struct pci_dev *, const struct pci_device_id *);
-
-void DWC_ETH_QOS_remove(struct pci_dev *);
-
-static void DWC_ETH_QOS_shutdown(struct pci_dev *);
-
-static int DWC_ETH_QOS_suspend_late(struct pci_dev *, pm_message_t);
-
-static int DWC_ETH_QOS_resume_early(struct pci_dev *);
-
-#ifdef CONFIG_PM
-static int DWC_ETH_QOS_suspend(struct pci_dev *, pm_message_t);
-
-static int DWC_ETH_QOS_resume(struct pci_dev *);
-#endif				/* end of CONFIG_PM */
-
-#endif
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_pktgen.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_pktgen.c
@@ -140,7 +140,7 @@ static int DWC_ETH_QOS_poll_pg_sq(struct
 	struct DWC_ETH_QOS_pg_ch_input *pg_ch_input =
 		&(pdata->pg->pg_ch_input[qInx]);
 	int received = 0;
-	struct hw_if_struct *hw_if = &pdata->hw_if;
+	hw_interface_t *hw_if = &pdata->hw_if;
 	struct sk_buff *skb = NULL;
 
 	DBGPR_PG("-->DWC_ETH_QOS_poll_pg_sq: qInx = %u\n", qInx);
@@ -263,7 +263,7 @@ static void DWC_ETH_QOS_tx_interrupt_pg(
 	    GET_TX_WRAPPER_DESC(qInx);
 	tx_descriptor_t *txptr = NULL;
 	struct DWC_ETH_QOS_tx_buffer *buffer = NULL;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	int err_incremented;
 	uint32_t reg_tail_ptr = 0, var_tail_ptr = 0, tail_ptr = 0, head_ptr = 0;
 	struct DWC_ETH_QOS_pg_ch_input *pg_ch_input =
@@ -782,7 +782,7 @@ static void DWC_ETH_QOS_prepare_hw_for_p
 	struct DWC_ETH_QOS_PGStruct *pg_struct = pdata->pg;
 	unsigned int qInx;
 	struct DWC_ETH_QOS_pg_ch_input *pg_ch_input = pg_struct->pg_ch_input;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	struct timespec now;
 	uint32_t varMAC_TCR = 0;
 
@@ -846,7 +846,7 @@ static void DWC_ETH_QOS_prepare_hw_for_p
 static void DWC_ETH_QOS_pg_timer_fun(unsigned long data)
 {
 	struct DWC_ETH_QOS_prv_data *pdata = (struct DWC_ETH_QOS_prv_data *)data;
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data = NULL;
 	struct DWC_ETH_QOS_pg_ch_input *pg_ch_input = NULL;
 	uint32_t qInx, head_ptr = 0, tail_ptr = 0, dma_dsr = 0;
@@ -906,7 +906,7 @@ static void DWC_ETH_QOS_pg_timer_fun(uns
 */
 static void DWC_ETH_QOS_pg_run(struct DWC_ETH_QOS_prv_data *pdata)
 {
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned int qInx;
 
 	DBGPR_PG("-->DWC_ETH_QOS_pg_run\n");
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_ptp.c
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_ptp.c
@@ -53,7 +53,7 @@ static int DWC_ETH_QOS_adjust_freq(struc
 {
 	struct DWC_ETH_QOS_prv_data *pdata =
 		container_of(ptp, struct DWC_ETH_QOS_prv_data, ptp_clock_ops);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned long flags;
 	uint64_t adj;
 	uint32_t diff, addend;
@@ -106,7 +106,7 @@ static int DWC_ETH_QOS_adjust_time(struc
 {
 	struct DWC_ETH_QOS_prv_data *pdata =
 		container_of(ptp, struct DWC_ETH_QOS_prv_data, ptp_clock_ops);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned long flags;
 	uint32_t sec, nsec;
 	uint32_t quotient, reminder;
@@ -154,7 +154,7 @@ static int DWC_ETH_QOS_get_time(struct p
 {
 	struct DWC_ETH_QOS_prv_data *pdata =
 		container_of(ptp, struct DWC_ETH_QOS_prv_data, ptp_clock_ops);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	uint64_t ns;
 	uint32_t reminder;
 	unsigned long flags;
@@ -196,7 +196,7 @@ static int DWC_ETH_QOS_set_time(struct p
 {
 	struct DWC_ETH_QOS_prv_data *pdata =
 		container_of(ptp, struct DWC_ETH_QOS_prv_data, ptp_clock_ops);
-	struct hw_if_struct *hw_if = &(pdata->hw_if);
+	hw_interface_t *hw_if = &(pdata->hw_if);
 	unsigned long flags;
 
 	DBGPR_PTP("-->DWC_ETH_QOS_set_time: ts->tv_sec = %ld,"
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_yheader.h
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_yheader.h
@@ -131,7 +131,8 @@
 
 /* Enable polling GBE ISR status registers */
 //#define GBE_POLLING
-/* Enable polling GBE debug logic */
+
+/* Enable GBE debug logic */
 //#define GBE_DEBUG
 
 #ifdef DWC_ETH_QOS_CONFIG_PTP
@@ -248,17 +249,19 @@
 #define DWC_ETH_QOS_Q_DCB 			0x2
 #define DWC_ETH_QOS_Q_GENERIC 		0x3
 
-/* Driver PMT macros */
-#define DWC_ETH_QOS_DRIVER_CONTEXT 1
-#define DWC_ETH_QOS_IOCTL_CONTEXT 2
-#define DWC_ETH_QOS_MAGIC_WAKEUP	(1 << 0)
-#define DWC_ETH_QOS_REMOTE_WAKEUP	(1 << 1)
-#define DWC_ETH_QOS_POWER_DOWN_TYPE(x)	\
-		((x->power_down_type & DWC_ETH_QOS_MAGIC_WAKEUP) ? \
-		"Magic packet" : \
-		((x->power_down_type & DWC_ETH_QOS_REMOTE_WAKEUP) ? \
-		"Remote wakeup packet" : \
-		"<error>"))
+/* Driver power status macros */
+#define DWC_ETH_QOS_POWER_ON           (0)
+#define DWC_ETH_QOS_MAGIC_WAKEUP       (1 << 0)
+#define DWC_ETH_QOS_REMOTE_WAKEUP      (1 << 1)
+#define DWC_ETH_QOS_NETIP_WAKEUP       (1 << 2)
+#define DWC_ETH_QOS_NETIP_PWRDWN       (1 << 3) // Intermediate state to indicate
+                                                // the device is going to StandBy mode
+#define DWC_ETH_QOS_NETIP_PWRUP        (1 << 4) // Intermediate state to indicate
+                                                // the device is exiting the StandBy mode
+#define DWC_ETH_QOS_NETIP_SPLHDR_REQ   (1 << 5) // Indicate a split header change was
+                                                // received while device was in StandBy mode
+#define DWC_ETH_QOS_NETIP_MTU_REQ      (1 << 6) // Indicate a mtu change request was
+                                                // received while device was in StandBy mode
 
 #define DWC_ETH_QOS_MAC_ADDR_LEN 6
 #define DWC_ETH_QOS_ETH_FRAME_LEN (ETH_FRAME_LEN + ETH_FCS_LEN + VLAN_HLEN)
@@ -493,7 +496,13 @@ typedef enum {
 struct DWC_ETH_QOS_prv_data;
 struct DWC_ETH_QOS_tx_wrapper_descriptor;
 
-struct hw_if_struct {
+typedef struct {
+   uint32_t mac_ier;
+   uint32_t dma_ier;
+   // TODO: Add descriptors configuration to save time in Tx and Rx operations
+} hw_config_t;
+
+typedef struct {
 
 	int(*tx_complete) (tx_descriptor_t *);
 	int(*tx_window_error) (tx_descriptor_t *);
@@ -529,7 +538,7 @@ struct hw_if_struct {
 	int(*stop_mac_tx_rx) (void);
 
 	int(*init) (struct DWC_ETH_QOS_prv_data *);
-	int(*exit) (void);
+	int(*sw_reset) (void);
 
 	void (*pre_xmit) (struct DWC_ETH_QOS_prv_data *, uint32_t qInx);
 	void (*dev_read) (struct DWC_ETH_QOS_prv_data *, uint32_t qInx);
@@ -537,54 +546,54 @@ struct hw_if_struct {
 	void (*rx_desc_init) (struct DWC_ETH_QOS_prv_data *, uint32_t qInx);
 	void (*rx_desc_reset) (uint32_t, struct DWC_ETH_QOS_prv_data *,
 			       uint32_t, uint32_t qInx);
-	 int(*tx_desc_reset) (uint32_t, struct DWC_ETH_QOS_prv_data *, uint32_t qInx);
-	/* last tx segmnet reports the tx status */
-	 int(*get_tx_desc_ls) (tx_descriptor_t *);
-	 int(*get_tx_desc_ctxt) (tx_descriptor_t *);
-	void (*update_rx_tail_ptr) (unsigned int qInx, unsigned int dma_addr);
-
-	/* for FLOW ctrl */
-	 int(*enable_rx_flow_ctrl) (void);
-	 int(*disable_rx_flow_ctrl) (void);
-	 int(*enable_tx_flow_ctrl) (uint32_t);
-	 int(*disable_tx_flow_ctrl) (uint32_t);
-
-	/* for PMT operations */
-	 int(*enable_magic_pmt) (void);
-	 int(*disable_magic_pmt) (void);
-	 int(*enable_remote_pmt) (void);
-	 int(*disable_remote_pmt) (void);
-	 int(*configure_rwk_filter) (uint32_t *, uint32_t);
-
-	/* for RX watchdog timer */
-	 int(*config_rx_watchdog) (uint32_t, uint32_t riwt);
-
-	/* for RX and TX threshold config */
-	 int(*config_rx_threshold) (uint32_t ch_no, uint32_t val);
-	 int(*config_tx_threshold) (uint32_t ch_no, uint32_t val);
-
-	/* for RX and TX Store and Forward Mode config */
-	 int(*config_rsf_mode) (uint32_t ch_no, uint32_t val);
-	 int(*config_tsf_mode) (uint32_t ch_no, uint32_t val);
-
-	/* for TX DMA Operate on Second Frame config */
-	 int(*config_osf_mode) (uint32_t ch_no, uint32_t val);
-
-	/* for INCR/INCRX config */
-	 int(*config_incr_incrx_mode) (uint32_t val);
-	/* for AXI PBL config */
-	int(*config_axi_pbl_val) (uint32_t val);
-	/* for AXI WORL config */
-	int(*config_axi_worl_val) (uint32_t val);
-	/* for AXI RORL config */
-	int(*config_axi_rorl_val) (uint32_t val);
+   int(*tx_desc_reset) (uint32_t, struct DWC_ETH_QOS_prv_data *, uint32_t qInx);
+   /* last tx segmnet reports the tx status */
+   int(*get_tx_desc_ls) (tx_descriptor_t *);
+   int(*get_tx_desc_ctxt) (tx_descriptor_t *);
+   void (*update_rx_tail_ptr) (unsigned int qInx, unsigned int dma_addr);
+
+   /* for FLOW ctrl */
+   int(*enable_rx_flow_ctrl) (void);
+   int(*disable_rx_flow_ctrl) (void);
+   int(*enable_tx_flow_ctrl) (uint32_t);
+   int(*disable_tx_flow_ctrl) (uint32_t);
+
+   /* for PMT operations */
+   int(*enable_magic_pmt) (void);
+   int(*disable_magic_pmt) (void);
+   int(*enable_remote_pmt) (void);
+   int(*disable_remote_pmt) (void);
+   int(*configure_rwk_filter) (uint32_t *, uint32_t);
+
+   /* for RX watchdog timer */
+   int(*config_rx_watchdog) (uint32_t, uint32_t riwt);
+
+   /* for RX and TX threshold config */
+   int(*config_rx_threshold) (uint32_t ch_no, uint32_t val);
+   int(*config_tx_threshold) (uint32_t ch_no, uint32_t val);
+
+   /* for RX and TX Store and Forward Mode config */
+   int(*config_rsf_mode) (uint32_t ch_no, uint32_t val);
+   int(*config_tsf_mode) (uint32_t ch_no, uint32_t val);
+
+   /* for TX DMA Operate on Second Frame config */
+   int(*config_osf_mode) (uint32_t ch_no, uint32_t val);
+
+   /* for INCR/INCRX config */
+   int(*config_incr_incrx_mode) (uint32_t val);
+   /* for AXI PBL config */
+   int(*config_axi_pbl_val) (uint32_t val);
+   /* for AXI WORL config */
+   int(*config_axi_worl_val) (uint32_t val);
+   /* for AXI RORL config */
+   int(*config_axi_rorl_val) (uint32_t val);
 
 	/* for RX and TX PBL config */
-	 int(*config_rx_pbl_val) (uint32_t ch_no, uint32_t val);
-	 int(*get_rx_pbl_val) (uint32_t ch_no);
-	 int(*config_tx_pbl_val) (uint32_t ch_no, uint32_t val);
-	 int(*get_tx_pbl_val) (uint32_t ch_no);
-	 int(*config_pblx8) (uint32_t ch_no, uint32_t val);
+   int(*config_rx_pbl_val) (uint32_t ch_no, uint32_t val);
+   int(*get_rx_pbl_val) (uint32_t ch_no);
+   int(*config_tx_pbl_val) (uint32_t ch_no, uint32_t val);
+   int(*get_tx_pbl_val) (uint32_t ch_no);
+   int(*config_pblx8) (uint32_t ch_no, uint32_t val);
 
 	/* for TX vlan control */
 	 void(*enable_vlan_reg_control) (struct DWC_ETH_QOS_tx_wrapper_descriptor *desc_data);
@@ -594,12 +603,12 @@ struct hw_if_struct {
 	 void(*configure_sa_via_reg) (uint32_t);
 
 	/* for handling rx interrupts */
-	void(*disable_rx_interrupt)(uint32_t);
-	void(*enable_rx_interrupt)(uint32_t);
+	void(*disable_rx_interrupt)(uint32_t, hw_config_t *);
+	void(*enable_rx_interrupt)(uint32_t, hw_config_t *);
 
 	/* for handling tx interrupts */
-	void(*disable_tx_interrupt)(uint32_t);
-	void(*enable_tx_interrupt)(uint32_t);
+	void(*disable_tx_interrupt)(uint32_t, hw_config_t *);
+	void(*enable_tx_interrupt)(uint32_t, hw_config_t *);
 
 	/* for handling MMC */
 	int(*disable_mmc_interrupts)(void);
@@ -713,7 +722,7 @@ struct hw_if_struct {
     /* for PTP offloading */
 	void(*config_ptpoffload_engine)(uint32_t, uint32_t);
 
-};
+} hw_interface_t;
 
 /* wrapper buffer structure to hold transmit pkt details */
 struct DWC_ETH_QOS_tx_buffer {
@@ -1072,7 +1081,8 @@ struct DWC_ETH_QOS_prv_data {
 	uint32_t mem_start_addr;
 	uint32_t mem_size;
 	int irq_number;
-	struct hw_if_struct hw_if;
+	hw_interface_t hw_if;
+	hw_config_t hw_cfg;
 	struct desc_if_struct desc_if;
 
 //	uint32_t tx_error_counters;
@@ -1125,9 +1135,8 @@ struct DWC_ETH_QOS_prv_data {
 	uint32_t tx_sa_ctrl_via_reg;
 	unsigned char mac_addr[DWC_ETH_QOS_MAC_ADDR_LEN];
 
-	/* keeps track of power mode for API based PMT control */
-	uint32_t power_down;
-	uint32_t power_down_type;
+	/* Keeps track of power mode */
+	uint32_t power_state;
 
 	/* AXI parameters */
 	uint32_t incr_incrx;
@@ -1246,6 +1255,7 @@ struct DWC_ETH_QOS_prv_data {
 	struct device_attribute itr_lat_attr;
 #ifdef GBE_DEBUG
 	struct device_attribute debug_attr;
+	struct device_attribute suspend_attr;
 #endif
 #ifdef GBE_POLLING
 	struct hrtimer gbe_timer;
@@ -1253,13 +1263,14 @@ struct DWC_ETH_QOS_prv_data {
 };
 
 typedef enum {
-	eSAVE,
-	eRESTORE
-} e_int_state;
+   GBE_STOP_STATE,
+   GBE_RUN_STATE,
+   GBE_STANDBY_STATE
+} gbe_power_state_t;
 
 /* Function prototypes*/
 
-void DWC_ETH_QOS_init_function_ptrs_dev(struct hw_if_struct *);
+void DWC_ETH_QOS_init_function_ptrs_dev(hw_interface_t *);
 void DWC_ETH_QOS_init_function_ptrs_desc(struct desc_if_struct *);
 struct net_device_ops *DWC_ETH_QOS_get_netdev_ops(void);
 struct ethtool_ops *DWC_ETH_QOS_get_ethtool_ops(void);
@@ -1286,8 +1297,8 @@ void print_pkt(struct sk_buff *skb, int
 void DWC_ETH_QOS_get_all_hw_features(struct DWC_ETH_QOS_prv_data *pdata);
 void DWC_ETH_QOS_print_all_hw_features(struct DWC_ETH_QOS_prv_data *pdata);
 void DWC_ETH_QOS_configure_flow_ctrl(struct DWC_ETH_QOS_prv_data *pdata);
-int DWC_ETH_QOS_powerup(struct net_device *, uint32_t);
-int DWC_ETH_QOS_powerdown(struct net_device *, uint32_t, uint32_t);
+int DWC_ETH_QOS_powerup(struct net_device *);
+int DWC_ETH_QOS_powerdown(struct net_device *, uint32_t);
 uint32_t DWC_ETH_QOS_usec2riwt(uint32_t usec, struct DWC_ETH_QOS_prv_data *pdata);
 void DWC_ETH_QOS_init_rx_coalesce(struct DWC_ETH_QOS_prv_data *pdata);
 void DWC_ETH_QOS_enable_rx_interrupts(struct DWC_ETH_QOS_prv_data *pdata);
@@ -1373,6 +1384,12 @@ do { \
    } \
 } while (0)
 
+#define ERR_PRINT(x) \
+   printk(KERN_ERR "[%s] ERROR: " x, __FUNCTION__)
+
+#define WRN_PRINT(x) \
+   printk(KERN_ALERT "[%s] WARNING: " x, __FUNCTION__)
+
 #ifdef GBE_DEBUG
 
 extern bool print_desc;
@@ -1388,4 +1405,6 @@ do { \
 
 #endif //GBE_DEBUG
 
+uint32_t gbe_config_to_speed(uint32_t config);
+
 #endif
--- a/drivers/net/ethernet/synopsys/DWC_ETH_QOS_yregacc.h
+++ b/drivers/net/ethernet/synopsys/DWC_ETH_QOS_yregacc.h
@@ -1577,7 +1577,7 @@ do {\
 #define DMA_IER_RBUE_OFF            7
 #define DMA_IER_RIE_OFF             6
 #define DMA_IER_TBUE_OFF            2
-#define DMA_IER_TXSE_OFF            1
+#define DMA_IER_TSE_OFF             1
 #define DMA_IER_TIE_OFF             0
 
 /* DMA Receive Interrupt Watchdog Timer */
