From 67942f544b73f7d49add6802cfc419caf03b3edc Mon Sep 17 00:00:00 2001
From: Andrei Danaila <andrei.danaila@intel.com>
Date: Thu, 2 Jun 2016 17:02:50 -0700
Subject: [PATCH 408/441] net: Puma6 Puma7 Merge UDMA & e1000 changes

Ported Puma6 UDMA and e1000 changes to Puma7.

Change-Id: I6491e612758621798d6d7cdabf486b7d99373f07
Signed-off-by: Andrei Danaila <andrei.danaila@intel.com>
---
 drivers/net/Kconfig                           |   27 +
 drivers/net/Makefile                          |    1 +
 drivers/net/ethernet/intel/e1000/e1000_hw.c   |  291 ++++-
 drivers/net/ethernet/intel/e1000/e1000_hw.h   |   55 +
 drivers/net/ethernet/intel/e1000/e1000_main.c |  126 +-
 drivers/net/udma/Makefile                     |   23 +
 drivers/net/udma/udma_hw.c                    |  612 +++++++++
 drivers/net/udma/udma_hw.h                    |  438 +++++++
 drivers/net/udma/udma_main.c                  | 1670 +++++++++++++++++++++++++
 drivers/net/udma/udma_main.h                  |  206 +++
 drivers/net/udma/udma_netdev.c                |  259 ++++
 drivers/net/udma/udma_skb.c                   |  127 ++
 12 files changed, 3832 insertions(+), 3 deletions(-)
 create mode 100644 drivers/net/udma/Makefile
 create mode 100644 drivers/net/udma/udma_hw.c
 create mode 100644 drivers/net/udma/udma_hw.h
 create mode 100644 drivers/net/udma/udma_main.c
 create mode 100644 drivers/net/udma/udma_main.h
 create mode 100644 drivers/net/udma/udma_netdev.c
 create mode 100644 drivers/net/udma/udma_skb.c

--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -295,6 +295,33 @@ config NET_SB1000
 
 	  If you don't have this card, of course say N.
 
+menuconfig UDMA
+	tristate "Intel(R) UDMA Driver support"
+        depends on ARCH_GEN3
+	default y
+	---help---
+	  For general information and support, go to the Intel support
+	  website at:
+
+	  <http://support.intel.com>
+
+	  To compile this driver as a module, choose M here. The module
+	  will be called udma.ko
+
+if UDMA
+config UDMA_RATE_LIMITATION
+	bool "UDMA rate limitation support"
+	default n
+	help
+        Add UDMA rate limitation support
+
+config  UDMA_DEBUG
+    	bool "udma debug tool support"
+	default y
+       help
+       Add UDMA debug tool support
+endif
+
 source "drivers/net/phy/Kconfig"
 
 source "drivers/net/plip/Kconfig"
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -23,6 +23,7 @@ obj-$(CONFIG_VETH) += veth.o
 obj-$(CONFIG_VIRTIO_NET) += virtio_net.o
 obj-$(CONFIG_VXLAN) += vxlan.o
 obj-$(CONFIG_NLMON) += nlmon.o
+obj-$(CONFIG_UDMA) += udma/
 
 #
 # Networking Drivers
--- a/drivers/net/ethernet/intel/e1000/e1000_hw.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_hw.c
@@ -137,10 +137,26 @@ static s32 e1000_set_phy_type(struct e10
 		break;
 	case RTL8211B_PHY_ID:
 		hw->phy_type = e1000_phy_8211;
+#ifdef CONFIG_ARCH_GEN3
+		if (hw->phy_revision == RTL8211D_PHY_REV_ID)
+        	hw->phy_type = e1000_phy_8211d;
+		else if (hw->phy_revision == RTL8211E_PHY_REV_ID)
+			hw->phy_type = e1000_phy_8211e;
+#endif
 		break;
 	case RTL8201N_PHY_ID:
 		hw->phy_type = e1000_phy_8201;
 		break;
+#ifdef CONFIG_ARCH_GEN3
+	case RTL8201E_PHY_ID:
+		hw->phy_type = e1000_phy_8201e;
+		if (hw->phy_revision == RTL8201FR_PHY_REV_ID)
+			hw->phy_type = e1000_phy_8201fr;
+		break;
+	case LAN8720A_PHY_ID:
+		hw->phy_type = e1000_phy_lan8720a;
+		break;
+#endif
 	default:
 		/* Should never have loaded on this device */
 		hw->phy_type = e1000_phy_undefined;
@@ -400,6 +416,23 @@ void e1000_set_media_type(struct e1000_h
 	}
 }
 
+#ifdef CONFIG_ARCH_GEN3
+static DEFINE_SPINLOCK(gbe_cru_lock);
+unsigned long cru_irqlocal;
+
+void gbe_cru_lock_acquire(unsigned long *irqlocal)
+{
+	spin_lock_irqsave(&gbe_cru_lock, *irqlocal);
+}
+EXPORT_SYMBOL(gbe_cru_lock_acquire);
+
+void gbe_cru_lock_release(unsigned long *irqlocal)
+{
+	spin_unlock_irqrestore(&gbe_cru_lock, *irqlocal);
+}
+EXPORT_SYMBOL(gbe_cru_lock_release);
+#endif
+
 /**
  * e1000_reset_hw - reset the hardware completely
  * @hw: Struct containing variables accessed by shared code
@@ -476,7 +509,16 @@ s32 e1000_reset_hw(struct e1000_hw *hw)
 		/* Reset is performed on a shadow of the control register */
 		ew32(CTRL_DUP, (ctrl | E1000_CTRL_RST));
 		break;
+#ifdef CONFIG_ARCH_GEN3
 	case e1000_ce4100:
+		/* Reset should synchronize with AVSync register accessed */
+		gbe_cru_lock_acquire(&cru_irqlocal);
+		ndelay(1000);
+		ew32(CTRL, (er32(CTRL) | E1000_CTRL_RST));
+		ndelay(1000);
+		gbe_cru_lock_release(&cru_irqlocal);
+		break;
+#endif
 	default:
 		ew32(CTRL, (ctrl | E1000_CTRL_RST));
 		break;
@@ -1002,6 +1044,9 @@ static s32 gbe_dhg_phy_setup(struct e100
 {
 	s32 ret_val;
 	u32 ctrl_aux;
+#ifdef CONFIG_ARCH_GEN3
+	u16 phy_data;
+#endif
 
 	switch (hw->phy_type) {
 	case e1000_phy_8211:
@@ -1010,7 +1055,34 @@ static s32 gbe_dhg_phy_setup(struct e100
 			e_dbg("e1000_copper_link_rtl_setup failed!\n");
 			return ret_val;
 		}
+#ifdef CONFIG_ARCH_GEN3
+		switch (hw->phy_mode) {
+			case FAKE_PHY_INTERNAL:
+				ctrl_aux = er32(CTL_AUX);
+				ctrl_aux |= 0x2;
+				ew32(CTL_AUX, ctrl_aux);
+				break;
+			case FAKE_PHY_EXTERNAL:
+				/* e1000_write_phy_reg_ex(hw, 0x1, 0xC003);
+				   e1000_write_phy_reg_ex(hw, 0x0, 0x96A1); // the external switch is configured in CEFDK
+				*/
+				break;
+			case REAL_PHY:
+			default:
+				break;
+		}
+#endif
+		break;
+#ifdef CONFIG_ARCH_GEN3
+	case e1000_phy_8211d:
+	case e1000_phy_8211e:
+		ret_val = e1000_copper_link_rtl_setup(hw);
+		if(ret_val) {
+			printk(" e1000_copper_link_rtl_setup failed!\n");
+			return ret_val;
+		}
 		break;
+#endif
 	case e1000_phy_8201:
 		/* Set RMII mode */
 		ctrl_aux = er32(CTL_AUX);
@@ -1030,7 +1102,89 @@ static s32 gbe_dhg_phy_setup(struct e100
 			e_dbg("e1000_copper_link_rtl_setup failed!\n");
 			return ret_val;
 		}
+#ifdef CONFIG_ARCH_GEN3
+		/* RMII mode setting in 8201N PHY chip */
+		ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &phy_data);
+		if (ret_val)
+			return ret_val;
+
+		phy_data |= 0x01;
+		ret_val = e1000_write_phy_reg(hw, PHY_CTRL, phy_data);
+		if (ret_val){
+			printk(" 8201N RMII mode setup failed!\n");
+		}
+#endif
+		break;
+
+#ifdef CONFIG_ARCH_GEN3
+	case e1000_phy_8201e:
+		/* Set RMII mode */
+		ctrl_aux = er32(CTL_AUX);
+		ctrl_aux |= E1000_CTL_AUX_RMII;
+		ew32(CTL_AUX, ctrl_aux);
+		E1000_WRITE_FLUSH();
+		/* Disable the J/K bits requried for recieve */
+		ctrl_aux = er32(CTL_AUX);
+		ctrl_aux |= 0x4;
+		ctrl_aux &= ~0x2;
+		ew32(CTL_AUX, ctrl_aux);
+		E1000_WRITE_FLUSH();
+		ret_val = e1000_copper_link_rtl_setup(hw);
+		if(ret_val){
+			printk(" e1000_copper_link_rtl_setup failed!\n");
+			return ret_val;
+		}
+		/* RMII mode setting in 8201E PHY chip */
+		ret_val = e1000_read_phy_reg(hw, PHY_TEST_REG, &phy_data);
+		if (ret_val)
+		return ret_val;
+
+		phy_data |= RMII_MODE_SET;
+		ret_val = e1000_write_phy_reg(hw, PHY_TEST_REG, phy_data);
+		if (ret_val){
+			printk(" 8201E RMII mode setup failed!\n");
+			return ret_val;
+		}
+		break;
+
+	case e1000_phy_8201fr:
+		/* Set RMII mode */
+		ctrl_aux = er32(CTL_AUX);
+		ctrl_aux |= E1000_CTL_AUX_RMII;
+		ew32(CTL_AUX, ctrl_aux);
+		E1000_WRITE_FLUSH();
+		/* enable the J/K bits requried for recieve */
+		ctrl_aux = er32(CTL_AUX);
+		ctrl_aux &= ~0x4;
+		ctrl_aux &= ~0x2;
+		ew32(CTL_AUX, ctrl_aux);
+		E1000_WRITE_FLUSH();
+		ret_val = e1000_copper_link_rtl_setup(hw);
+		if(ret_val){
+			printk(" e1000_copper_link_rtl_setup failed!\n");
+			return ret_val;
+		}
+		break;
+	case e1000_phy_lan8720a:
+		/* Set RMII mode */
+		ctrl_aux = er32(CTL_AUX);
+		ctrl_aux |= E1000_CTL_AUX_RMII;
+		ew32(CTL_AUX, ctrl_aux);
+		E1000_WRITE_FLUSH();
+		/* enable RX/TX the J/K bits requried for recieve */
+		ctrl_aux = er32(CTL_AUX);
+		ctrl_aux |= 0x8;
+		ctrl_aux &= ~0x4;
+		ctrl_aux &= ~0x2;
+		ew32(CTL_AUX, ctrl_aux);
+		E1000_WRITE_FLUSH();
+		ret_val = e1000_copper_link_rtl_setup(hw);
+		if(ret_val){
+			printk(" e1000_copper_link_rtl_setup failed!\n");
+			return ret_val;
+		}
 		break;
+#endif
 	default:
 		e_dbg("Error Resetting the PHY\n");
 		return E1000_ERR_PHY_TYPE;
@@ -1376,8 +1530,13 @@ static s32 e1000_copper_link_autoneg(str
 		hw->autoneg_advertised = AUTONEG_ADVERTISE_SPEED_DEFAULT;
 
 	/* IFE/RTL8201N PHY only supports 10/100 */
+#ifdef CONFIG_ARCH_GEN3
+	if ((hw->phy_type == e1000_phy_8201) || (hw->phy_type == e1000_phy_8201e) || \
+		(hw->phy_type == e1000_phy_8201fr) || (hw->phy_type == e1000_phy_lan8720a))
+#else
 	if (hw->phy_type == e1000_phy_8201)
-		hw->autoneg_advertised &= AUTONEG_ADVERTISE_10_100_ALL;
+#endif
+        hw->autoneg_advertised &= AUTONEG_ADVERTISE_10_100_ALL;
 
 	e_dbg("Reconfiguring auto-neg advertisement params\n");
 	ret_val = e1000_phy_setup_autoneg(hw);
@@ -1526,6 +1685,10 @@ static s32 e1000_setup_copper_link(struc
 		if (ret_val)
 			return ret_val;
 
+#ifdef CONFIG_ARCH_GEN3
+		hw->cegbe_is_link_up = (phy_data & MII_SR_LINK_STATUS) != 0;
+#endif
+
 		if (phy_data & MII_SR_LINK_STATUS) {
 			/* Config the MAC and PHY after link is up */
 			ret_val = e1000_copper_link_postconfig(hw);
@@ -1562,11 +1725,22 @@ s32 e1000_phy_setup_autoneg(struct e1000
 		return ret_val;
 
 	/* Read the MII 1000Base-T Control Register (Address 9). */
+#ifdef CONFIG_ARCH_GEN3
+	if ((hw->phy_type == e1000_phy_8201) || (hw->phy_type == e1000_phy_8201e) || \
+		(hw->phy_type == e1000_phy_8201fr) || (hw->phy_type == e1000_phy_lan8720a)) {
+		mii_1000t_ctrl_reg = 0;
+	} else {
+		ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &mii_1000t_ctrl_reg);
+		if (ret_val)
+			return ret_val;
+	}
+#else
 	ret_val = e1000_read_phy_reg(hw, PHY_1000T_CTRL, &mii_1000t_ctrl_reg);
 	if (ret_val)
 		return ret_val;
 	else if (hw->phy_type == e1000_phy_8201)
 		mii_1000t_ctrl_reg &= ~REG9_SPEED_MASK;
+#endif
 
 	/* Need to parse both autoneg_advertised and fc and set up
 	 * the appropriate PHY registers.  First we will parse for
@@ -1680,7 +1854,12 @@ s32 e1000_phy_setup_autoneg(struct e1000
 
 	e_dbg("Auto-Neg Advertising %x\n", mii_autoneg_adv_reg);
 
+#ifdef CONFIG_ARCH_GEN3
+	if ((hw->phy_type == e1000_phy_8201) || (hw->phy_type == e1000_phy_8201e)|| \
+		(hw->phy_type == e1000_phy_8201fr) || (hw->phy_type == e1000_phy_lan8720a)) {
+#else
 	if (hw->phy_type == e1000_phy_8201) {
+#endif
 		mii_1000t_ctrl_reg = 0;
 	} else {
 		ret_val = e1000_write_phy_reg(hw, PHY_1000T_CTRL,
@@ -2003,6 +2182,24 @@ static s32 e1000_config_mac_to_phy(struc
 
 		e1000_config_collision_dist(hw);
 		break;
+#ifdef CONFIG_ARCH_GEN3
+	case e1000_phy_8201e:
+	case e1000_phy_8201fr:
+	case e1000_phy_lan8720a:
+			   ret_val = e1000_read_phy_reg(hw, PHY_CTRL, &phy_data);
+			   if (ret_val)
+				   return ret_val;
+			   if (phy_data & RTL_PHY_CTRL_FD)
+				   ctrl |= E1000_CTRL_FD;
+			   else
+				   ctrl &= ~E1000_CTRL_FD;
+			   if (phy_data & RTL_PHY_CTRL_SPD_100)
+				   ctrl |= E1000_CTRL_SPD_100;
+			   else
+				   ctrl |= E1000_CTRL_SPD_10;
+			   e1000_config_collision_dist(hw);
+			   break;
+#endif
 	default:
 		/* Set up duplex in the Device Control and Transmit Control
 		 * registers depending on negotiated values.
@@ -2490,6 +2687,10 @@ s32 e1000_check_for_link(struct e1000_hw
 		if (ret_val)
 			return ret_val;
 
+#ifdef CONFIG_ARCH_GEN3
+		hw->cegbe_is_link_up = (phy_data & MII_SR_LINK_STATUS) != 0;
+#endif
+
 		if (phy_data & MII_SR_LINK_STATUS) {
 			hw->get_link_status = false;
 			/* Check if there was DownShift, must be checked
@@ -2852,6 +3053,54 @@ static u16 e1000_shift_in_mdi_bits(struc
 	return data;
 }
 
+#ifdef CONFIG_ARCH_GEN3
+static s32 e1000_read_phy_reg_fake(struct e1000_hw *hw, u32 reg_addr, u16 *phy_data)
+{
+	switch (reg_addr) {
+	case 0x00:
+		*phy_data = 0x3100;
+		break;
+	case 0x01:
+		*phy_data = 0x796d;
+		break;
+	case 0x02:
+		*phy_data = 0x001C;
+		break;
+	case 0x03:
+		*phy_data = 0xC912;
+		break;
+	case 0x04:
+		*phy_data = 0x05E1;
+		break;
+	case 0x05:
+		*phy_data = 0x01E0;
+		break;
+	case 0x06:
+		*phy_data = 0x0004;
+		break;
+	case 0x07:
+		*phy_data = 0x2001;
+		break;
+	case 0x09:
+		*phy_data = 0x0300;
+		break;
+	case 0x0A:
+		*phy_data = 0x3C00;
+		break;
+	case 0x0F:
+		*phy_data = 0x3000;
+		break;
+	case 0x11:
+		*phy_data = 0xAC00;
+		break;
+	default:
+		*phy_data = 0x0000;
+		break;
+	}
+
+	return E1000_SUCCESS;
+}
+#endif
 
 /**
  * e1000_read_phy_reg - read a phy register
@@ -2870,6 +3119,13 @@ s32 e1000_read_phy_reg(struct e1000_hw *
 
 	spin_lock_irqsave(&e1000_phy_lock, flags);
 
+#ifdef CONFIG_ARCH_GEN3
+	if(hw->phy_mode != REAL_PHY) {
+	        spin_unlock_irqrestore(&e1000_phy_lock, flags);
+            return e1000_read_phy_reg_fake(hw, reg_addr, phy_data);
+	}
+#endif
+
 	if ((hw->phy_type == e1000_phy_igp) &&
 	    (reg_addr > MAX_PHY_MULTI_PAGE_REG)) {
 		ret_val = e1000_write_phy_reg_ex(hw, IGP01E1000_PHY_PAGE_SELECT,
@@ -2994,6 +3250,13 @@ static s32 e1000_read_phy_reg_ex(struct
 	return E1000_SUCCESS;
 }
 
+#ifdef CONFIG_ARCH_GEN3
+static s32 e1000_write_phy_reg_fake(struct e1000_hw *hw, u32 reg_addr, u16 phy_data)
+{
+	return E1000_SUCCESS;
+}
+#endif
+
 /**
  * e1000_write_phy_reg - write a phy register
  *
@@ -3012,6 +3275,13 @@ s32 e1000_write_phy_reg(struct e1000_hw
 
 	spin_lock_irqsave(&e1000_phy_lock, flags);
 
+#ifdef CONFIG_ARCH_GEN3
+	if(hw->phy_mode != REAL_PHY) {
+	        spin_unlock_irqrestore(&e1000_phy_lock, flags);
+            return e1000_write_phy_reg_fake(hw, reg_addr, phy_data);
+	}
+#endif
+
 	if ((hw->phy_type == e1000_phy_igp) &&
 	    (reg_addr > MAX_PHY_MULTI_PAGE_REG)) {
 		ret_val = e1000_write_phy_reg_ex(hw, IGP01E1000_PHY_PAGE_SELECT,
@@ -3266,9 +3536,18 @@ static s32 e1000_detect_gig_phy(struct e
 			match = true;
 		break;
 	case e1000_ce4100:
+#ifdef CONFIG_ARCH_GEN3
+		if ((hw->phy_id == RTL8211B_PHY_ID) ||
+		    (hw->phy_id == RTL8201N_PHY_ID) ||
+		    (hw->phy_id == RTL8201E_PHY_ID) ||
+		    (hw->phy_id == LAN8720A_PHY_ID) ||
+		    (hw->phy_id == RTL8201FR_PHY_ID) ||
+		    (hw->phy_id == M88E1118_E_PHY_ID))
+#else
 		if ((hw->phy_id == RTL8211B_PHY_ID) ||
 		    (hw->phy_id == RTL8201N_PHY_ID) ||
 		    (hw->phy_id == M88E1118_E_PHY_ID))
+#endif
 			match = true;
 		break;
 	case e1000_82541:
@@ -3518,8 +3797,18 @@ s32 e1000_phy_get_info(struct e1000_hw *
 
 	if (hw->phy_type == e1000_phy_igp)
 		return e1000_phy_igp_get_info(hw, phy_info);
+#ifdef CONFIG_ARCH_GEN3
+	else if ((hw->phy_type == e1000_phy_8211) ||
+			(hw->phy_type == e1000_phy_8201e) ||
+			(hw->phy_type == e1000_phy_lan8720a) ||
+			(hw->phy_type == e1000_phy_8201fr) ||
+			(hw->phy_type == e1000_phy_8211d) ||
+			(hw->phy_type == e1000_phy_8211e) ||
+	        (hw->phy_type == e1000_phy_8201))
+#else
 	else if ((hw->phy_type == e1000_phy_8211) ||
 	         (hw->phy_type == e1000_phy_8201))
+#endif
 		return E1000_SUCCESS;
 	else
 		return e1000_phy_m88_get_info(hw, phy_info);
--- a/drivers/net/ethernet/intel/e1000/e1000_hw.h
+++ b/drivers/net/ethernet/intel/e1000/e1000_hw.h
@@ -214,6 +214,13 @@ typedef enum {
 	e1000_phy_igp,
 	e1000_phy_8211,
 	e1000_phy_8201,
+#ifdef CONFIG_ARCH_GEN3
+	e1000_phy_8201e,
+	e1000_phy_8211d,
+	e1000_phy_8211e,
+    e1000_phy_8201fr,
+	e1000_phy_lan8720a,
+#endif
 	e1000_phy_undefined = 0xFF
 } e1000_phy_type;
 
@@ -1348,6 +1355,16 @@ struct e1000_hw_stats {
 	u64 icrxoc;
 };
 
+#ifdef CONFIG_ARCH_GEN3
+enum phy_mode {
+	REAL_PHY = 0,
+	FAKE_PHY_INTERNAL,
+	FAKE_PHY_EXTERNAL,
+	INVALID_PHY,
+};
+#define MARVAL_PHY_ADDRESS 0x4
+#endif
+
 /* Structure containing variables used by the shared code (e1000_hw.c) */
 struct e1000_hw {
 	u8 __iomem *hw_addr;
@@ -1375,6 +1392,9 @@ struct e1000_hw {
 	u32 phy_id;
 	u32 phy_revision;
 	u32 phy_addr;
+#ifdef CONFIG_ARCH_GEN3
+	enum phy_mode phy_mode;
+#endif
 	u32 original_fc;
 	u32 txcw;
 	u32 autoneg_failed;
@@ -1433,6 +1453,9 @@ struct e1000_hw {
 	bool leave_av_bit_off;
 	bool bad_tx_carr_stats_fd;
 	bool has_smbus;
+#ifdef CONFIG_ARCH_GEN3
+	bool cegbe_is_link_up;
+#endif
 };
 
 #define E1000_EEPROM_SWDPIN0   0x0001	/* SWDPIN 0 EEPROM Value */
@@ -2252,6 +2275,10 @@ struct e1000_host_command_info {
 #define EEPROM_FLASH_VERSION          0x0032
 #define EEPROM_CHECKSUM_REG           0x003F
 
+#ifdef CONFIG_ARCH_GEN3
+#define EEPROM_CE4100_FAKE_LENGTH     0x80
+#endif
+
 #define E1000_EEPROM_CFG_DONE         0x00040000	/* MNG config cycle done */
 #define E1000_EEPROM_CFG_DONE_PORT_1  0x00080000	/* ...for second port */
 
@@ -2517,6 +2544,9 @@ struct e1000_host_command_info {
 #define PHY_1000T_CTRL   0x09	/* 1000Base-T Control Reg */
 #define PHY_1000T_STATUS 0x0A	/* 1000Base-T Status Reg */
 #define PHY_EXT_STATUS   0x0F	/* Extended Status Reg */
+#ifdef CONFIG_ARCH_GEN3
+#define PHY_TEST_REG     0x19 /* Test Register */
+#endif
 
 #define MAX_PHY_REG_ADDRESS        0x1F	/* 5 bit address bus (0-0x1F) */
 #define MAX_PHY_MULTI_PAGE_REG     0xF	/* Registers equal on all pages */
@@ -2601,6 +2631,9 @@ struct e1000_host_command_info {
 #define MII_CR_SPEED_SELECT_LSB 0x2000	/* bits 6,13: 10=1000, 01=100, 00=10 */
 #define MII_CR_LOOPBACK         0x4000	/* 0 = normal, 1 = loopback */
 #define MII_CR_RESET            0x8000	/* 0 = normal, 1 = PHY reset */
+#ifdef CONFIG_ARCH_GEN3
+#define RMII_MODE_SET           0x0200  /* 0 = MII Mode, 1 = RMII Mode */
+#endif
 
 /* PHY Status Register */
 #define MII_SR_EXTENDED_CAPS     0x0001	/* Extended register capabilities */
@@ -2930,9 +2963,31 @@ struct e1000_host_command_info {
 #define L1LXT971A_PHY_ID   0x001378E0
 
 #define RTL8211B_PHY_ID    0x001CC910
+
+#ifdef CONFIG_ARCH_GEN3
+#define RTL8211B_PHY_REV_ID     0b0010
+#endif
 #define RTL8201N_PHY_ID    0x8200
 #define RTL_PHY_CTRL_FD    0x0100 /* Full duplex.0=half; 1=full */
+
+#ifdef CONFIG_ARCH_GEN3
+#define RTL_PHY_CTRL_SPD_100    0x2000 /* Force 100Mb */
+#else
 #define RTL_PHY_CTRL_SPD_100    0x200000 /* Force 100Mb */
+#endif
+
+#ifdef CONFIG_ARCH_GEN3
+#define RTL8201E_PHY_ID     0x001CC810
+#define RTL8211D_PHY_ID     0x001CC910//It's the same as RTL8211B
+#define RTL8211D_PHY_REV_ID     0b0100
+#define RTL8211E_PHY_ID     0x001CC910//It's the same as RTL8211B
+#define RTL8211E_PHY_REV_ID     0b0101
+
+#define RTL8201FR_PHY_ID     0x001CC810
+#define RTL8201FR_PHY_REV_ID    0b0110
+
+#define LAN8720A_PHY_ID		 0x0007C0F0
+#endif
 
 /* Bits...
  * 15-5: page
--- a/drivers/net/ethernet/intel/e1000/e1000_main.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_main.c
@@ -33,6 +33,15 @@
 #include <linux/bitops.h>
 #include <linux/if_vlan.h>
 
+#ifdef CONFIG_ARCH_GEN3
+static const char *phy_mode_name[] = {
+    "Real Phy Mode",
+    "Internal Fake Phy Mode",
+    "External Fake Phy Mode",
+    "Invalid Phy Mode",
+};
+#endif
+
 char e1000_driver_name[] = "e1000";
 static char e1000_driver_string[] = "Intel(R) PRO/1000 Network Driver";
 #define DRV_VERSION "7.3.21-k8-NAPI"
@@ -744,12 +753,20 @@ static void e1000_dump_eeprom(struct e10
 	int i;
 	u16 csum_old, csum_new = 0;
 
-	eeprom.len = ops->get_eeprom_len(netdev);
+#ifdef CONFIG_ARCH_GEN3
+	if (adapter->hw.mac_type == e1000_ce4100)
+		eeprom.len = EEPROM_CE4100_FAKE_LENGTH;
+	else
+#endif
+		eeprom.len = ops->get_eeprom_len(netdev);
+
 	eeprom.offset = 0;
 
 	data = kmalloc(eeprom.len, GFP_KERNEL);
-	if (!data)
+	if (!data) {
+		pr_err("Unable to allocate memory to dump EEPROM data\n");
 		return;
+	}
 
 	ops->get_eeprom(netdev, &eeprom, data);
 
@@ -875,6 +892,57 @@ static const struct net_device_ops e1000
 	.ndo_set_features	= e1000_set_features,
 };
 
+#ifdef CONFIG_ARCH_GEN3
+static enum phy_mode g_phy_mode = INVALID_PHY;
+
+int set_gmac_phy_mode(enum phy_mode mode)
+{
+	g_phy_mode = mode;
+	return 0;
+}
+
+static enum phy_mode  detect_phy_mode(void)
+{
+	u32 phy_base; /* L2 switch bar 0 */
+	u8 __iomem *virt_base;
+	u32 reg_val; /* slave config register */
+	struct pci_dev *pdev;
+	enum phy_mode ret = REAL_PHY;
+
+	pdev = pci_get_device(0x8086, 0x08BD, NULL);
+	if(!pdev)
+		return ret;
+
+	pci_read_config_dword(pdev, 0x10, &phy_base);
+	pci_dev_put(pdev);
+
+	virt_base = ioremap_nocache(phy_base, 256 * 1024);
+	if(!virt_base)
+		return ret;
+
+	reg_val = readl(virt_base + 0x3A004);
+	iounmap(virt_base);
+
+	reg_val = (reg_val & 0b1011);
+
+    switch (reg_val) {
+		case 0b0000:
+		case 0b1000:
+	        ret = FAKE_PHY_EXTERNAL;
+			printk("GMUX setting: GMAC0 is connected to external switch (RGMII0)\n");
+			break;
+	    case 0b1011:
+			ret = FAKE_PHY_INTERNAL;
+			printk("GMUX setting: GMAC0 is connected to internal switch (RGMII0)\n");
+			break;
+		case 0b1001:
+		case 0b1010:
+		default:
+			break;
+	}
+	return ret;
+}
+#endif
 /**
  * e1000_init_hw_struct - initialize members of hw struct
  * @adapter: board private struct
@@ -890,6 +958,9 @@ static int e1000_init_hw_struct(struct e
 				struct e1000_hw *hw)
 {
 	struct pci_dev *pdev = adapter->pdev;
+#ifdef CONFIG_ARCH_GEN3
+	uint32_t socid;
+#endif
 
 	/* PCI config space info */
 	hw->vendor_id = pdev->vendor;
@@ -897,6 +968,19 @@ static int e1000_init_hw_struct(struct e
 	hw->subsystem_vendor_id = pdev->subsystem_vendor;
 	hw->subsystem_id = pdev->subsystem_device;
 	hw->revision_id = pdev->revision;
+#ifdef CONFIG_ARCH_GEN3
+	if (INVALID_PHY == g_phy_mode) {
+		intelce_get_soc_info(&socid, NULL);
+		if (CE2600_SOC_DEVICE_ID == socid) {
+			hw->phy_mode = detect_phy_mode();
+		} else {
+			hw->phy_mode = REAL_PHY;
+		}
+	} else {
+		hw->phy_mode = g_phy_mode;
+	}
+	printk("GBE working in %s\n", phy_mode_name[hw->phy_mode & 0x3]);
+#endif
 
 	pci_read_config_word(pdev, PCI_COMMAND, &hw->pci_cmd_word);
 
@@ -1726,6 +1810,8 @@ static int e1000_setup_rx_resources(stru
 	rxdr->desc = dma_alloc_coherent(&pdev->dev, rxdr->size, &rxdr->dma,
 					GFP_KERNEL);
 	if (!rxdr->desc) {
+		e_err(probe, "Unable to allocate memory for the Rx descriptor "
+		      "ring\n");
 setup_rx_desc_die:
 		vfree(rxdr->buffer_info);
 		return -ENOMEM;
@@ -1744,6 +1830,8 @@ setup_rx_desc_die:
 		if (!rxdr->desc) {
 			dma_free_coherent(&pdev->dev, rxdr->size, olddesc,
 					  olddma);
+			e_err(probe, "Unable to allocate memory for the Rx "
+			      "descriptor ring\n");
 			goto setup_rx_desc_die;
 		}
 
@@ -2443,14 +2531,48 @@ static void e1000_watchdog(struct work_s
 	struct e1000_tx_ring *txdr = adapter->tx_ring;
 	u32 link, tctl;
 
+#ifdef CONFIG_ARCH_GEN3
+	u16 link_up;
+	s32 ret_val;
+#endif
+
 	if (test_bit(__E1000_DOWN, &adapter->flags))
 		return;
 
+#ifdef CONFIG_ARCH_GEN3
+	/*
+	 * Test the PHY for link status on Intel CE SoC MAC.
+	 * If the link status is different than the last link status stored
+	 * in the adapter->hw structure, then set hw->get_link_status = 1
+	 */
+	ret_val = e1000_read_phy_reg(&adapter->hw, PHY_STATUS, &link_up);
+	ret_val = e1000_read_phy_reg(&adapter->hw, PHY_STATUS, &link_up);
+	if (ret_val)
+		pr_info("Link status detection from PHY failed!\n");
+
+	link_up = ((link_up & MII_SR_LINK_STATUS) != 0);
+	if(link_up != adapter->hw.cegbe_is_link_up)
+		adapter->hw.get_link_status = true;
+	else
+		adapter->hw.get_link_status = false;
+#endif
+
 	mutex_lock(&adapter->mutex);
 	link = e1000_has_link(adapter);
 	if ((netif_carrier_ok(netdev)) && link)
 		goto link_up;
 
+#ifdef CONFIG_ARCH_GEN3
+	if (hw->mac_type == e1000_ce4100) {
+        //FIXME - why are there two calls here?`
+		ret_val = e1000_read_phy_reg(&adapter->hw, PHY_STATUS, &link_up);
+		ret_val = e1000_read_phy_reg(&adapter->hw, PHY_STATUS, &link_up);
+		if (ret_val)
+			pr_info("Link status detection from PHY failed!\n");
+		link = ((link_up & MII_SR_LINK_STATUS) != 0);
+	}
+#endif
+
 	if (link) {
 		if (!netif_carrier_ok(netdev)) {
 			u32 ctrl;
--- /dev/null
+++ b/drivers/net/udma/Makefile
@@ -0,0 +1,23 @@
+#
+# Copyright (c) 2012, Intel Corporation and its suppliers.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms and conditions of the GNU General Public License,
+# version 2, as published by the Free Software Foundation.
+#
+# This program is distributed in the hope it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+# more details.
+#
+# You should have received a copy of the GNU General Public License along with
+# this program; if not, write to the Free Software Foundation, Inc.,
+# 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+#
+#
+
+
+obj-$(CONFIG_UDMA) += udma.o
+obj-$(CONFIG_UDMA) += udma_netdev.o
+
+udma-objs := udma_main.o udma_hw.o udma_skb.o
--- /dev/null
+++ b/drivers/net/udma/udma_hw.c
@@ -0,0 +1,612 @@
+/*
+ *
+ *  GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2012-2015 Intel Corporation. All rights reserved.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *    Intel Corporation
+ *    2200 Mission College Blvd.
+ *    Santa Clara, CA  97052
+ *
+ */
+
+/* UDMA low level driver  */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/device.h>
+#include <asm/uaccess.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/pci.h>
+
+#include "udma_hw.h"
+
+#define SWITCH_RX_TX(direction,r,t) ((direction == UDMA_RX)?(r):(t))
+
+#define UDMA_DRV_VERSION "1.0.0"
+char udma_driver_name[] = "udma";
+const char udma_driver_version[] = UDMA_DRV_VERSION;
+
+
+static const struct __cntx_regset udma_cntx_rx_regset = {
+		.cdesc = (UDMA_RX_REG_BASE + UDMA_CURR_DESC),
+		.ndesc = (UDMA_RX_REG_BASE + UDMA_NEXT_DESC),
+		.sdma  = (UDMA_RX_REG_BASE + UDMA_SRCDMA_START),
+		.ddma  = (UDMA_RX_REG_BASE + UDMA_DSTDMA_START),
+		.size  = (UDMA_RX_REG_BASE + UDMA_SRCDMA_SIZE),
+		.flag  = UDMA_RX_REG_BASE + UDMA_FLAGS_MODE,
+		.other = UDMA_RX_REG_BASE + UDMA_OTHER_MODE,
+};
+
+static const struct __cntx_regset udma_cntx_tx_regset = {
+		.cdesc = UDMA_TX_REG_BASE + UDMA_CURR_DESC,
+		.ndesc = UDMA_TX_REG_BASE + UDMA_NEXT_DESC,
+		.sdma  = UDMA_TX_REG_BASE + UDMA_SRCDMA_START,
+		.ddma  = UDMA_TX_REG_BASE + UDMA_DSTDMA_START,
+		.size  = UDMA_TX_REG_BASE + UDMA_SRCDMA_SIZE,
+		.flag  = UDMA_TX_REG_BASE + UDMA_FLAGS_MODE,
+		.other = UDMA_TX_REG_BASE + UDMA_OTHER_MODE,
+};
+
+static const struct __intr_regset udma_intr_regset = {
+		.int_mask = UDMA_INTR_MASK,
+		.int_stat = UDMA_INTR_STATUS,
+};
+
+
+/* INTR enable register bits */
+static const struct __port_regset udma_port_intr_unmask[] = {
+	{
+		UDMA_DST_INT0_EN, /* RX0, DMA channel 0 DST interrupt enable  when destination buffer is full */
+		UDMA_SRC_INT1_EN, /* TX0, DMA channel 1 SRC interrupt enable when source buffer is empty */
+	}, /* Port 0 */
+	{
+		UDMA_DST_INT2_EN, /* RX1, DMA channel 2 DST interrupt enable when destination buffer is full */
+		UDMA_SRC_INT3_EN, /* TX1, DMA channel 3 SRC interrupt enable when source buffer is empty */
+	} /* Port 1 */
+};
+
+static const struct __intr_status_bitset udma_intr_status[] = {
+	{
+		UDMA_DST_INT0_ACT_BIT, /* RX0, DMA channel 0 DST interrupt status bit offset when destination buffer is full */
+		UDMA_SRC_INT1_ACT_BIT, /* TX0, DMA channel 1 SRC interrupt status bit offset when source buffer is empty */
+	},
+	{
+		UDMA_DST_INT2_ACT_BIT, /* RX1, DMA channel 2 DST interrupt status bit offset when destination buffer is full */
+		UDMA_SRC_INT3_ACT_BIT, /* TX1, DMA channel 3 SRC interrupt status bit offset when source buffer is empty */
+	}
+};
+
+/* DMA address for port 0 & 1 */
+static const struct __port_regset udma_port_dma[] = {
+                {UDMA_CONTEXT0_DMA_ADDR,UDMA_CONTEXT1_DMA_ADDR}, /* Port 0 */
+                {UDMA_CONTEXT2_DMA_ADDR,UDMA_CONTEXT3_DMA_ADDR}  /* Port 1 */
+};
+/* Port 0 & Port 1 shares the same cntx regs */
+static const struct udma_cntx_regset udma_cntx_regs = {
+	.rx 	= 	(struct __cntx_regset *)&udma_cntx_rx_regset,
+	.tx 	= 	(struct __cntx_regset *)&udma_cntx_tx_regset,
+	.intr 	= 	(struct __intr_regset *)&udma_intr_regset,
+};
+
+
+
+
+
+int  udma_setup_sw(void *dev);
+void  udma_free_sw(void *dev);
+struct udma_hw * udma_alloc_hw(size_t size);
+
+static const struct pci_device_id udma_pci_tbl[] = {
+        { PCI_DEVICE( 0x8086, UDMA_DEVICE_ID), .driver_data = 0 },
+        {0},
+};
+
+MODULE_DEVICE_TABLE(pci, udma_pci_tbl);
+
+void udma_regs_dump(struct udma_hw *hw, bool direction)
+{
+	struct __cntx_regset *regset = NULL;
+	regset = SWITCH_RX_TX(direction,udma_cntx_regs.rx,udma_cntx_regs.tx);
+
+	udma_info("	CONTEXT %s registers: ", SWITCH_RX_TX(direction,"Rx","Tx"));
+
+	udma_info("		Current DESC(%x) 	0x%x\n", regset->cdesc, udma_readl(hw,regset->cdesc));
+	udma_info("		Next DESC(%x)	 	0x%x\n", regset->ndesc, udma_readl(hw,regset->ndesc));
+	udma_info("		SRCDMA START(%x) 	0x%x\n", regset->sdma, udma_readl(hw,regset->sdma));
+	udma_info("		DSTDMA START(%x) 	0x%x\n", regset->ddma, udma_readl(hw,regset->ddma));
+	udma_info("		SRCDMA SIZE(%x) 	0x%x\n", regset->size, udma_readl(hw,regset->size));
+	udma_info("		FLAGS MODE(%x) 		0x%x\n", regset->flag, udma_readl(hw,regset->flag));
+	udma_info("		OTHER MODE(%x) 		0x%x\n", regset->other, udma_readl(hw,regset->other));
+
+	udma_info("	INTERRUPT registers: ");
+	udma_info("		INTR MASK(%x) 		0x%x\n", udma_cntx_regs.intr->int_mask, udma_readl(hw,udma_cntx_regs.intr->int_mask));
+	udma_info("		INTR STAT(%x) 		0x%x\n", udma_cntx_regs.intr->int_stat, udma_readl(hw,udma_cntx_regs.intr->int_stat));
+
+
+}
+
+/*
+ * udma_is_active - check whether UDMA context is active or not
+ * @direction: 0 for rx, 1 for tx
+ *
+ * return 1, when it's active;
+ * return 0, when it's stop
+*/
+
+static  bool udma_hw_rx_is_active(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.rx->flag);
+	if (value & UDMA_DMA_IS_ACTIVE)
+		return true;
+
+	value = udma_readl(hw, udma_cntx_regs.rx->size);
+	if ((value>>24) == DESC_HANDLING_FLAG)
+		return true;
+	return false;
+}
+
+static  bool udma_hw_tx_is_active(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.tx->flag);
+	if (value & UDMA_DMA_IS_ACTIVE)
+		return true;
+
+	value = udma_readl(hw, udma_cntx_regs.tx->size);
+	if ((value>>24) == DESC_HANDLING_FLAG)
+		return true;
+	return false;
+}
+
+
+static u32 udma_hw_get_curr_rx_desc(struct udma_hw *hw)
+{
+	return udma_readl(hw, udma_cntx_regs.rx->cdesc);
+}
+
+static  u32 udma_hw_get_curr_tx_desc(struct udma_hw *hw)
+{
+	return udma_readl(hw, udma_cntx_regs.tx->cdesc);
+}
+
+static u32 udma_hw_get_next_rx_desc(struct udma_hw *hw)
+{
+	return udma_readl(hw, udma_cntx_regs.rx->ndesc);
+}
+
+static  u32 udma_hw_get_next_tx_desc(struct udma_hw *hw)
+{
+	return udma_readl(hw, udma_cntx_regs.tx->ndesc);
+}
+
+
+
+/*
+ * udma_hw_irq_mask - mask interrupt
+ * @direction - the transfer direction
+ *    0 means RX, 1 means TX
+*/
+static int udma_hw_disable_rx_irq(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.intr->int_mask);
+	value &= ~udma_port_intr_unmask[hw->port].rx;
+	udma_writel(hw, udma_cntx_regs.intr->int_mask, value);
+	return 0;
+}
+
+static int udma_hw_disable_tx_irq(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.intr->int_mask);
+	value &= ~udma_port_intr_unmask[hw->port].tx;
+	udma_writel(hw, udma_cntx_regs.intr->int_mask, value);
+
+	return 0;
+}
+
+static int udma_hw_enable_rx_irq(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.intr->int_mask);
+	value |= udma_port_intr_unmask[hw->port].rx;
+	udma_writel(hw, udma_cntx_regs.intr->int_mask, value);
+
+	return 0;
+}
+
+static int udma_hw_enable_tx_irq(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.intr->int_mask);
+	value |= udma_port_intr_unmask[hw->port].tx;
+	udma_writel(hw, udma_cntx_regs.intr->int_mask, value);
+
+	return 0;
+}
+
+static int udma_hw_clear_rx_irq(struct udma_hw *hw)
+{
+	udma_writel(hw, udma_cntx_regs.intr->int_stat, udma_intr_status[hw->port].rx);
+	return 0;
+}
+
+static int udma_hw_clear_tx_irq(struct udma_hw *hw)
+{
+	udma_writel(hw, udma_cntx_regs.intr->int_stat, udma_intr_status[hw->port].tx);
+	return 0;
+}
+
+static  u32 udma_hw_get_irq_status(struct udma_hw *hw)
+{
+	 return udma_readl(hw,udma_cntx_regs.intr->int_stat);
+}
+
+static inline int udma_hw_start_rx_transfer(struct udma_hw *hw, u32 desc_dma)
+{
+	udma_writel(hw, udma_cntx_regs.rx->other, UDMA_OTHER_MODE_DEFAULT_SETTING);
+	udma_writel(hw, udma_cntx_regs.rx->ndesc, desc_dma);
+	udma_writel(hw, udma_cntx_regs.rx->size, 0);
+	udma_writel(hw, udma_cntx_regs.rx->flag, UDMA_FLAG_MODE_START_LL);
+	return 0;
+}
+
+static inline int udma_hw_start_tx_transfer(struct udma_hw *hw, u32 desc_dma)
+{
+	udma_writel(hw, udma_cntx_regs.tx->other, UDMA_OTHER_MODE_DEFAULT_SETTING);
+	udma_writel(hw, udma_cntx_regs.tx->ndesc, desc_dma);
+	udma_writel(hw, udma_cntx_regs.tx->size, 0);
+	udma_writel(hw, udma_cntx_regs.tx->flag, UDMA_FLAG_MODE_START_LL);
+	return 0;
+}
+
+static inline int udma_hw_stop_rx_transfer(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.rx->other);
+	udma_writel(hw, udma_cntx_regs.rx->other, UDMA_STOP | value);
+	return 0;
+}
+
+static inline int udma_hw_stop_tx_transfer(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.tx->other);
+	udma_writel(hw, udma_cntx_regs.tx->other, UDMA_STOP | value);
+	return 0;
+}
+
+
+static inline bool udma_hw_rx_is_stopped(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.rx->other);
+	return !(value & UDMA_STOP);
+}
+
+static inline bool udma_hw_tx_is_stopped(struct udma_hw *hw)
+{
+	u32 value;
+
+	value = udma_readl(hw, udma_cntx_regs.tx->other);
+	return !(value & UDMA_STOP);
+}
+
+/* 	udma_hw_ll_desc_en : Enable a descriptor
+ *
+ * Steps:
+ * 1. Update the src_dma & dst_dma field
+ * 2. Update the size field
+ **/
+static void udma_hw_init_rx_desc(struct udma_hw *hw, struct udma_desc *desc)
+{
+	/* rx , enable interrupt at each descriptor */
+	desc->src = udma_port_dma[hw->port].rx;
+}
+
+static void udma_hw_init_tx_desc(struct udma_hw *hw, struct udma_desc *desc)
+{
+	desc->dst = udma_port_dma[hw->port].tx;
+}
+
+static inline u32 udma_hw_desc_tx_burstsz(struct udma_hw *hw)
+{
+	return (u32)(hw->mode.udma_to_l2_burst << 12);
+}
+
+static inline u32 udma_hw_desc_rx_burstsz(struct udma_hw *hw)
+{
+	return (u32)(hw->mode.l2_to_udma_burst << 12);
+}
+
+static inline u32 udma_hw_desc_tx_gapval(struct udma_hw *hw)
+{
+	return (u32)(hw->mode.udma_to_l2_gap << 16);
+}
+
+static inline u32 udma_hw_desc_rx_gapval(struct udma_hw *hw)
+{
+	return (u32)(hw->mode.l2_to_udma_gap << 16);
+}
+
+
+static void udma_hw_update_rx_desc(struct udma_hw *hw, struct udma_desc *desc,
+		u32 dma, u32 len)
+{
+	/* rx , enable interrupt at each descriptor */
+	desc->dst = dma;
+	desc->union_field.size = len & 0x00FFFFFF;
+	desc->flags = udma_hw_desc_rx_burstsz(hw) | udma_hw_desc_rx_gapval(hw)
+		| UDMA_FLAG_MODE_RX_LL | UDMA_PACKET_POSITION_STARTING_ENDING;
+}
+
+static void udma_hw_update_tx_desc(struct udma_hw *hw, struct udma_desc *desc,
+		u32 dma, u32 len)
+{
+	/* tx, do not set the interrupt, only enable at the last descriptor */
+	desc->src = dma;
+	desc->union_field.size = len & 0x00FFFFFF;
+	desc->flags = udma_hw_desc_tx_burstsz(hw) | udma_hw_desc_tx_gapval(hw)
+		| UDMA_FLAG_MODE_TX_LL | UDMA_PACKET_POSITION_STARTING_ENDING;
+}
+
+static inline void udma_hw_clear_rx_term(struct udma_hw *hw)
+{
+    u32 val = udma_readl(hw, udma_cntx_regs.rx->flag);
+    val &= (~UDMA_TERM_EN);
+    udma_writel(hw, udma_cntx_regs.rx->flag, val);
+}
+
+static inline u32 udma_hw_rx_is_term(struct udma_hw *hw)
+{
+	u32 val = udma_readl(hw, udma_cntx_regs.rx->flag);
+	return (val & UDMA_TERM_EN);
+}
+
+static void udma_hw_clear_tx_term(struct udma_hw *hw)
+{
+    u32 val = udma_readl(hw, udma_cntx_regs.tx->flag);
+    val &= (~UDMA_TERM_EN);
+    udma_writel(hw, udma_cntx_regs.tx->flag, val);
+}
+
+static int udma_hw_init(struct udma_hw *hw)
+{
+	udma_hw_disable_tx_irq(hw);
+	udma_hw_disable_rx_irq(hw);
+	udma_writel(hw, udma_cntx_regs.rx->ndesc, 0);
+	udma_writel(hw, udma_cntx_regs.tx->ndesc, 0);
+	udma_hw_clear_tx_irq(hw);
+	udma_hw_clear_rx_irq(hw);
+	/* Set the other mode registers */
+	udma_writel(hw, udma_cntx_regs.tx->other, UDMA_OTHER_MODE_DEFAULT_SETTING);
+	udma_writel(hw, udma_cntx_regs.rx->other, UDMA_OTHER_MODE_DEFAULT_SETTING);
+
+	return 0;
+}
+
+static int udma_hw_exit(struct udma_hw *hw)
+{
+
+	udma_hw_disable_tx_irq(hw);
+	udma_hw_disable_rx_irq(hw);
+
+	udma_hw_clear_tx_irq(hw);
+	udma_hw_clear_rx_irq(hw);
+
+	udma_writel(hw, udma_cntx_regs.rx->ndesc, 0);
+	udma_writel(hw, udma_cntx_regs.tx->ndesc, 0);
+
+	return 0;
+}
+
+/*****************************************************************************
+ *
+ * Initializaton/Exit
+ *
+*****************************************************************************/
+
+
+static struct udma_hw_operations udma_hw_ops = {
+	.rx_is_active				=	udma_hw_rx_is_active,
+	.tx_is_active				=	udma_hw_tx_is_active,
+
+	.get_curr_rx_desc			= 	udma_hw_get_curr_rx_desc,
+	.get_curr_tx_desc			= 	udma_hw_get_curr_tx_desc,
+
+	.get_next_rx_desc			= 	udma_hw_get_next_rx_desc,
+	.get_next_tx_desc			= 	udma_hw_get_next_tx_desc,
+
+	.start_rx_transfer			= 	udma_hw_start_rx_transfer,
+	.start_tx_transfer			= 	udma_hw_start_tx_transfer,
+
+	.stop_rx_transfer			=	udma_hw_stop_rx_transfer,
+	.stop_tx_transfer			=	udma_hw_stop_tx_transfer,
+
+	.rx_is_stopped				=	udma_hw_rx_is_stopped,
+	.tx_is_stopped				=	udma_hw_tx_is_stopped,
+
+	.init_rx_desc				=	udma_hw_init_rx_desc,
+	.init_tx_desc				=	udma_hw_init_tx_desc,
+	.update_rx_desc				=	udma_hw_update_rx_desc,
+	.update_tx_desc				=	udma_hw_update_tx_desc,
+
+	.hw_init					= 	udma_hw_init,
+	.hw_exit					=	udma_hw_exit,
+
+	.get_irq_status				=	udma_hw_get_irq_status,
+
+	.disable_rx_irq				=	udma_hw_disable_rx_irq,
+	.enable_rx_irq				=	udma_hw_enable_rx_irq,
+	.clear_rx_irq				=	udma_hw_clear_rx_irq,
+
+	.disable_tx_irq				=	udma_hw_disable_tx_irq,
+	.enable_tx_irq				=	udma_hw_enable_tx_irq,
+	.clear_tx_irq				=	udma_hw_clear_tx_irq,
+
+	.clear_rx_term				=	udma_hw_clear_rx_term,
+	.rx_is_term				=	udma_hw_rx_is_term,
+	.clear_tx_term				=	udma_hw_clear_tx_term,
+};
+
+/**
+ * udma_probe - Device Initialization Routine
+ * Returns 0 on success, negative on failure
+ *
+ * udma_probe initializes an adapter identified by a pci_dev structure.
+ * The OS initialization, configuring of the device private structure
+ **/
+
+static int udma_hw_probe(struct pci_dev *pdev,
+				     const struct pci_device_id *ent)
+{
+	int ret = -ENODEV;
+	struct udma_hw *hw = NULL;
+
+
+	/* enable device */
+	ret = pci_enable_device(pdev);
+	if (ret) {
+			dev_err(&pdev->dev, "pci_enable_device failed.\n");
+			return ret;
+	}
+
+	hw = udma_alloc_hw(sizeof(struct udma_hw));
+	if (IS_ERR(hw)) {
+			dev_err(&pdev->dev, "Cannot allocate memory\n");
+			ret = -ENOMEM;
+			goto free_resource;
+	}
+	pci_request_region(pdev,0,(pdev->devfn>>3)?UDMA1_NAME:UDMA0_NAME);
+
+	hw->ioaddr = (volatile void __iomem *)pci_ioremap_bar(pdev,0);
+	if (!hw->ioaddr) {
+		udma_err("error, failed to ioremap udma csr space\n");
+		ret = -ENOMEM;
+		goto free_mem;
+	}
+	/* DMA port map */
+	hw->port = pdev->devfn>>3;
+	hw->ops = &udma_hw_ops;
+	hw->pdev = pdev;
+
+	/* udma burst/gap default settings*/
+   hw->mode.udma_to_l2_gap = UDMA_GAP_VAL_0_CLKS;
+   hw->mode.udma_to_l2_burst = UDMA_BURST_SZ_128_BYTES;
+   hw->mode.l2_to_udma_gap = UDMA_GAP_VAL_0_CLKS;
+   hw->mode.l2_to_udma_burst = UDMA_BURST_SZ_128_BYTES;
+
+	pci_set_drvdata(pdev, hw);
+
+	udma_hw_init(hw);
+	ret = udma_setup_sw(udma_hw_priv(hw));
+	if (ret)
+		goto free_iomem;
+
+	printk(KERN_INFO "Intel(R) UDMA Port %d Device Driver Init Done \n",hw->port);
+
+	return 0;
+
+free_iomem:
+	iounmap(hw->ioaddr);
+free_mem:
+	kfree(hw);
+free_resource:
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	udma_err("udma HW probe failure \n");
+	return ret;
+}
+/**
+ * udma_remove - Device Removal Routine
+ * udma_remove is called by the PCI subsystem to alert the driver
+ * that it should release a PCI device.  The could be caused by a
+ * Hot-Plug event, or because the driver is going to be removed from
+ * memory.
+ **/
+void udma_hw_remove(struct pci_dev *pdev)
+{
+	struct udma_hw *hw = pci_get_drvdata(pdev);
+	if (hw == NULL)
+		return;
+	udma_free_sw(udma_hw_priv(hw));
+	udma_hw_exit(hw);
+	pci_set_drvdata(pdev, NULL);
+	iounmap(hw->ioaddr);
+	kfree(hw);
+	pci_release_region(pdev,0);
+	pci_disable_device(pdev);
+	printk(KERN_INFO "Intel(R) UDMA Device Driver Exit \n");
+}
+
+static struct pci_driver udma_driver = {
+        .name           = udma_driver_name,
+        .id_table       = udma_pci_tbl,
+        .probe          = udma_hw_probe,
+        .remove         = udma_hw_remove,
+};
+
+/**
+ * udma_init - Driver Registration Routine
+ *
+ **/
+static int __init udma_drv_init(void)
+{
+	int ret;
+	udma_info("Intel (R) UDMA Driver - %s\n", udma_driver_version);
+	udma_info("Copyright (c) 2012 Intel Corperation. \n");
+	ret = pci_register_driver(&udma_driver);
+
+	return ret;
+}
+
+
+/**
+ * udma_exit - Driver Exit Cleanup Routine
+ *
+ **/
+static void __exit udma_drv_exit(void)
+{
+	pci_unregister_driver(&udma_driver);
+}
+
+module_init(udma_drv_init);
+module_exit(udma_drv_exit);
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("Intel(R) UDMA Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(UDMA_DRV_VERSION);
+
+
+
+
+/* udma_hw.c */
--- /dev/null
+++ b/drivers/net/udma/udma_hw.h
@@ -0,0 +1,438 @@
+/*******************************************************************************
+
+  Intel UDMA driver
+  Copyright(c) 2012-2015 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  e1000-devel Mailing List <e1000-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+/* udma_hw.h
+ *  UDMA hardware layer
+ */
+#ifndef UDMA_HW_H
+#define UDMA_HW_H
+#include <linux/udma_api.h>
+
+#define DEBUG 1
+
+#ifdef DEBUG
+/*
+#define udma_warning(fmt, args...) do \
+	{\
+		printk(KERN_DEBUG "[%18s:%1d]" fmt "\n", __FUNCTION__,__LINE__,##args); \
+	} while (0)
+#define WARN_FUNC printk(KERN_DEBUG " func[%s] line [%d] \n", __FUNCTION__, __LINE__);
+*/
+#define udma_warning(fmt, args...) do { } while (0)
+#define WARN_FUNC do { } while (0);
+#define udma_dbg(format, arg...) do { } while (0)
+#define udma_debug(format, arg...) do { \
+      printk(KERN_INFO "UDMA_DEBUG(%s): " format,__func__,##arg); \
+   } while (0)
+
+/*
+#define udma_dbg(fmt, args...) do \
+	{\
+		printk(KERN_INFO "[%s] " fmt "\n", __func__,##args); \
+	} while (0)
+#define print_func printk(kern_debug " func[%s] line [%d] \n", __function__, __line__);
+*/
+#define udma_info(format, arg...) do \
+	{\
+		printk(KERN_INFO format, ##arg); \
+	} while (0)
+
+#else
+#define udma_warning(fmt, args...) do { } while (0)
+#define udma_dbg(format, arg...) do { } while (0)
+
+//#define udma_info(format, arg...) do { } while (0)
+#define udma_info(format, arg...) do \
+	{\
+		printk(KERN_INFO "	   %s " format "\n","UDMA - ",##arg); \
+	} while (0)
+
+#endif
+
+#define udma_err(format, arg...) do \
+   {\
+      printk(KERN_ERR "UDMA_ERROR: " format,##arg); \
+   } while (0)
+
+/* Total size for registers */
+#define UDMA_PORT_REGS_SPACE_SIZE 				0x88
+
+/* Generic UDMA Context Registers */
+
+#define UDMA_CURR_DESC 							0x008
+#define UDMA_NEXT_DESC							0x00C
+#define UDMA_SRCDMA_START						0x010
+#define UDMA_DSTDMA_START						0x014
+#define UDMA_SRCDMA_SIZE						0x018
+#define UDMA_FLAGS_MODE							0x01c
+#define UDMA_OTHER_MODE							0x020
+
+
+/* UDMA FLAGS MODE Register Attributes */
+#define UDMA_DMA_IS_ACTIVE                   (1<<31)
+#define UDMA_SRC_INT_EN                      (1<<30)
+#define UDMA_DST_INT_EN                      (1<<29)
+#define UDMA_TERM_EN                         (1<<28)
+#define UDMA_SRC_BIG_ENDIAN                  (1<<25)
+#define UDMA_DST_BIG_ENDIAN                  (1<<24)
+#define UDMA_PACKET_POSITION_STARTING        (1<<21)
+#define UDMA_PACKET_POSITION_ENDING          (2<<21)
+#define UDMA_PACKET_POSITION_STARTING_ENDING (3<<21)
+
+/* UDMA GAP Register Values */
+#define UDMA_XDMA_GAP_0_CLKS					(0<<16)
+#define UDMA_XDMA_GAP_16_CLKS					(1<<16)
+#define UDMA_XDMA_GAP_64_CLKS					(2<<16)
+#define UDMA_XDMA_GAP_256_CLKS					(3<<16)
+#define UDMA_XDMA_GAP_1024_CLKS					(4<<16)
+#define UDMA_XDMA_GAP_2048_CLKS					(5<<16)
+#define UDMA_XDMA_GAP_4096_CLKS					(6<<16)
+#define UDMA_XDMA_GAP_8192_CLKS					(7<<16)
+
+#define UDMA_XBURST_SZ_4_BYTES					(0<<12)
+#define UDMA_XBURST_SZ_8_BYTES					(1<<12)
+#define UDMA_XBURST_SZ_16_BYTES					(2<<12)
+#define UDMA_XBURST_SZ_32_BYTES					(3<<12)
+#define UDMA_XBURST_SZ_64_BYTES					(4<<12)
+
+#define UDMA_READ_EN							(1<<7)	// Equal to TX from ATOM side point of view
+#define UDMA_SRC_ADDR_MODE_LINEAR				(0<<5)
+#define UDMA_SRC_ADDR_MODE_FIXED				(2<<5)
+#define UDMA_SRC_ADDR_MODE_FIXED_CONTINUE		(3<<5)
+#define UDMA_SRC_LINK_LIST_EN					(1<<4)
+
+#define UDMA_WRITE_EN							(1<<3) // Equal to Rx from ATOM side point of view
+#define UDMA_DST_ADDR_MODE_LINEAR				(0<<1)
+#define UDMA_DST_ADDR_MODE_FIXED				(2<<1)
+#define UDMA_DST_ADDR_MODE_FIXED_CONTINUE		(3<<1)
+#define UDMA_DST_LINK_LIST_EN					(1<<0)
+
+/* UDMA OTHER MODE Register Attributes */
+#define UDMA_LL_OWNERSHIP_TAGS_EN				(1<<3)
+#define UDMA_LL_PRE_FETCH_DISABLE				(1<<2)
+#define UDMA_STOP								(1<<0)
+
+/* Starting a linked-list transfer */
+#define UDMA_FLAG_MODE_START_LL		(UDMA_SRC_ADDR_MODE_LINEAR | \
+					UDMA_DST_ADDR_MODE_LINEAR | \
+					UDMA_SRC_LINK_LIST_EN | \
+					UDMA_DST_LINK_LIST_EN)
+
+#define UDMA_FLAG_MODE_TX_LL		(UDMA_READ_EN | \
+					UDMA_SRC_ADDR_MODE_LINEAR | \
+					UDMA_SRC_LINK_LIST_EN | \
+					UDMA_DST_ADDR_MODE_FIXED)
+
+#define UDMA_FLAG_MODE_TX_LL_ENDING	(UDMA_TERM_EN | UDMA_READ_EN | \
+					UDMA_SRC_ADDR_MODE_LINEAR | \
+					UDMA_SRC_LINK_LIST_EN | \
+					UDMA_DST_ADDR_MODE_FIXED)
+
+#define UDMA_FLAG_MODE_RX_LL		(UDMA_WRITE_EN | \
+					UDMA_DST_ADDR_MODE_LINEAR | \
+					UDMA_DST_LINK_LIST_EN | \
+					UDMA_SRC_ADDR_MODE_FIXED | \
+					UDMA_DST_INT_EN)
+
+#define UDMA_FLAG_MODE_RX_LL_ENDING	(UDMA_TERM_EN | UDMA_WRITE_EN | \
+					UDMA_DST_ADDR_MODE_LINEAR | \
+					UDMA_DST_LINK_LIST_EN | \
+					UDMA_SRC_ADDR_MODE_FIXED)
+
+#define UDMA_OTHER_MODE_DEFAULT_SETTING		(UDMA_LL_OWNERSHIP_TAGS_EN | \
+						UDMA_LL_PRE_FETCH_DISABLE)
+
+
+/* UDMA Interrupt Request Registers */
+#define UDMA_INTR_MASK							0x80
+	#define UDMA_STOP_INT5							(1<<17)
+	#define UDMA_STOP_INT4							(1<<16)
+	#define UDMA_STOP_INT3							(1<<15)
+	#define UDMA_STOP_INT2							(1<<14)
+	#define UDMA_STOP_INT1							(1<<13)
+	#define UDMA_STOP_INT0							(1<<12)
+	#define UDMA_DST_INT5_EN						(1<<11)
+	#define UDMA_DST_INT4_EN						(1<<10)
+	#define UDMA_DST_INT3_EN						(1<<9)
+	#define UDMA_DST_INT2_EN						(1<<8)
+	#define UDMA_DST_INT1_EN						(1<<7)
+	#define UDMA_DST_INT0_EN						(1<<6)
+	#define UDMA_SRC_INT5_EN 						(1<<5)
+	#define UDMA_SRC_INT4_EN 						(1<<4)
+	#define UDMA_SRC_INT3_EN 						(1<<3)
+	#define UDMA_SRC_INT2_EN 						(1<<2)
+	#define UDMA_SRC_INT1_EN 						(1<<1)
+	#define UDMA_SRC_INT0_EN 						(1<<0)
+
+#define UDMA_INTR_STATUS						0x84
+	#define UDMA_STOP_INT5_ACT						(1<<17)
+	#define UDMA_STOP_INT4_ACT						(1<<16)
+	#define UDMA_STOP_INT3_ACT						(1<<15)
+	#define UDMA_STOP_INT2_ACT						(1<<14)
+	#define UDMA_STOP_INT1_ACT						(1<<13)
+	#define UDMA_STOP_INT0_ACT						(1<<12)
+	#define UDMA_DST_INT5_ACT						(1<<11)
+	#define UDMA_DST_INT4_ACT						(1<<10)
+	#define UDMA_DST_INT3_ACT						(1<<9)
+	#define UDMA_DST_INT2_ACT						(1<<8)
+	#define UDMA_DST_INT1_ACT						(1<<7)
+	#define UDMA_DST_INT0_ACT						(1<<6)
+
+	#define UDMA_DST_INT3_ACT_BIT					(9)
+	#define UDMA_DST_INT2_ACT_BIT					(8)
+	#define UDMA_DST_INT1_ACT_BIT					(7)
+	#define UDMA_DST_INT0_ACT_BIT					(6)
+
+
+	#define UDMA_SRC_INT5_ACT						(1<<5)
+	#define UDMA_SRC_INT4_ACT 						(1<<4)
+	#define UDMA_SRC_INT3_ACT 						(1<<3)
+	#define UDMA_SRC_INT2_ACT 						(1<<2)
+	#define UDMA_SRC_INT1_ACT						(1<<1)
+	#define UDMA_SRC_INT0_ACT 						(1<<0)
+
+	#define UDMA_SRC_INT3_ACT_BIT					(3)
+	#define UDMA_SRC_INT2_ACT_BIT					(2)
+	#define UDMA_SRC_INT1_ACT_BIT					(1)
+	#define UDMA_SRC_INT0_ACT_BIT					(0)
+
+#define UDMA_HW_VALID_TX_INTR_STATE(port) 			((port)?UDMA_SRC_INT3_ACT:UDMA_SRC_INT1_ACT)
+#define UDMA_HW_VALID_RX_INTR_STATE(port) 			((port)?UDMA_DST_INT2_ACT:UDMA_DST_INT0_ACT)
+#define UDMA_HW_VALID_RX_STOP_INTR_STATE(port) 		((port)?UDMA_SRC_INT2_ACT:UDMA_SRC_INT0_ACT)
+
+
+
+#define UDMA_HW_VALID_INTR_STATE(port) 				(UDMA_HW_VALID_TX_INTR_STATE(port)|UDMA_HW_VALID_RX_INTR_STATE(port))
+
+#define UDMA_RX_REG_BASE 							0x00
+#define UDMA_TX_REG_BASE 							0x40
+
+
+/* UDMA Port Address Map */
+/*
+ * port 0:
+ * 		context 0: L2SW -> DDR
+ * 		context 1: DDR  -> L2SW
+ * port 1:
+ * 		context 2: L2SW -> DDR
+ * 		context 3: DDR  -> L2SW
+ */
+/* UDMA port 0 */
+#define UDMA_CONTEXT0_DMA_ADDR						0xFFFA0000
+#define UDMA_CONTEXT1_DMA_ADDR						0xFFFB0000
+
+/* UDMA port 1 */
+#define UDMA_CONTEXT2_DMA_ADDR						0xFFFC0000
+#define UDMA_CONTEXT3_DMA_ADDR						0xFFFD0000
+
+/* NPCPU port */
+#define UDMA_CONTEXT4_DMA_ADDR						0xFFFE0000
+#define UDMA_CONTEXT5_DMA_ADDR						0xFFFF0000
+
+/* Debug port */
+#define UDMA_CONTEXT_DUMP_DMA_ADDR					0xFFF00000
+
+
+
+struct __port_regset {
+	u32 rx;
+	u32 tx;
+};
+
+struct __intr_regset {
+	u32 int_mask;
+	u32 int_stat;
+};
+struct __intr_status_bitset {
+	u32 rx;
+	u32 tx;
+};
+
+struct __cntx_regset {
+	unsigned short cdesc;
+	unsigned short ndesc;
+	unsigned short sdma;
+	unsigned short ddma;
+	unsigned short size;
+	unsigned short flag;
+	unsigned short other;
+};
+struct udma_cntx_regset {
+	struct __cntx_regset *rx;
+	struct __cntx_regset *tx;
+	struct __intr_regset *intr;
+};
+
+#define UDMA_DEVICE_ID									0x0947
+
+#define UDMA0_NAME "udma0"
+#define UDMA1_NAME "udma1"
+
+#define UDMA_RX  										(0)
+#define UDMA_TX  										(1)
+#define UDMA_MAX_RX_BUFFER_SIZE 						BIT(24)
+#define UDMA_MAX_XMIT_SIZE 								(1526)  /* The L2SW only support 1526 bytes as MTU */
+//#define UDMA_DESC_MAX_XMIT_SIZE 					(1514)	/* header + data  */
+#define UDMA_MIN_XMIT_SIZE 								(64)
+#define UDMA_RX_MIN_BUFFER_SIZE							(2*1024)
+
+/* UDMA Descriptor based operations */
+#define SET_DESC_FLAGS(desc, value)    ((desc)->flags |= (value))
+#define CLEAR_DESC_FLAGS(desc, value)  ((desc)->flags &= (~(value)))
+#define TEST_DESC_FLAGS(desc, value)   (((desc)->flags & (value)) == (value))
+
+#define DESC_INITIAL_FLAG								0x00	/* Initial state, sw owns the descriptor */
+#define DESC_HANDLING_FLAG								0x80	/* Descriptor is being processed by DMA */
+#define DESC_DONE_FLAG									0xc0	/* Descriptor processed state */
+
+#define DESC_IS_INITIAL(d)       ((d)->union_field.fields.ownership == DESC_INITIAL_FLAG)
+#define DESC_IS_IN_PROGRESS(d) 	((d)->union_field.fields.ownership == DESC_HANDLING_FLAG)
+#define DESC_IS_DONE(d)				((d)->union_field.fields.ownership == DESC_DONE_FLAG)
+#define DESC_IS_NULL(d)				(((d)->src == 0) || ((d)->dst == 0))
+
+#define DESC_DATA_BUFFER_LEN(d)  (((d)->union_field.size<<8)>>8)
+
+/* UDMA Descriptor */
+struct udma_desc {
+	__le32 next_desc;									/* Address of the next descriptor */
+	union {
+		__le32 size;									/* Data buffer length */
+		struct {
+		u8 length[3];									/* buffer lengh */
+		u8 ownership;									/* owner ship field */
+		} fields;
+	} union_field;
+	__le32 src;											/* Source Addr */
+	__le32 dst;											/* Target Addr */
+	__le32 flags;										/* Flag mode */
+}__attribute__((aligned(4)));
+
+struct udma_mode {
+	udma_burstsz_t udma_to_l2_burst;
+	udma_burstsz_t l2_to_udma_burst;
+	udma_gapval_t udma_to_l2_gap;
+	udma_gapval_t l2_to_udma_gap;
+};
+
+/* Describe a UDMA port */
+struct udma_hw {
+	u8							port;					/* The port number of this UDMA HW */
+	volatile void __iomem 		*ioaddr;				/* Virtual address of reg base address */
+	struct udma_hw_operations 	*ops;
+	struct pci_dev				*pdev;
+	struct udma_mode mode;
+	unsigned long private[0] ____cacheline_aligned;
+};
+
+struct udma_hw_operations {
+	bool (*tx_is_active)(struct udma_hw *);
+	bool (*rx_is_active)(struct udma_hw *);
+
+	u32 (*get_curr_rx_desc)(struct udma_hw *);
+	u32 (*get_curr_tx_desc)(struct udma_hw *);
+
+	u32 (*get_next_rx_desc)(struct udma_hw *);
+	u32 (*get_next_tx_desc)(struct udma_hw *);
+
+	int (*start_rx_transfer)(struct udma_hw *, u32 );
+	int (*start_tx_transfer)(struct udma_hw *, u32 );
+
+	int  (*stop_rx_transfer)(struct udma_hw *);
+	int  (*stop_tx_transfer)(struct udma_hw *);
+
+	bool (*tx_is_stopped)(struct udma_hw *);
+	bool (*rx_is_stopped)(struct udma_hw *);
+
+	void (*init_rx_desc)(struct udma_hw *, struct udma_desc *);
+	void (*init_tx_desc)(struct udma_hw *, struct udma_desc *);
+
+	void (*update_rx_desc)(struct udma_hw *, struct udma_desc *, u32, u32);
+	void (*update_tx_desc)(struct udma_hw *, struct udma_desc *, u32, u32);
+
+	int (*hw_init)(struct udma_hw *);
+	int (*hw_exit)(struct udma_hw *);
+
+	u32  (*get_irq_status)(struct udma_hw *);
+
+	int (*disable_rx_irq)(struct udma_hw *);
+	int (*enable_rx_irq)(struct udma_hw *);
+
+	int (*disable_tx_irq)(struct udma_hw *);
+	int (*enable_tx_irq)(struct udma_hw *);
+
+	int (*clear_rx_irq)(struct udma_hw *);
+	int (*clear_tx_irq)(struct udma_hw *);
+
+	void (*clear_rx_term)(struct udma_hw *);
+	void (*clear_tx_term)(struct udma_hw *);
+
+	u32 (*rx_is_term)(struct udma_hw *);
+
+};
+
+static inline void udma_writel(struct udma_hw *hw, int reg, u32 val)
+{
+    __raw_writel(val, hw->ioaddr + reg);
+}
+static inline u32 udma_readl(struct udma_hw *hw,int reg)
+{
+    return __raw_readl(hw->ioaddr + reg);
+}
+
+static inline void *udma_hw_priv(struct udma_hw *hw)
+{
+	return (void *)hw->private;
+}
+
+static inline uint8_t udma_read_and_test_bits(struct udma_hw *hw,int reg, u32 val)
+{
+   return ((udma_readl(hw, reg) & (val)) > 0);
+}
+
+static inline void udma_read_and_set_bits(struct udma_hw *hw,int reg, u32 val)
+{
+    udma_writel(hw, reg, udma_readl(hw, reg) | (val));
+}
+static inline void udma_read_and_clr_bits(struct udma_hw *hw,int reg, u32 val)
+{
+    udma_writel(hw, reg, udma_readl(hw, reg) & (~(val)));
+}
+
+static inline void udma_set_bits_nr(struct udma_hw *hw,int reg, u32 nr)
+{
+	__set_bit(nr,hw->ioaddr + reg);
+}
+
+static inline void udma_clr_bits_nr(struct udma_hw *hw,int reg, u32 nr)
+{
+	__clear_bit(nr,hw->ioaddr + reg);
+}
+
+void udma_regs_dump(struct udma_hw *hw, bool direction);
+
+#endif /* UDMA_HW_H */
+
--- /dev/null
+++ b/drivers/net/udma/udma_main.c
@@ -0,0 +1,1670 @@
+/*
+ * GPL LICENSE SUMMARY
+ *
+ *  Copyright(c) 2012-2015 Intel Corporation. All rights reserved.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of version 2 of the GNU General Public License as
+ *  published by the Free Software Foundation.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *  The full GNU General Public License is included in this distribution
+ *  in the file called LICENSE.GPL.
+ *
+ *  Contact Information:
+ *    Intel Corporation
+ *    2200 Mission College Blvd.
+ *    Santa Clara, CA  97052
+ *
+ */
+
+/* UDMA  Driver main stack */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/device.h>
+#include <asm/uaccess.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/pci.h>
+#include <linux/ctype.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include <linux/time.h>
+#include <linux/udma_api.h>
+#include <linux/proc_fs.h>
+
+#include "udma_hw.h"
+#include "udma_main.h"
+
+/* Rx buffer size definitions */
+#define UDMA_MIN_RX_SIZE      (1522)   // 1500 MTU + 18 HDR + 4 VLAN
+#define UDMA_MAX_RX_SIZE      (9216)
+
+/* Interrupt Throttle Rate (ITR) constants */
+#define ITR_LOW_INT_FREQ         (10000)  // Low latency interrupt frequency
+#define INTR_FREQ_MIN_FIXED      (1000)   // Min. interrupt frequency
+#define INTR_FREQ_MAX_FIXED      (10000)  // Max. interrupt frequency
+#define ITR_AVG_PKT_SIZE         (1538)   // Average packet size in bytes
+#define ITR_DEFAULT_PPC          (20)     // Packets per chain
+#define ITR_MIN_MBPS             (100)    // Min. data rate in Mbps
+#define ITR_MAX_MBPS             (1200)   // Max. data rate in Mbps
+#define ITR_MIN_STEP_MBPS        (50)     // Step increment in Mbps
+#define ITR_DEFAULT_MIN_MBPS     (200)    // Default minimum Mbps
+#define ITR_DEFAULT_MAX_MBPS     (800)    // Default maximum Mbps
+#define ITR_DEFAULT_STEP_MBPS    (100)    // Default step increment in Mbps
+
+#define ONE_SEC_TO_NS            (1000000000)
+#define MAX_UDMA_STOP_DELAY      (200)
+
+static u32 burst_sizes[] = {4, 8, 16, 32, 64, 128};
+static u32 gap_values[] = {0, 16, 64, 256, 1024, 2048, 4096, 8192};
+
+static void udma_start_rx_transfer(struct udma_device *umdev);
+static void udma_restart_rx_transfer(struct udma_device *umdev);
+static void udma_continue_rx_transfer(struct udma_device *umdev);
+static void udma_start_tx_transfer(struct udma_device *umdev);
+static int udma_allocate_rx_buffers(struct udma_device *udma_dev, int count);
+static int udma_setup_resources(struct udma_device *udma_dev);
+static void udma_free_resources(struct udma_device *udma_dev);
+
+#ifdef DEBUG
+
+#define UDMA_WARN_ON_RETURN(condition,ret) ({      \
+      int __ret_warn_on = !!(condition);           \
+      if (__ret_warn_on) return ret;               \
+   })
+
+static inline void udma_print_desc(struct udma_desc *desc)
+{
+   udma_info("desc         :  0x%x\n", (u32)desc);
+   udma_info("next_desc    :  0x%x\n", (u32)desc->next_desc);
+   udma_info("src          :  0x%x\n", desc->src);
+   udma_info("dest         :  0x%x\n", desc->dst);
+   udma_info("size         :  0x%x\n", desc->union_field.size);
+   udma_info("flags        :  0x%x\n\n", (u32)desc->flags);
+}
+
+static void udma_print_ring(struct udma_queue *q, bool print_descs)
+{
+   struct udma_ring *ring = &q->ring;
+   udma_info("\n ======================= Ring Info =======================\n");
+   udma_info("    dma            :  0x%x\n", (u32)ring->dma);
+   udma_info("    desc           :  0x%p\n", ring->desc);
+   udma_info("    direction      :  %d\n", q->direction);
+   udma_info("    dma_size       :  0x%x\n", ring->dma_size);
+   udma_info("    entries        :  %d\n", ring->entries);
+   udma_info("    to_be_clean    :  %d\n", ring->to_be_clean);
+   udma_info("    to_be_use      :  %d\n", ring->to_be_use);
+   udma_info("    tail           :  %d\n", ring->tail);
+   udma_info("    new_tail       :  %d\n", ring->new_tail);
+   udma_info("\n =========================================================\n");
+
+   if (print_descs) {
+      int i;
+      udma_info("\n ==================== Ring Entries ====================\n");
+      for( i = 0 ; i < ring->entries ; i++ ) {
+         udma_print_desc(INDEX_TO_DESC(i,ring));
+      }
+      udma_info("\n ======================================================\n");
+   }
+}
+
+#else 
+#define UDMA_WARN_ON_RETURN(condition,ret) do{}while(0)
+#define udma_print_ring(q, descs) \
+   do { \
+   } while(0)
+#endif //UDMA_DEBUG
+
+static inline u16 __get_using_desc(struct udma_ring *ring)
+{
+   u16 i = ring->to_be_use, next;
+   bool keep_going = true;
+   struct udma_desc *cur_desc = NULL, *next_desc = NULL;
+   cur_desc = INDEX_TO_DESC(i,ring);
+   do {
+      rmb();
+      next = NEXT_DESC_IDX(i, ring);
+      next_desc = INDEX_TO_DESC(next,ring);
+      prefetch(next_desc);
+      if ((i!=ring->tail) && DESC_IS_DONE(cur_desc)) {
+         i = next;
+         cur_desc = next_desc;
+      } else {
+         keep_going = false;
+      }
+   } while (keep_going);
+   return i;
+}
+
+static bool udma_rx_is_stopped(struct udma_device *umdev)
+{
+   struct udma_hw *hw = umdev->hw;
+   struct udma_desc *udma_tail_desc = NULL;
+   struct udma_ring *rx = &umdev->rx.ring;
+   u32 desc = 0, status = 0;
+   int index;
+   bool ret = false;
+
+   if (!(hw->ops->rx_is_active(hw))) {
+      status = hw->ops->get_irq_status(hw);
+      if (status & UDMA_HW_VALID_RX_STOP_INTR_STATE(hw->port)) {
+         ret = true;
+      } else {
+         desc = hw->ops->get_curr_rx_desc(hw);
+         index = DESC_DMA_TO_DESC_INDEX(desc,rx);
+         udma_tail_desc = INDEX_TO_DESC(rx->tail,rx);
+         ret = ((index == rx->tail) && DESC_IS_DONE(udma_tail_desc));
+      }
+   }
+   return ret;
+}
+
+static bool udma_tx_is_stopped(struct udma_device *umdev)
+{
+   struct udma_hw *hw = umdev->hw;
+   struct udma_desc *udma_desc = NULL;
+   struct udma_ring *tx = &umdev->tx.ring;
+   u32 desc;
+   int index;
+   bool ret = false;
+
+   if (!(hw->ops->tx_is_active(hw))) {
+      desc = hw->ops->get_next_tx_desc(hw);
+      if (unlikely(desc == 0)) {
+         ret = true;
+      } else {
+         desc = hw->ops->get_curr_tx_desc(hw);
+         index = DESC_DMA_TO_DESC_INDEX(desc,tx);
+         udma_desc = INDEX_TO_DESC(tx->tail,tx);
+         ret = ((index == tx->tail) && DESC_IS_DONE(udma_desc));
+      }
+   }
+   return ret;
+}
+
+static void udma_tx_q_pop(struct udma_device *umdev,
+                          const unsigned int idx, bool drop)
+{
+   struct udma_queue *q = &umdev->tx;
+   struct udma_desc *desc = NULL;
+   struct udma_buffer *buffer_info = NULL;
+   u32 data_len = 0;
+
+   desc = INDEX_TO_DESC(idx,&q->ring);
+   buffer_info = &q->ring.buffer_info[idx];
+   if (!drop) {
+      data_len = buffer_info->length - DESC_DATA_BUFFER_LEN(desc);
+      q->itr.c_bytes += data_len;
+      q->stats.bytes += data_len;
+      q->stats.pkts++;
+      q->itr.c_packets++;
+   }
+
+   if (likely(buffer_info->dma)) {
+      dma_unmap_single(&umdev->hw->pdev->dev, buffer_info->dma,
+                        buffer_info->length, DMA_TO_DEVICE);
+      buffer_info->dma = 0;
+   }
+   if (likely(buffer_info->skb)) {
+      dev_kfree_skb_any(buffer_info->skb);
+      buffer_info->skb = 0;
+   }
+}
+
+static void udma_rx_q_pop(struct udma_device *umdev,
+                          const unsigned int idx, bool drop)
+{
+   struct udma_queue *q = &umdev->rx;
+   struct udma_desc *desc = NULL;
+   struct udma_buffer *buffer_info = NULL;
+   u32 data_size = 0;
+
+   buffer_info = &q->ring.buffer_info[idx];
+   desc = INDEX_TO_DESC(idx,&q->ring);
+   data_size = umdev->rx_udma_size - DESC_DATA_BUFFER_LEN(desc);
+
+   if (likely(buffer_info->dma)) {
+      dma_unmap_single(&umdev->hw->pdev->dev, buffer_info->dma,
+                        buffer_info->length, DMA_FROM_DEVICE);
+      buffer_info->dma = 0;
+   }
+
+   if (!drop && likely(DESC_IS_DONE(desc))) {
+      q->stats.bytes += data_size;
+      q->itr.c_bytes += data_size;
+      q->stats.pkts++;
+      q->itr.c_packets++;
+      /* Remove FCS and transfer to net stack */
+      data_size -= ETH_FCS_LEN;
+      skb_put(buffer_info->skb, data_size);
+      q->callback(buffer_info->skb, umdev->netdev);
+      buffer_info->skb = NULL;
+   } else if (buffer_info->skb) {
+      dev_kfree_skb_any(buffer_info->skb);
+      buffer_info->skb = NULL;
+      q->stats.drops++;
+   }
+}
+
+static void inline udma_irq_enable(struct udma_hw *hw)
+{
+   hw->ops->enable_tx_irq(hw);
+   hw->ops->enable_rx_irq(hw);
+}
+
+static void inline udma_irq_disable(struct udma_hw *hw)
+{
+   hw->ops->disable_tx_irq(hw);
+   hw->ops->disable_rx_irq(hw);
+}
+
+/**
+ * udma_intr - Interrupt Handler
+ * @irq: interrupt number
+ * @data: pointer to a private device structure
+ **/
+static irqreturn_t udma_intr(int irq, void *data)
+{
+   struct udma_device *umdev = (struct udma_device *)data;
+   struct udma_hw *hw = umdev->hw;
+   struct udma_queue *rx = &umdev->rx;
+   struct udma_queue *tx = &umdev->tx;
+   u32 status = hw->ops->get_irq_status(hw);
+
+   if (!(UDMA_HW_VALID_INTR_STATE(hw->port) & status)) {
+      return IRQ_NONE;
+   }
+
+   /* Disable interrupts */
+   udma_irq_disable(umdev->hw);
+
+   /* Tx interrupt */
+   if (status & UDMA_HW_VALID_TX_INTR_STATE(hw->port)) {
+      udma_dbg("Tx udma_intr status 0x%x \n",status);
+      spin_lock(&tx->ring.lock);
+      tx->stats.irqs++;
+      hw->ops->clear_tx_irq(hw);
+      spin_unlock(&tx->ring.lock);
+   }
+
+   /* Rx interrupt */
+   if (status & UDMA_HW_VALID_RX_INTR_STATE(hw->port)) { 
+      udma_dbg("Rx udma_intr status 0x%x \n",status);
+      spin_lock(&rx->ring.lock);
+      rx->stats.irqs++;
+      hw->ops->clear_rx_irq(hw);
+      spin_unlock(&rx->ring.lock);
+   }
+
+   /* Start HR timer */
+   if (!hrtimer_active(&umdev->itr_timer)) {
+      hrtimer_start(&umdev->itr_timer,
+         ns_to_ktime(umdev->itr_cfg.ns), HRTIMER_MODE_REL);
+   }
+
+   return IRQ_HANDLED;
+}
+
+#ifdef SOFT_IRQ_STATS
+static void udma_update_q_stats(struct udma_queue *q)
+{
+   int pkts;
+   if (q->direction == UDMA_RX)
+      /* Number of packets received and waiting for releasing */
+      pkts = IDX_SUB(q->ring.to_be_use, q->ring.to_be_clean, &q->ring);
+   else
+      /* Number of packets waiting for transmission */
+      pkts = IDX_SUB(q->ring.new_tail, q->ring.to_be_clean, &q->ring);
+
+   if (pkts > q->stats.max_pkts)
+      q->stats.max_pkts = pkts;
+   q->stats.acc_pkts += pkts;
+}
+#endif
+
+/* Clean finished descriptors */
+static int udma_clean_done_desc(struct udma_device *umdev, struct udma_queue *q,
+                                const int budget, bool get_spinlock)
+{
+   int i, using = 0, cleaned = 0;
+   unsigned long flags = 0;
+   struct udma_ring *ring = &q->ring;
+
+   if (get_spinlock) spin_lock_irqsave(&ring->lock, flags);
+   ring->to_be_use = __get_using_desc(ring);
+#ifdef SOFT_IRQ_STATS
+   udma_update_q_stats(q);
+#endif
+   using = ring->to_be_use;
+   if (get_spinlock) spin_unlock_irqrestore(&ring->lock, flags);
+   
+   i = ring->to_be_clean;
+   while (i != using && cleaned < budget) {
+      q->pop_desc(umdev, i, false);
+      cleaned++;
+      i = NEXT_DESC_IDX(i,ring);
+   }
+   ring->to_be_clean = i;
+   return cleaned;
+}
+
+static int udma_process_tx_queue(struct udma_device *umdev)
+{
+   unsigned long flags;
+   int done = 0;
+
+   spin_lock_irqsave(&umdev->tx.ring.lock, flags);
+   /* Clean all Tx finished descriptors */
+   done = udma_clean_done_desc(umdev, &umdev->tx,
+                               UDMA_RING_VALID_MAX_NUM(umdev->tx.ring), false);
+
+   if (udma_tx_is_stopped(umdev)) {
+      udma_start_tx_transfer(umdev);
+   }
+   spin_unlock_irqrestore(&umdev->tx.ring.lock, flags);
+   return done;
+}
+
+static int udma_process_rx_queue(struct udma_device *umdev, const int budget)
+{
+   unsigned long flags;
+   int done = 0;
+   /* Clean Rx finished descriptors up to budget */
+   done = udma_clean_done_desc(umdev, &umdev->rx, budget, true);
+   if (done)
+      udma_allocate_rx_buffers(umdev, done);
+
+   spin_lock_irqsave(&umdev->rx.ring.lock, flags);
+   if (udma_rx_is_stopped(umdev)) {
+      udma_start_rx_transfer(umdev);
+   } else {
+      udma_continue_rx_transfer(umdev);
+      /* Check stop again to avoid UDMA stopped during the transaction */
+      if (udma_rx_is_stopped(umdev))
+         udma_restart_rx_transfer(umdev);
+   }
+   spin_unlock_irqrestore(&umdev->rx.ring.lock, flags);
+   return done;
+}
+
+static void udma_set_fixed_itr(struct udma_itr_config *itr, u32 cps)
+{
+   if((cps < INTR_FREQ_MIN_FIXED) || (cps > INTR_FREQ_MAX_FIXED)) {
+      udma_err("Interrupt frequency must be in the range from %d to %d.\n",
+               INTR_FREQ_MIN_FIXED, INTR_FREQ_MAX_FIXED);
+   } else {
+      itr->mode = ITR_FIXED;
+      itr->ns = ONE_SEC_TO_NS/cps;
+   }
+}
+
+static inline u32 mbps_to_cps(u32 mbps, u32 ppc)
+{
+   return ((mbps * 1000000) / (ppc * ITR_AVG_PKT_SIZE * 8));
+}
+
+static inline u32 cps_to_mbps(u32 cps, u32 ppc)
+{
+   return (cps * (ppc * ITR_AVG_PKT_SIZE * 8) / 1000000);
+}
+
+static void udma_set_adaptive_itr(struct udma_itr_config *itr,
+                                  int min_mbps, int max_mbps,
+                                  int step_mbps, int ppc)
+{
+   if( min_mbps < ITR_MIN_MBPS ) {
+      udma_err("Minimum data rate for adaptive mode is (%d)\n", ITR_MIN_MBPS);
+   } else if( max_mbps > ITR_MAX_MBPS ) {
+      udma_err("Maximum data rate for adaptive mode is (%d)\n", ITR_MAX_MBPS);
+   } else if ( step_mbps < ITR_MIN_STEP_MBPS ) {
+      udma_err("Minimum step for adaptive mode is (%d)\n", ITR_MIN_STEP_MBPS);
+   } else {
+      itr->mode = ITR_ADAPTIVE;
+      itr->ppc = ppc;
+      itr->min_cps = mbps_to_cps(min_mbps, ppc);
+      itr->max_cps = mbps_to_cps(max_mbps, ppc);
+      itr->step_cps = mbps_to_cps(step_mbps, ppc);
+   }
+}
+
+static void udma_init_adaptive_itr(struct udma_device *umdev)
+{
+   memset(&umdev->rx.itr, 0, sizeof(struct udma_adapt_itr));
+   memset(&umdev->tx.itr, 0, sizeof(struct udma_adapt_itr));
+   umdev->rx.itr.latency = LOW_LATENCY;
+   umdev->tx.itr.latency = LOW_LATENCY;
+   umdev->rx.itr.cps = ITR_LOW_INT_FREQ;
+   umdev->tx.itr.cps = ITR_LOW_INT_FREQ;
+   umdev->itr_cfg.ns = ONE_SEC_TO_NS/ITR_LOW_INT_FREQ;
+}
+
+static void udma_update_itr(struct udma_itr_config *itr_cfg,
+                            struct udma_adapt_itr *itr)
+{
+   u16 new_itr_latency = itr->latency;
+   u16 new_cps = itr->cps;
+   u32 bytes_per_packet = 0;
+   if (itr->c_packets) {
+      bytes_per_packet = itr->c_bytes/itr->c_packets;
+      switch (itr->latency) {
+         case LOW_LATENCY:
+            if ((bytes_per_packet > 1200) && (itr->c_packets > 5))
+               new_itr_latency = BULK_LATENCY;
+         break;
+         case BULK_LATENCY:
+            if ((itr->c_bytes < 6000) && (itr->c_packets < 5))
+               new_itr_latency = LOW_LATENCY;
+         break;
+      }
+
+      if (new_itr_latency == LOW_LATENCY) {
+         /* LOW LATENCY CASE */
+         new_cps = min(itr->cps + itr_cfg->step_cps, (u32)ITR_LOW_INT_FREQ);
+      } else {
+         /* BULK LATENCY CASE */
+         if( bytes_per_packet > 1600 ) {
+            /* Jumbo frames detected */
+            new_cps = itr_cfg->min_cps;
+         } else if (new_itr_latency != itr->latency) {
+            /* Going from low to bulk latency mode */
+            new_cps = min(itr->cps - itr_cfg->step_cps, itr_cfg->max_cps);
+         } else if (itr->c_packets < itr->lo_ppc) {
+            /* Packets in chain is too low, decrease interrupt frequency */
+            new_cps = max(itr->cps - itr_cfg->step_cps, itr_cfg->min_cps);
+         } else if (itr->c_packets > itr->hi_ppc) {
+            /* Packets in chain is too high, increase interrupt frequency */
+            new_cps = min(itr->cps + itr_cfg->step_cps, itr_cfg->max_cps);
+         }
+         if (itr->cps != new_cps) {
+            /* Update low and high packets per chain thresholds */
+            u32 thres_cps = (itr_cfg->step_cps>>1)+(itr_cfg->step_cps>>2);
+            itr->lo_ppc = ((new_cps-thres_cps)*itr_cfg->ppc)/new_cps;
+            itr->hi_ppc = (((new_cps+thres_cps)*itr_cfg->ppc)/new_cps) + 1;
+         }
+      }
+      itr->cps = new_cps;
+      itr->latency = new_itr_latency;
+   }
+}
+
+static int udma_process_queues(struct napi_struct *napi, int budget)
+{
+   struct udma_device *umdev = container_of(napi, struct udma_device, napi);
+   int tx_work_done, rx_work_done;
+
+#ifdef SOFT_IRQ_STATS
+   umdev->tx.stats.s_irqs++;
+   umdev->rx.stats.s_irqs++;
+#endif
+
+   tx_work_done = udma_process_tx_queue(umdev);
+   rx_work_done = udma_process_rx_queue(umdev,budget);
+
+   if (tx_work_done == UDMA_RING_VALID_MAX_NUM(umdev->tx.ring)) {
+      rx_work_done = budget;
+   }
+
+   if (rx_work_done < budget) {
+      if (umdev->itr_cfg.mode == ITR_ADAPTIVE) {
+         udma_update_itr(&umdev->itr_cfg, &umdev->tx.itr);
+         udma_update_itr(&umdev->itr_cfg, &umdev->rx.itr);
+         umdev->itr_cfg.ns = ONE_SEC_TO_NS/min(umdev->rx.itr.cps,
+                                               umdev->tx.itr.cps);
+      }
+      napi_complete(napi);
+      udma_irq_enable(umdev->hw);
+   }
+   return rx_work_done;
+}
+
+static void udma_start_rx_transfer(struct udma_device *umdev)
+{
+   struct udma_hw *hw = umdev->hw;
+   struct udma_ring *rx = &umdev->rx.ring;
+   struct udma_desc *udma_desc_new_tail;
+
+   if(rx->tail != rx->new_tail) {
+      rx->to_be_use = NEXT_DESC_IDX(rx->tail, rx);
+      udma_desc_new_tail = &rx->desc[rx->new_tail];
+      SET_DESC_FLAGS(udma_desc_new_tail, UDMA_TERM_EN);
+      rx->tail = rx->new_tail;
+      hw->ops->start_rx_transfer(hw, DESC_INDEX_TO_DESC_DMA(rx->to_be_use,rx));
+   }
+}
+
+static void udma_continue_rx_transfer(struct udma_device *umdev)
+{
+   struct udma_hw *hw = umdev->hw;
+   struct udma_ring *rx = &umdev->rx.ring;
+   struct udma_desc *udma_desc_tail;
+
+   if (rx->tail != rx->new_tail) {
+      udma_desc_tail = &rx->desc[rx->tail];
+      CLEAR_DESC_FLAGS(udma_desc_tail, UDMA_TERM_EN);
+      rx->tail = rx->new_tail;
+      udma_desc_tail = &rx->desc[rx->tail];
+      SET_DESC_FLAGS(udma_desc_tail, UDMA_TERM_EN);
+      /* clear termination in flag register after update tail */
+      if(hw->ops->rx_is_term(hw)){
+         hw->ops->clear_rx_term(hw);
+      }
+   }
+}
+
+static void udma_restart_rx_transfer(struct udma_device *umdev)
+{
+   struct udma_hw *hw = umdev->hw;
+   struct udma_ring *rx = &umdev->rx.ring;
+   struct udma_desc *udma_desc_tail;
+
+   rx->to_be_use = __get_using_desc(rx);
+   udma_desc_tail = &rx->desc[rx->tail];
+   if (!DESC_IS_DONE(udma_desc_tail)) {
+      hw->ops->start_rx_transfer(hw, DESC_INDEX_TO_DESC_DMA(rx->to_be_use,rx));
+   }
+}
+
+static void udma_start_tx_transfer(struct udma_device *umdev)
+{
+   struct udma_hw *hw = umdev->hw;
+   struct udma_ring *tx = &umdev->tx.ring;
+
+   if (tx->tail != tx->new_tail) {
+      tx->to_be_use = NEXT_DESC_IDX(tx->tail, tx);
+      SET_DESC_FLAGS(&tx->desc[tx->new_tail], UDMA_TERM_EN | UDMA_SRC_INT_EN);
+      tx->tail = tx->new_tail;
+      hw->ops->start_tx_transfer(hw, DESC_INDEX_TO_DESC_DMA(tx->to_be_use,tx));
+   } 
+}
+
+static inline bool is_ring_full(struct udma_ring *ring)
+{
+   return (((ring->new_tail + 2)%ring->entries) == ring->to_be_clean);
+}
+
+static inline int add_tx_skb(struct udma_device *umdev, struct sk_buff *skb)
+{
+   struct udma_buffer *buffer_info = NULL;
+   dma_addr_t dma;
+   u32 len = skb_headlen(skb), idx = 0;
+   int ret = UDMA_OK;
+   struct udma_ring *tx = &umdev->tx.ring;
+   struct udma_hw *hw = umdev->hw;
+
+   if (skb->len < ETH_ZLEN) {
+      WARN_ON(skb_pad(skb,ETH_ZLEN + ETH_FCS_LEN - skb->len));
+      len = ETH_ZLEN + ETH_FCS_LEN;
+   } else {
+      WARN_ON(skb_pad(skb,ETH_FCS_LEN));
+      len += ETH_FCS_LEN;
+   }
+
+   dma = dma_map_single(&hw->pdev->dev, skb->data, len, DMA_TO_DEVICE);
+   if (dma_mapping_error(&hw->pdev->dev, dma)) {
+      dev_err(&hw->pdev->dev, "DMA map failed\n");
+      ret = UDMA_ERR;
+   } else {
+      /* Fill in the buffer info to the descriptor */
+      idx = NEXT_DESC_IDX(tx->new_tail, tx);
+      tx->new_tail = idx;
+      buffer_info = &tx->buffer_info[idx];
+      buffer_info->skb = skb;
+      buffer_info->dma = dma;
+      buffer_info->length = len;
+      hw->ops->update_tx_desc(hw, &tx->desc[idx],dma,len);
+   }
+   if (udma_tx_is_stopped(umdev))
+      udma_start_tx_transfer(umdev);
+   return ret;
+}
+
+int udma_send_packet(unsigned char port, struct sk_buff *skb)
+{
+   struct udma_device *umdev = NULL;
+   struct udma_ring *tx = NULL;
+   int ret = UDMA_OK;
+   unsigned long flags;
+
+   /* skb check*/
+   if (unlikely((!skb) || (skb->len <= 0) || (skb->data_len !=0)))
+      return UDMA_INVALID_PARAM;
+
+   umdev = udma_devs[port];
+   tx = &umdev->tx.ring;
+
+   spin_lock_irqsave(&tx->lock, flags);
+   if (is_ring_full(tx)) {
+      /* Release processed buffers in Tx queue */
+      udma_clean_done_desc(umdev, &umdev->tx,
+                           UDMA_RING_VALID_MAX_NUM(umdev->tx.ring), false);
+      ret = UDMA_FULL;
+      umdev->tx.stats.drops++;
+   } else {
+      ret = add_tx_skb(umdev, skb);
+   }
+   spin_unlock_irqrestore(&tx->lock, flags);
+
+   return ret;
+}
+
+/**
+ * Allocate buffers to UDMA driver to receive packets.
+ * @umdev - UDMA device
+ * @count - Number of buffers to be allocated
+ *
+ * It's required to allocate a single buffer for a packet. 
+ * Thus the coming free buffer size should be large enough,
+ * i.e larger than 1536 bytes. Otherwise, the buffer would be refused.
+ *
+ * return UDMA_OK: Buffers were successfully allocated.
+ * return UDMA_ERR: Failure allocating or mapping buffer.
+*/
+static int udma_allocate_rx_buffers(struct udma_device *umdev, int count)
+{
+   struct udma_hw *hw = NULL;
+   struct udma_ring *rx = NULL;
+   unsigned long flags;
+   struct udma_buffer *buffer_info;
+   dma_addr_t dma;
+   int ret = UDMA_OK;
+   struct sk_buff *skb = NULL;
+   int i = 0;
+
+   hw = umdev->hw;
+   rx = &umdev->rx.ring;
+
+   while (count-- ) {
+      skb = netdev_alloc_skb_ip_align(umdev->netdev, umdev->rx_udma_size +
+                                      umdev->rx_skb_reserve);
+      if(unlikely(!skb)){
+         printk("allocate new skb failed in %s function\n", __func__);
+         return UDMA_ERR;
+      }
+
+      if (umdev->rx_skb_reserve)
+         skb_reserve(skb, umdev->rx_skb_reserve);
+
+      dma = dma_map_single(&hw->pdev->dev, skb->data, umdev->rx_udma_size,
+                           DMA_FROM_DEVICE);
+      if (dma_mapping_error(&hw->pdev->dev, dma)) {
+         dev_err(&hw->pdev->dev, "DMA map failed\n");
+         dev_kfree_skb_any(skb);
+         return UDMA_ERR;
+      }
+
+      /* Fill in the buffer info to the descriptor */
+      spin_lock_irqsave(&rx->lock, flags);
+      i = NEXT_DESC_IDX(rx->new_tail, rx);
+      rx->new_tail = i;
+      buffer_info = &rx->buffer_info[i];
+      buffer_info->skb = skb;
+      buffer_info->dma = dma;
+      buffer_info->length = umdev->rx_udma_size;
+      hw->ops->update_rx_desc(hw, &rx->desc[i], dma, umdev->rx_udma_size);
+      spin_unlock_irqrestore(&rx->lock, flags);
+   }
+
+   return ret;
+}
+
+/**  udma_register_handler - register the Rx callback
+ *
+ * @port - udma port number, could be 0 or 1
+ * @rx_handle - Rx callback. Once a buffer is received, UDMA driver fills the
+                buffer descriptor information and calls rx_handle
+ * Note that the rx_callback is called in a softIRQ context
+ *
+ * A prototype of a udma_handle is:
+ *       int udma_handle(int port, udma_buffer_desc_t *buffer_desc);
+ *
+ * At UDMA driver exit, it will invalide the buffer descriptors and will send
+ * all of the received Rx free buffers to upper layer for clean by the
+ * Rx_handle callback.
+ *
+ * return UDMA_OK, success
+ * return UDMA_ERR, failure
+*/
+int udma_register_handler(unsigned char port, struct net_device *dev,
+                          rx_callback_t rx_handle)
+{
+   struct udma_device *umdev = NULL;
+   struct udma_hw *hw = NULL;
+   unsigned long flags = 0;
+   int err = 0;
+
+   UDMA_WARN_ON_RETURN((port >= UDMA_PORT_NUM_TOTAL),UDMA_INVALID_PARAM);
+   UDMA_WARN_ON_RETURN(!rx_handle,UDMA_INVALID_PARAM);
+
+   umdev = udma_devs[port];
+   if (umdev == NULL) {
+      udma_err("UDMA Driver is not installed\n");
+      return UDMA_UNINITIALIZED;
+   }
+
+   mutex_lock(&umdev->mutex);
+   if (umdev->state != PORT_AVAILABLE) {
+      udma_err("UDMA device %d is already being used by others.\n", port);
+      err = UDMA_BUSY;
+      goto error;
+   }
+
+   umdev->netdev = dev;
+   hw = umdev->hw;
+   hw->ops->hw_init(hw);
+
+   err = udma_setup_resources(umdev);
+   if (err) {
+      hw->ops->hw_exit(hw);
+      goto error;
+   }
+
+   err = request_irq (hw->pdev->irq, udma_intr, IRQF_SHARED,
+                      hw->port? UDMA1_NAME : UDMA0_NAME, umdev);
+   if (err) {
+      udma_err("Error requesting UDMA interrupt for port %d!\n", hw->port);
+      udma_free_resources(umdev);
+      hw->ops->hw_exit(hw);
+      goto error;
+   }
+
+   if (umdev->itr_cfg.mode == ITR_ADAPTIVE ) {
+      udma_init_adaptive_itr (umdev);
+   }
+   umdev->rx.callback = rx_handle;
+   umdev->state = PORT_IN_USE;
+
+   netif_napi_add(dev,&umdev->napi, udma_process_queues,64);
+   napi_enable(&umdev->napi);
+   udma_irq_enable(umdev->hw);
+
+   /* Enable UDMA Rx */
+   udma_allocate_rx_buffers(umdev, UDMA_RING_VALID_MAX_NUM(umdev->rx.ring));
+   spin_lock_irqsave(&umdev->rx.ring.lock, flags);
+   udma_start_rx_transfer(umdev);
+   spin_unlock_irqrestore(&umdev->rx.ring.lock, flags);
+
+error:
+   mutex_unlock(&umdev->mutex);
+
+   return err;
+}
+
+/**  udma_flush - Stop the UDMA, flushes the pending requests in the UDMA port
+                  and releases all the remaining buffers.
+ *
+ * @port - udma port number, could be 0 or 1
+ *
+ * This function is expected to be called by upper layer when exit
+ *
+*/
+void udma_flush(unsigned char port)
+{
+   struct udma_device *umdev = NULL;
+   struct udma_hw *hw = NULL;
+   int delay;
+
+   if (WARN_ON(port >= UDMA_PORT_NUM_TOTAL)) {
+      udma_err("%s invalid parameters \n",__FUNCTION__);
+      return;
+   }
+
+   umdev = udma_devs[port];
+   hw = umdev->hw;
+
+   mutex_lock(&umdev->mutex);
+   if (umdev->state != PORT_IN_USE) {
+      udma_err("UDMA device %d is not currently in use!\n", port);
+      goto error;
+   }
+
+   hrtimer_cancel(&umdev->itr_timer);
+   napi_disable(&umdev->napi);
+   netif_napi_del(&umdev->napi);
+
+   /* Stop and clear IRQs */
+   hw->ops->stop_tx_transfer(hw);
+   delay = MAX_UDMA_STOP_DELAY;
+   while (!hw->ops->tx_is_stopped(hw) && delay--) {
+      mdelay(1);
+   } 
+   hw->ops->clear_tx_irq(hw);
+
+   hw->ops->stop_rx_transfer(hw);
+   delay = MAX_UDMA_STOP_DELAY;
+   while (!hw->ops->rx_is_stopped(hw) && delay--) {
+      mdelay(1);
+   }
+   hw->ops->clear_rx_irq(hw);
+
+   udma_irq_disable(umdev->hw);
+   hw->ops->hw_exit(hw);
+   free_irq(hw->pdev->irq, umdev);
+
+   udma_free_resources(umdev);
+   umdev->netdev = NULL;
+   umdev->state = PORT_AVAILABLE;
+
+error:
+   mutex_unlock(&umdev->mutex);
+}
+
+EXPORT_SYMBOL_GPL(udma_send_packet);
+EXPORT_SYMBOL_GPL(udma_register_handler);
+EXPORT_SYMBOL_GPL(udma_flush);
+
+enum hrtimer_restart itr_timeout(struct hrtimer *timer)
+{
+
+   struct udma_device *umdev;
+   umdev = container_of(timer, struct udma_device, itr_timer);
+   /* Schedule NAPI softIRQ to start polling */
+   if (napi_schedule_prep(&umdev->napi)) {
+      umdev->tx.itr.c_bytes   = 0;
+      umdev->tx.itr.c_packets = 0;
+      umdev->rx.itr.c_bytes   = 0;
+      umdev->rx.itr.c_packets = 0;
+      __napi_schedule(&umdev->napi);
+   }
+   return HRTIMER_NORESTART;
+}
+
+/****************************************************************************
+* The UDMA SW stack setup/initialization routine
+****************************************************************************/
+
+static int udma_alloc_ring (struct udma_ring *ring, struct udma_hw *hw,
+   void (*init_desc)(struct udma_hw *, struct udma_desc *))
+{
+   struct udma_desc *desc = NULL;
+   int i = 0;
+
+   spin_lock_init(&ring->lock);
+   ring->buffer_info = vzalloc(sizeof(struct udma_buffer) * ring->entries);
+
+   if (!ring->buffer_info) {
+      udma_err("Cannot allocate memory for buffer_info\n");
+      return -ENOMEM;
+   }
+
+   ring->dma_size = sizeof(struct udma_desc) * ring->entries;
+   ring->desc = dma_alloc_coherent(&hw->pdev->dev, ring->dma_size,
+                                   &ring->dma, GFP_KERNEL);
+   if (!ring->desc) {
+      vfree(ring->buffer_info);
+      return -ENOMEM;
+   }
+   desc = ring->desc;
+
+   /* Link the descriptors one by one */
+   while (i < ring->entries) {
+      memset(&desc[i], 0, sizeof(struct udma_desc));
+      desc[i].next_desc = DESC_INDEX_TO_DESC_DMA((i+1)%ring->entries, ring);
+      (*init_desc)(hw, &desc[i]);
+      i++;
+   }
+
+   return 0;
+}
+
+static void udma_free_ring_dma(struct udma_ring *ring, struct pci_dev *pdev)
+{
+   if (ring->buffer_info) {
+      vfree(ring->buffer_info);
+      ring->buffer_info = 0;
+   }
+   if (ring->dma) {
+      dma_free_coherent(&pdev->dev, ring->dma_size, ring->desc, ring->dma);
+      ring->dma = 0;
+      ring->dma_size = 0;
+   }
+}
+
+static int udma_setup_resources(struct udma_device *umdev)
+{
+   int err;
+   err = udma_alloc_ring(&umdev->rx.ring, umdev->hw,
+                          umdev->hw->ops->init_rx_desc);
+   if (err != 0) {
+      udma_err("Error allocating Rx ring for port %d.\n", umdev->hw->port);
+   }
+   else if ((err = udma_alloc_ring(&umdev->tx.ring, umdev->hw,
+                                    umdev->hw->ops->init_tx_desc)) != 0) {
+      udma_err("Error allocating Tx ring for port %d.\n", umdev->hw->port);
+      udma_free_ring_dma(&umdev->rx.ring, umdev->hw->pdev);
+   } else {
+      memset(&umdev->rx.stats, 0, sizeof(struct udma_stats));
+      memset(&umdev->tx.stats, 0, sizeof(struct udma_stats));
+      umdev->rx.pop_desc = udma_rx_q_pop;
+      umdev->tx.pop_desc = udma_tx_q_pop;
+   }
+   return err;
+}
+
+static void udma_free_q(struct udma_device *umdev, struct udma_queue *q)
+{
+   u32 i = 0, entries = q->ring.entries, dir = q->direction;
+   struct udma_ring *ring = &q->ring;
+
+   /* Clean all descriptors in the ring */
+   for (i = 0; i < entries; i++)
+      q->pop_desc(umdev, i, true);
+   memset(ring->buffer_info, 0, (sizeof(struct udma_buffer) * ring->entries));
+   memset(ring->desc, 0, ring->dma_size);
+
+   udma_free_ring_dma(ring, umdev->hw->pdev);
+
+   memset(q, 0, sizeof(struct udma_queue));
+   q->direction = dir;
+   q->ring.entries = entries;
+   q->ring.ent_mask = entries -1;
+}
+
+static void udma_free_resources(struct udma_device *umdev)
+{
+   udma_free_q(umdev, &umdev->tx);
+   udma_free_q(umdev, &umdev->rx);
+}
+
+static int udma_dev_init(struct udma_device *umdev)
+{
+   hrtimer_init(&umdev->itr_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+   umdev->itr_timer.function = itr_timeout;
+   mutex_init(&umdev->mutex);
+   memset(&umdev->rx, 0, sizeof(struct udma_queue));
+   memset(&umdev->tx, 0, sizeof(struct udma_queue));
+   umdev->rx.ring.entries = UDMA_RING_DEFAULT_ENTRIES;
+   umdev->tx.ring.entries = UDMA_RING_DEFAULT_ENTRIES;
+   umdev->rx.ring.ent_mask = UDMA_RING_DEFAULT_ENTRIES - 1;
+   umdev->tx.ring.ent_mask = UDMA_RING_DEFAULT_ENTRIES - 1;
+   umdev->rx.direction = UDMA_RX;
+   umdev->tx.direction = UDMA_TX;
+   umdev->rx_udma_size = UDMA_MIN_RX_SIZE;
+   umdev->rx_skb_reserve = 0;
+   umdev->netdev = NULL;
+   umdev->state = PORT_AVAILABLE;
+   udma_set_fixed_itr(&umdev->itr_cfg, INTR_FREQ_MIN_FIXED);
+   return 0;
+}
+
+struct udma_hw *udma_alloc_hw(size_t size)
+{
+   struct udma_device *umdev = NULL;
+   struct udma_hw *hw = NULL;
+   
+   hw = kzalloc(sizeof(struct udma_device) + size, GFP_KERNEL);
+   if (NULL == hw) {
+         udma_err("Cannot allocate memory\n");
+         return ERR_PTR(-ENOMEM);
+   }
+   
+   umdev = (struct udma_device *)udma_hw_priv(hw);
+   umdev->hw = hw;
+
+   return hw;
+}
+
+EXPORT_SYMBOL_GPL(udma_alloc_hw);
+
+#ifdef DEBUG
+/* For debug purpose */
+static int udma_open(struct inode *inode, struct file *filp)
+{
+   return 0;
+}
+static int udma_close(struct inode *inode, struct file *filp)
+{
+   return 0;
+}
+static long udma_unlocked_ioctl(struct file *filp, unsigned int arg,
+                                unsigned long cmd)
+{
+   struct udma_device *umdev = NULL;
+   struct udma_hw *hw = NULL;
+   int port = arg - UDMA_PORT_MAGIC_BASE;
+
+   if (port >= UDMA_PORT_NUM_TOTAL)
+      return -EIO;
+   umdev = udma_devs[port];
+   hw = umdev->hw;
+
+   switch (cmd) {
+      case UDMA_DUMP_TX_CURR_RING:
+         udma_print_ring(&umdev->tx, true);
+         break;
+      case UDMA_DUMP_RX_CURR_RING:
+         udma_print_ring(&umdev->rx, true);
+         break;
+      case UDMA_DUMP_STATS:
+      case UDMA_DUMP_ITR_INFO:
+         //TODO Implement these debug use cases.
+         break;
+      case UDMA_DUMP_CURR_TX_REGS:
+         udma_regs_dump(hw,UDMA_TX);
+         break;
+      case UDMA_DUMP_CURR_RX_REGS:
+         udma_regs_dump(hw,UDMA_RX);
+         break;
+      default:
+         udma_err("UDMA driver receive Wrong IOCTL command = 0x%lx\n",cmd);
+         return -EFAULT;
+   }
+   return 0;
+}
+
+static struct file_operations udma_fops = {
+   .owner            = THIS_MODULE,
+   .unlocked_ioctl   = udma_unlocked_ioctl,
+   .open             = udma_open,
+   .release          = udma_close,
+};
+
+#endif
+
+#define PRINT_QUEUE_STATS(dir) \
+   "   [" #dir "] Queue Size        : (%u)\n" \
+   "   [" #dir "] IRQs              : (%u)\n" \
+   "   [" #dir "] Packets           : (%u)\n" \
+   "   [" #dir "]   Total bytes     : (%u)\n" \
+   "   [" #dir "]   Total drops     : (%u)\n", \
+   umdev->dir.ring.entries, umdev->dir.stats.irqs, umdev->dir.stats.pkts, \
+   umdev->dir.stats.bytes, umdev->dir.stats.drops
+
+#define PRINT_S_IRQ_STATS(dir) \
+   "   [" #dir "] softIRQs          : (%u)\n" \
+   "   [" #dir "]   Max pkts        : (%u)\n" \
+   "   [" #dir "]   Acc pkts        : (%u)\n", \
+   umdev->dir.stats.s_irqs, umdev->dir.stats.max_pkts, \
+   umdev->dir.stats.acc_pkts
+
+int proc_udma_status(struct seq_file *sf, void *v)
+{
+   struct udma_device *umdev;
+   int idx;
+   for (idx = 0; idx < UDMA_PORT_NUM_TOTAL; idx++) {
+      umdev = udma_devs[idx];
+      seq_printf(sf, "\nudma port(%d):\n", idx);
+      seq_printf(sf, PRINT_QUEUE_STATS(tx));
+#ifdef SOFT_IRQ_STATS
+      seq_printf(sf, PRINT_S_IRQ_STATS(tx));
+#endif
+      seq_printf(sf, "\n   [rx] Buffer Size       : (%u)\n",
+                 umdev->rx_udma_size + umdev->rx_skb_reserve);
+      seq_printf(sf, "   [rx] Buffer Reserved   : (%u)\n",
+                 umdev->rx_skb_reserve);
+      seq_printf(sf, PRINT_QUEUE_STATS(rx));
+#ifdef SOFT_IRQ_STATS
+      seq_printf(sf, PRINT_S_IRQ_STATS(rx));
+#endif
+   }
+   return 0;
+}
+
+int proc_gap_burst_status(struct seq_file *sf, void *v)
+{
+   struct udma_device *umdev;
+   int idx;
+   for (idx = 0; idx < UDMA_PORT_NUM_TOTAL; idx++) {
+      umdev = udma_devs[idx];
+      seq_printf(sf, "udma port(%d):\n", idx);
+      seq_printf(sf,
+         "     [udma --> l2] gap value: %d cycles, burst size: %d bytes\n",
+         gap_values[umdev->hw->mode.udma_to_l2_gap],
+         burst_sizes[umdev->hw->mode.udma_to_l2_burst]);
+      seq_printf(sf,
+         "     [l2 --> udma] gap value: %d cycles, burst size: %d bytes\n",
+         gap_values[umdev->hw->mode.l2_to_udma_gap],
+         burst_sizes[umdev->hw->mode.l2_to_udma_burst]);
+   }
+   return 0;
+}
+
+int proc_itr_status(struct seq_file *sf, void *v)
+{
+   struct udma_device *umdev;
+   int idx;
+   for (idx = 0; idx < UDMA_PORT_NUM_TOTAL; idx++) {
+      umdev = udma_devs[idx];
+      seq_printf(sf, "udma port(%d):\n", idx);
+      seq_printf(sf, "     mode: %d, ns: %d, ppc: %d\n",
+         umdev->itr_cfg.mode, umdev->itr_cfg.ns, umdev->itr_cfg.ppc);
+      seq_printf(sf, "     min_cps: %d, max_cps: %d, step_cps: %d\n",
+         umdev->itr_cfg.min_cps, umdev->itr_cfg.max_cps,
+         umdev->itr_cfg.step_cps);
+      seq_printf(sf, "     min_mbps: %d, max_mbps: %d, step_mbps: %d\n",
+         cps_to_mbps(umdev->itr_cfg.min_cps, umdev->itr_cfg.ppc),
+         cps_to_mbps(umdev->itr_cfg.max_cps, umdev->itr_cfg.ppc),
+         cps_to_mbps(umdev->itr_cfg.step_cps, umdev->itr_cfg.ppc));
+      seq_printf(sf,
+         "     tx.latency: %d, tx.cps: %d\n"
+         "     rx.latency: %d, rx.cps: %d\n",
+         umdev->tx.itr.latency, umdev->tx.itr.cps,
+         umdev->rx.itr.latency, umdev->rx.itr.cps);
+   }
+   return 0;
+}
+
+static void udma_parse_args(char *str, u8 *argc, char **argv, int max_arg)
+{
+   char *ch;
+   bool in_token = false;
+   for (ch = str; *ch != '\0' && *argc < max_arg; ch++) {
+      if (*ch == '\t' || *ch == ' ') {
+         *ch = '\0';
+         in_token = false;
+      } else if (!isdigit(*ch)) {
+         printk(KERN_DEBUG "invalid parameter format!\n");
+      } else if (!in_token) {
+         argv[*argc] = ch;
+         (*argc)++;
+         in_token = true;
+      }
+   }
+}
+
+static int udma_burst_index(int burst_value)
+{
+   int i = 0, array_size = sizeof(burst_sizes) / sizeof(u32);
+   while (i < array_size && burst_sizes[i] != burst_value) i++;
+   return ((i == array_size)?(-1):i);
+}
+
+static int udma_gap_index(int gap_value)
+{
+   int i = 0, array_size = sizeof(gap_values) / sizeof(u32);
+   while (i < array_size && gap_values[i] != gap_value) i++;
+   return ((i == array_size)?(-1):i);
+}
+
+int udma_set_burstsize( uint8_t port, udma_burstsz_t udma_to_l2_bst,
+                        udma_burstsz_t l2_to_udma_bst)
+{
+   struct udma_device *umdev = NULL;
+   int ret = UDMA_ERR;
+   if ((port < 0) || (port > 1)) {
+      udma_err("Port number must be 0 or 1.\n");
+   } else if ((umdev = udma_devs[port]) == NULL) {
+         udma_err("UDMA Driver is not installed\n");
+   } else if ((udma_to_l2_bst < UDMA_BURST_SZ_4_BYTES) ||
+              (udma_to_l2_bst > UDMA_BURST_SZ_128_BYTES) ||
+              (l2_to_udma_bst < UDMA_BURST_SZ_4_BYTES) ||
+              (l2_to_udma_bst > UDMA_BURST_SZ_128_BYTES)) {
+      udma_err("Burst size must be 4, 8, 16, 32, 64, 128 bytes.\n");
+   } else {
+      mutex_lock(&umdev->mutex);
+      if (umdev->state != PORT_AVAILABLE) {
+         udma_err("UDMA port(%d) is in use.\n", port);
+      } else {
+         umdev->hw->mode.udma_to_l2_burst = udma_to_l2_bst;
+         umdev->hw->mode.l2_to_udma_burst = l2_to_udma_bst;
+         ret = UDMA_OK;
+      }
+      mutex_unlock(&umdev->mutex);
+   }
+   return ret;
+}
+
+int udma_set_gapval( uint8_t port, udma_gapval_t udma_to_l2_gap,
+                     udma_gapval_t l2_to_udma_gap)
+{
+   struct udma_device *umdev = NULL;
+   int ret = UDMA_ERR;
+   if ((port < 0) || (port > 1)) {
+      udma_err("Port number must be 0 or 1.\n");
+   } else if ((umdev = udma_devs[port]) == NULL) {
+         udma_err("UDMA Driver is not installed\n");
+   } else if ((udma_to_l2_gap < UDMA_GAP_VAL_0_CLKS) ||
+              (udma_to_l2_gap > UDMA_GAP_VAL_8192_CLKS) ||
+              (l2_to_udma_gap < UDMA_GAP_VAL_0_CLKS) ||
+              (l2_to_udma_gap > UDMA_GAP_VAL_8192_CLKS)) {
+      udma_err("Gap value must be 0, 16, 64, 256, 1024, 2048, "
+               "4096 or 8192 clocks:\n");
+   } else {
+      mutex_lock(&umdev->mutex);
+      if (umdev->state != PORT_AVAILABLE) {
+         udma_err("UDMA port(%d) is in use.\n", port);
+      } else {
+         umdev->hw->mode.udma_to_l2_gap = udma_to_l2_gap;
+         umdev->hw->mode.l2_to_udma_gap = l2_to_udma_gap;
+         ret = UDMA_OK;
+      }
+      mutex_unlock(&umdev->mutex);
+   }
+   return ret;
+}
+
+int proc_gap_burst_control(struct file *file, const char __user *buffer,
+                           size_t count, loff_t *ppos)
+{
+   char *str;
+   int port = -1, udma_to_l2_gap = -1, l2_to_udma_gap = -1;
+   int udma_to_l2_bst = -1, l2_to_udma_bst = -1;
+   u8 argc = 0;
+   char *argv[5];
+
+   str = kmalloc(count, GFP_KERNEL);
+   if (!str)
+      return -ENOMEM;
+
+   if (copy_from_user(str, buffer, count))
+      return -EFAULT;
+
+   udma_parse_args(str,&argc,argv, 5);
+
+   if (argc == 5) {
+      port = simple_strtol(argv[0], NULL, 10);
+      udma_to_l2_gap = udma_gap_index((int)simple_strtol(argv[1], NULL, 10));
+      udma_to_l2_bst = udma_burst_index((int)simple_strtol(argv[2], NULL, 10));
+      l2_to_udma_gap = udma_gap_index((int)simple_strtol(argv[3], NULL, 10));
+      l2_to_udma_bst = udma_burst_index((int)simple_strtol(argv[4], NULL, 10));
+      udma_set_burstsize(port, udma_to_l2_bst, l2_to_udma_bst);
+      udma_set_gapval(port, udma_to_l2_gap, l2_to_udma_gap);
+   }
+
+   kfree(str);
+   return count;
+}
+
+int proc_itr_control(struct file *file, const char __user *buffer,
+                     size_t count, loff_t *ppos)
+{
+   char *str;
+   int port = -1, itr_mode = -1;
+   int itr_p1 = ITR_DEFAULT_MIN_MBPS, itr_p2 = ITR_DEFAULT_MAX_MBPS;
+   int itr_p3 = ITR_DEFAULT_STEP_MBPS, itr_p4 = ITR_DEFAULT_PPC;
+   u8 argc = 0;
+   char *argv[6];
+   struct udma_device *umdev = NULL;
+
+   str = kmalloc(count, GFP_KERNEL);
+   if (!str)
+      return -ENOMEM;
+
+   if (copy_from_user(str, buffer, count))
+      return -EFAULT;
+
+   udma_parse_args(str,&argc,argv, 6);
+   if( argc > 1 ) {
+      port = simple_strtol(argv[0], NULL, 10);
+      itr_mode = simple_strtol(argv[1], NULL, 10);
+      if( argc > 2 )
+         itr_p1 = simple_strtol(argv[2], NULL, 10);
+      if( argc > 3 )
+         itr_p2 = simple_strtol(argv[3], NULL, 10);
+      if( argc > 4 )
+         itr_p3 = simple_strtol(argv[4], NULL, 10);
+      if( argc > 5 )
+         itr_p4 = simple_strtol(argv[5], NULL, 10);
+   }
+
+   if((port < 0) || (port > 1))
+      udma_err("Port number must be 0 or 1.\n");
+   else
+      umdev = udma_devs[port];
+
+   if (umdev == NULL) {
+      udma_err("UDMA Driver is not available for port(%d).\n", port);
+   } else if((itr_mode < 0) || (itr_mode > ITR_DISABLED)) {
+      udma_err("ITR mode must be 0 (adaptive), 1 (fixed) or 2 (disabled).\n");
+   } else {
+      mutex_lock(&umdev->mutex);
+      if (umdev->state != PORT_AVAILABLE) {
+         udma_err("UDMA port(%d) is in use.\n", port);
+      } else if (itr_mode == ITR_FIXED) {
+         udma_set_fixed_itr(&umdev->itr_cfg, itr_p1);
+      } else if(itr_mode == ITR_ADAPTIVE) {
+         udma_set_adaptive_itr(&umdev->itr_cfg, itr_p1, itr_p2, itr_p3, itr_p4);
+      } else {
+         umdev->itr_cfg.ns = 1;
+      }
+      mutex_unlock(&umdev->mutex);
+   }
+
+   kfree(str);
+   return count;
+}
+
+#define VALID_ENTRIES(ent) \
+    (ent >= UDMA_RING_MIN_ENTRIES) && (ent <= UDMA_RING_MAX_ENTRIES) && \
+   !(ent & (ent - 1))
+
+int proc_queue_control(struct file *file, const char __user *buffer,
+                       size_t count, loff_t *ppos)
+{
+   char *str;
+   int port = -1, rx_entries = -1, tx_entries = -1;
+   u8 argc = 0;
+   char *argv[3];
+   struct udma_device *umdev = NULL;
+
+   str = kmalloc(count, GFP_KERNEL);
+   if (!str)
+      return -ENOMEM;
+
+   if (copy_from_user(str, buffer, count))
+      return -EFAULT;
+
+   udma_parse_args(str,&argc,argv, 3);
+   if( argc > 1 ) {
+      port = simple_strtol(argv[0], NULL, 10);
+      rx_entries = simple_strtol(argv[1], NULL, 10);
+      if( argc > 2 )
+         tx_entries = simple_strtol(argv[2], NULL, 10);
+   }
+
+   if ((port < 0) || (port > 1))
+      udma_err("Port number must be 0 or 1.\n");
+   else
+      umdev = udma_devs[port];
+
+   if (umdev == NULL) {
+      udma_err("UDMA Driver is not available for port(%d).\n", port);
+   } else {
+      mutex_lock(&umdev->mutex);
+      if (umdev->state != PORT_AVAILABLE) {
+         udma_err("UDMA port(%d) is in use.\n", port);
+      } else {
+         if (VALID_ENTRIES(rx_entries)) {
+            umdev->rx.ring.entries = rx_entries;
+            umdev->rx.ring.ent_mask = rx_entries - 1;
+         } else {
+            udma_err("Rx elements must be a power of 2 between %d and %d\n",
+                     UDMA_RING_MIN_ENTRIES, UDMA_RING_MAX_ENTRIES);
+         }
+         if (VALID_ENTRIES(tx_entries)) {
+            umdev->tx.ring.entries = tx_entries;
+            umdev->tx.ring.ent_mask = tx_entries - 1;
+         } else {
+            udma_err("Tx elements must be a power of 2 between %d and %d\n",
+                     UDMA_RING_MIN_ENTRIES, UDMA_RING_MAX_ENTRIES);
+         }
+      }
+      mutex_unlock(&umdev->mutex);
+   }
+   kfree(str);
+   return count;
+}
+
+int proc_rx_buf_control(struct file *file, const char __user *buffer,
+                        size_t count, loff_t *ppos)
+{
+   char *str;
+   int port = -1, rx_size = -1, rx_skb_res = 0;
+   u8 argc = 0;
+   char *argv[3];
+   struct udma_device *umdev = NULL;
+
+   str = kmalloc(count, GFP_KERNEL);
+   if (!str)
+      return -ENOMEM;
+
+   if (copy_from_user(str, buffer, count))
+      return -EFAULT;
+
+   udma_parse_args(str,&argc,argv, 3);
+   if (argc > 1) {
+      port = simple_strtoul(argv[0], NULL, 10);
+      rx_size = simple_strtoul(argv[1], NULL, 10);
+      if (argc > 2)
+         rx_skb_res = simple_strtoul(argv[2], NULL, 10);
+   }
+
+   if ((port < 0) || (port > 1))
+      udma_err("Port number must be 0 or 1.\n");
+   else
+      umdev = udma_devs[port];
+
+   if (umdev == NULL) {
+      udma_err("UDMA Driver is not available for port(%d).\n", port);
+   } else {
+      mutex_lock(&umdev->mutex);
+      if (umdev->state != PORT_AVAILABLE) {
+         udma_err("UDMA port(%d) is in use.\n", port);
+      } else if (rx_size < UDMA_MIN_RX_SIZE ||
+                 rx_size > UDMA_MAX_RX_SIZE) {
+         udma_err("Rx buffer size must be between %d and %d\n",
+                  UDMA_MIN_RX_SIZE, UDMA_MAX_RX_SIZE);
+      } else if (rx_skb_res < 0 ||
+                 rx_skb_res > (rx_size - UDMA_MIN_RX_SIZE)) {
+         udma_err("Max space to be reserved for requested buffer size is %d\n",
+                  rx_size - UDMA_MIN_RX_SIZE);
+      } else {
+         umdev->rx_udma_size = rx_size - rx_skb_res;
+         umdev->rx_skb_reserve = rx_skb_res;
+      }
+      mutex_unlock(&umdev->mutex);
+   }
+   kfree(str);
+   return count;
+}
+
+int proc_udma_help(struct seq_file *sf, void *v)
+{
+   seq_printf(sf,
+      "UDMA Statistics and Configuration:\n");
+   seq_printf(sf,
+      "- To show the ports statistics:\n"
+      "  # cat /proc/udma/status\n\n");
+   seq_printf(sf,
+      "- To show the current gap and burst values:\n"
+      "  # cat /proc/udma/gap_burst_status\n\n");
+   seq_printf(sf,
+      "- To set new gap and burst values:\n"
+      "  # echo \"<PORT> <TO_L2_GAP> <TO_L2_BURST> <TO_UDMA_GAP> "
+      "<TO_UDMA_BURST>\" > /proc/udma/gap_burst_control\n");
+   seq_printf(sf,
+      "  Notes:\n"
+      "   * The PORT number must be 0 or 1.\n"
+      "   * The GAP must be 0, 16, 64, 256, 1024, 2048, 4096 or 8192\n"
+      "     idle clock cycles between bursts.\n"
+      "   * The BURST must be 4, 8, 16, 32, 64, 128\n"
+      "     bytes per DMA bus transfer.\n\n");
+   seq_printf(sf,
+      "- To set the number of elements in the Rx and Tx queues:\n"
+      "  # echo \"<PORT_NUMBER> <RX_ELEMENTS> <TX_ELEMENTS>\" > "
+      "/proc/udma/queue_control\n");
+   seq_printf(sf,
+      "  Notes:\n"
+      "   * The PORT number must be 0 or 1.\n"
+      "   * The ELEMENTS values must be a power of two.\n\n");
+   seq_printf(sf,
+      "- To set Rx buffers configuration:\n"
+      "  # echo \"<PORT_NUMBER> <RX_BUF_SIZE> <RX_BUF_RESERVE>\" > "
+      "/proc/udma/rx_buf_control\n");
+   seq_printf(sf,
+      "  Notes:\n"
+      "   * The PORT number must be 0 or 1.\n"
+      "   * The RX_BUF_SIZE is the total size, in bytes, of the buffers\n"
+      "     allocated by the UDMA driver to receive data.\n"
+      "   * The RX_BUF_RESERVE is optional and represents the number of bytes\n"
+      "     to be reserved in the Rx buffers for the UDMA client (i.e. for a\n"
+      "     header required by a WiFi driver).\n\n");
+   seq_printf(sf,
+      "The UDMA configuration can be changed when the port is not in use\n"
+      "(e.g. when the interfaces using the port to be changed are all down).\n\n");
+/* TODO Add help to set the ITR configuration
+      "[PORT] [ITR_MODE] [MIN_MBPS] [MAX_MBPS] [STEP_MBPS] [PPC]" */
+   return 0;
+}
+
+int proc_udma_status_open(struct inode *inode, struct file *file)
+{
+   return single_open(file, proc_udma_status, PDE_DATA(inode));
+}
+
+struct file_operations proc_udma_status_fops = {
+   .owner   = THIS_MODULE,
+   .open    = proc_udma_status_open,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .release = seq_release,
+};
+
+int proc_gap_burst_status_open(struct inode *inode, struct file *file)
+{
+   return single_open(file, proc_gap_burst_status, PDE_DATA(inode));
+}
+
+struct file_operations proc_gap_burst_status_fops = {
+   .owner   = THIS_MODULE,
+   .open    = proc_gap_burst_status_open,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .release = seq_release,
+};
+
+int proc_itr_status_open(struct inode *inode, struct file *file)
+{
+   return single_open(file, proc_itr_status, PDE_DATA(inode));
+}
+
+struct file_operations proc_itr_status_fops = {
+   .owner   = THIS_MODULE,
+   .open    = proc_itr_status_open,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .release = seq_release,
+};
+
+int proc_udma_help_open(struct inode *inode, struct file *file)
+{
+   return single_open(file, proc_udma_help, PDE_DATA(inode));
+}
+
+struct file_operations proc_udma_help_fops = {
+   .owner   = THIS_MODULE,
+   .open    = proc_udma_help_open,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .release = seq_release,
+};
+
+struct file_operations proc_gap_burst_control_fops = {
+   .owner   = THIS_MODULE,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .write   = proc_gap_burst_control,
+};
+
+struct file_operations proc_itr_control_fops = {
+   .owner   = THIS_MODULE,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .write   = proc_itr_control,
+};
+
+struct file_operations proc_queue_control_fops = {
+   .owner   = THIS_MODULE,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .write   = proc_queue_control,
+};
+
+struct file_operations proc_rx_buf_control_fops = {
+   .owner   = THIS_MODULE,
+   .read    = seq_read,
+   .llseek  = seq_lseek,
+   .write   = proc_rx_buf_control,
+};
+
+int create_proc_filesystem(void)
+{
+   struct proc_dir_entry *dir;
+   int err = -1;
+   dir = proc_mkdir("udma", NULL);
+   if (!dir) {
+      udma_err("proc mkdir error\n");
+   } else if (proc_create("status", 0, dir, &proc_udma_status_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("gap_burst_status", 0, dir,
+                          &proc_gap_burst_status_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("itr_status", 0, dir,
+                          &proc_itr_status_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("help", 0, dir, &proc_udma_help_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("gap_burst_control", 0, dir,
+                          &proc_gap_burst_control_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("itr_control", 0, dir,
+                          &proc_itr_control_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("queue_control", 0, dir,
+                          &proc_queue_control_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else if (proc_create("rx_buf_control", 0, dir,
+                          &proc_rx_buf_control_fops) == NULL) {
+      udma_err("proc create entry error\n");
+   } else {
+      err = 0;
+   }
+   return err;
+}
+
+int udma_setup_sw(void *dev)
+{
+   struct udma_device *umdev = (struct udma_device *)dev;
+   int err;
+   static int create_proc_flag = 0;
+
+   if (WARN_ON(NULL == umdev)) return -EINVAL;
+
+   err = udma_dev_init(umdev);
+   if (0 != err) {
+      udma_err("UDMA SW layer setup failure\n");
+      return -ENODEV;
+   }
+   udma_devs[umdev->hw->port] = umdev;
+
+   /* create proc filesystem utilities for UDMA tuning */
+   if (!create_proc_flag) {
+      err = create_proc_filesystem();
+      create_proc_flag = 1;
+   }
+
+#ifdef DEBUG
+   if (!umdev->hw->port) {
+		if (proc_create(UDMA_PROC_FS, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP |
+                      S_IROTH | S_IWOTH, NULL, &udma_fops) == NULL) {
+			udma_err("Cannot create proc entry!\n");
+			return -EIO;
+		}
+   }
+#endif
+   return err;
+}
+
+/* udma_unregister - Called by hw layer as removal routine */
+void udma_free_sw(void *dev)
+{
+   struct udma_device *umdev = (struct udma_device *)dev;
+
+   if (WARN_ON(umdev == NULL)) return ;
+
+   hrtimer_cancel(&umdev->itr_timer);
+   udma_devs[umdev->hw->port] = NULL;
+
+#ifdef DEBUG
+   if (!umdev->hw->port) 
+      remove_proc_entry(UDMA_PROC_FS, NULL);
+#endif
+
+}
+
+EXPORT_SYMBOL_GPL(udma_setup_sw);
+EXPORT_SYMBOL_GPL(udma_free_sw);
+
+/* udma_main.c */
--- /dev/null
+++ b/drivers/net/udma/udma_main.h
@@ -0,0 +1,206 @@
+/*******************************************************************************
+
+  Intel UDMA driver
+  Copyright(c) 1999 - 2015 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+  Contact Information:
+  Linux NICS <linux.nics@intel.com>
+  udma-devel Mailing List <udma-devel@lists.sourceforge.net>
+  Intel Corporation, 5200 N.E. Elam Young Parkway, Hillsboro, OR 97124-6497
+
+*******************************************************************************/
+
+
+#ifndef _UDMA_MAIN_H_
+#define _UDMA_MAIN_H_
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include "udma_hw.h"
+#include <linux/spinlock.h>
+#include <linux/hrtimer.h>
+#include <linux/udma_skb.h>
+
+/*
+ * Tx : APPCPU    -->   L2SW
+ * Rx:  APPCPU    <--   L2SW
+*/
+
+#define UDMA_RING_DEFAULT_ENTRIES      (1 << 10)
+#define UDMA_RING_MIN_ENTRIES          (1 << 4)
+#define UDMA_RING_MAX_ENTRIES          (1 << 13)
+#define UDMA_RING_VALID_MAX_NUM(ring)  ((ring).entries - 2)
+
+/* Enable/disable debugging statistics */
+#define SOFT_IRQ_STATS 1
+
+struct udma_device;
+
+/* Prototype to pop a descriptor from a queue */
+typedef void (*udma_queue_pop_t)(struct udma_device *umdev, // UDMA device
+                                 const unsigned int  idx,   // Descriptor index
+                                 bool drop);                // Drop descriptor flag
+
+enum udma_state {
+   PORT_AVAILABLE = 0,
+   PORT_IN_USE
+};
+
+enum udma_latency_mode {
+   LOW_LATENCY = 0,
+   BULK_LATENCY = 1,
+   INVALID_LATENCY = 255
+};
+
+enum udma_itr_mode {
+   ITR_ADAPTIVE = 0,
+   ITR_FIXED = 1,
+   ITR_DISABLED = 2
+};
+
+/* wrapper around a pointer to a buffer,
+ * so a DMA handle can be stored along with the buffer
+ */
+struct udma_buffer {
+   struct sk_buff *skb;
+   dma_addr_t dma;
+   u16 length;
+};
+
+struct udma_ring {
+   struct spinlock      lock ____cacheline_aligned_in_smp;
+   struct udma_buffer  *buffer_info;                        /* Descriptor */
+   struct udma_desc    *desc;                               /* Virtual address of desc. */
+   dma_addr_t           dma ____cacheline_aligned_in_smp;   /* Phys address of desc. */
+
+   u32                  dma_size;
+   u32                  entries;
+   u32                  ent_mask;
+
+   /*
+   Empty:      to_be_use == tail
+   Full:       to_be_clean == tail+1
+   A free descriptor means the descriptor with no packet buffer attached
+   */
+   volatile u16  to_be_clean; /* The first free desc. brings nothing */
+   volatile u16  to_be_use;   /* The desc is just started maybe being processed by UDMA */
+   volatile u16  tail;        /* The next avaliable desc. to be cleaned */
+   volatile u16  new_tail;    /* The first free desc. brings nothing */
+};
+
+/* ITR data for adaptive mode */
+struct udma_adapt_itr {
+   u16 latency;      // Latency mode
+   u16 cps;          // Chains per second
+   u16 lo_ppc;       // Low threshold for packets per chain
+   u16 hi_ppc;       // High threshold for packets per chain
+   u32 c_bytes;      // Number of bytes in current chain
+   u32 c_packets;    // Number of packets in current chain
+};
+
+/* Interrupt Throttle Rate (ITR) data */
+struct udma_itr_config {
+   u32   mode;      // ITR mode
+   u32   ns;        // Latency in nanoseconds
+   u32   ppc;       // Expected packets per chain (adaptive mode)
+   u32   min_cps;   // Minimum number of chains per second (adaptive mode)
+   u32   max_cps;   // Maximum number of chains per second (adaptive mode)
+   u32   step_cps;  // Step size (bulk@adaptive mode)
+};
+
+/* UDMA statistics */
+struct udma_stats {
+   u32 irqs;      // Interrupts counter
+   u32 pkts;      // Processed packets counter
+   u32 bytes;     // Processed bytes counter
+   u32 drops;     // Dropped packets counter
+#ifdef SOFT_IRQ_STATS
+   /* Statistics for finished packets when soft IRQ was triggered */
+   u32 s_irqs;    // Soft-IRQs counter
+   u32 max_pkts;  // Max number of packets in queue
+   u32 acc_pkts;  // Total accumulated packets
+#endif
+};
+
+struct udma_queue {
+   struct udma_ring        ring ____cacheline_aligned_in_smp;
+   struct udma_stats       stats;
+   struct udma_adapt_itr   itr;
+   udma_queue_pop_t        pop_desc;
+   rx_callback_t           callback;
+   u32                     direction; /* 0: UDMA_RX - 1: UDMA_TX*/
+};
+
+/* UDMA device structure, includes HW, ring, buffer */
+struct udma_device {
+   /* structs defined in udma_hw.h */
+   struct udma_hw         *hw;
+   struct net_device      *netdev;
+   struct napi_struct      napi;
+
+   struct hrtimer          itr_timer;
+   struct udma_itr_config  itr_cfg;
+
+   struct mutex            mutex;
+   enum udma_state         state;
+
+   struct udma_queue       rx;
+   struct udma_queue       tx;
+
+   u32                     rx_udma_size;
+   u32                     rx_skb_reserve;
+};
+
+struct udma_device *udma_devs[UDMA_PORT_NUM_TOTAL] = {NULL, NULL};
+
+/* UDMA Ring index operations */
+#define IDX_ADD(i,j,r)     (((i) + (j)) & (r)->ent_mask)
+#define IDX_SUB(i,j,r)     (((i) + ((r)->entries - j)) & (r)->ent_mask)
+#define PREV_DESC_IDX(i,r) IDX_SUB(i,1,r)
+#define NEXT_DESC_IDX(i,r) IDX_ADD(i,1,r)
+
+/* Desc DMA address to desc index */
+#define DESC_DMA_TO_DESC_INDEX(d,ring) \
+   (((u32)(d - ring->dma))/sizeof(struct udma_desc))
+
+/* Desc index to Desc DMA address */
+#define DESC_INDEX_TO_DESC_DMA(i,ring) \
+   ((ring)->dma + (i) * sizeof(struct udma_desc))
+
+/* Ring index to the ring desc virtual address */
+#define INDEX_TO_DESC(i,ring) \
+   (&(((struct udma_desc *)((ring)->desc))[i]))
+
+#ifdef DEBUG
+
+#define UDMA_PORT_MAGIC_BASE  100
+#define UDMA_PORT0            100
+#define UDMA_PORT1            101
+
+#define UDMA_PROC_FS "udma_dbg"
+
+#define UDMA_DUMP_TX_CURR_RING   121
+#define UDMA_DUMP_RX_CURR_RING   122
+#define UDMA_DUMP_STATS          123
+#define UDMA_DUMP_ITR_INFO       124
+#define UDMA_DUMP_CURR_TX_REGS   125
+#define UDMA_DUMP_CURR_RX_REGS   126
+
+#endif
+
+#endif /* _UDMA_MAIN_H_ */
--- /dev/null
+++ b/drivers/net/udma/udma_netdev.c
@@ -0,0 +1,259 @@
+/*******************************************************************************
+
+  Intel(R) UDMA Network Device Model sample code
+
+  Copyright(c) 2012 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+*******************************************************************************/
+
+
+
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/vmalloc.h>
+#include <linux/pagemap.h>
+#include <linux/delay.h>
+#include <linux/netdevice.h>
+#include <linux/tcp.h>
+#include <linux/ipv6.h>
+#include <linux/slab.h>
+#include <net/checksum.h>
+#include <net/ip6_checksum.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/if_vlan.h>
+#include <linux/cpu.h>
+#include <linux/smp.h>
+#include <linux/pm_qos.h>
+#include <linux/pm_runtime.h>
+#include <linux/aer.h>
+#include <asm/io.h>
+
+#include <linux/udma_skb.h>
+
+#define DRV_VERSION "0.1"
+
+
+#define MAX_TRY_TIMES           	10
+
+
+
+
+
+/* Get MAC Adress like the same way as e1000 */
+#define CONFIG_RAM_BASE         0x60000
+#define GBE_CONFIG_OFFSET       0x0
+#define NODE_ADDRESS_SIZE       6
+
+#define GBE_CONFIG_RAM_BASE \
+        ((unsigned int)(CONFIG_RAM_BASE + GBE_CONFIG_OFFSET))
+
+#define GBE_CONFIG_BASE_VIRT \
+        ((void __iomem *)phys_to_virt(GBE_CONFIG_RAM_BASE))
+
+#define GBE_CONFIG_FLASH_READ(base, offset, count, data) \
+        (ioread16_rep(base + (offset << 1), data, count))
+
+
+
+struct udma_adapter{
+	unsigned char udma_port;
+	u8 mac_addr[NODE_ADDRESS_SIZE];
+};
+static struct net_device *udma_net_dev[UDMA_PORT_NUM_TOTAL];
+
+/* Set MAC Address */
+static void udma_set_mac_addr(unsigned char udma_port, struct net_device *netdev, struct udma_adapter *adapter){
+       u16 eeprom_data, i, offset;
+
+      for (i = 0; i < NODE_ADDRESS_SIZE; i += 2) {
+               offset = i >> 1;
+               GBE_CONFIG_FLASH_READ(GBE_CONFIG_BASE_VIRT, offset, 1,&eeprom_data);
+               adapter->mac_addr[i] = (u8) (eeprom_data & 0x00FF);
+               adapter->mac_addr[i + 1] = (u8) (eeprom_data >> 8);
+       }
+       adapter->mac_addr[NODE_ADDRESS_SIZE - 1] += udma_port + 1;
+
+       memcpy(netdev->dev_addr, adapter->mac_addr, netdev->addr_len);
+       memcpy(netdev->perm_addr, adapter->mac_addr, netdev->addr_len);
+}
+
+
+static netdev_tx_t udma_net_xmit(struct sk_buff *skb,
+				    struct net_device *netdev)
+{
+	struct udma_adapter *adapter = netdev_priv(netdev);
+	unsigned char port;
+	size_t size = 0;
+	netdev_tx_t ret ;
+
+	port = adapter->udma_port;
+
+	size =	udma_xmit_skb(port, skb);
+	if (size) {
+		//netdev->stats.tx_bytes += size;
+		//netdev->stats.tx_packets++;
+		ret = NETDEV_TX_OK;
+	} else {
+		netif_stop_queue(netdev);
+		ret = NETDEV_TX_BUSY;
+	}
+	return ret;
+
+}
+static void udma_net_rx_callback(struct sk_buff *skb,
+				    struct net_device *netdev)
+{
+	WARN_ON(!skb);
+	WARN_ON(!netdev);
+//	printk("Received skb from net %8x len %8d",netdev,skb->len);
+
+	netdev->stats.rx_packets++;
+	netdev->stats.rx_bytes += skb_headlen(skb);
+	//n = list_first_entry(&netdev->napi_list, struct napi_struct, dev_list);
+//	skb->ip_summed = CHECKSUM_UNNECESSARY;
+	skb->protocol = eth_type_trans(skb, skb->dev);
+	netif_receive_skb(skb);
+//	ret = napi_gro_receive(n, skb);
+//	if (ret == GRO_DROP)
+	return;
+}
+
+static int udma_net_open(struct net_device *netdev)
+{
+	struct udma_adapter *adapter = netdev_priv(netdev);
+	uint8_t port = adapter->udma_port;
+	int ret = 0;
+
+	ret = udma_register(port,netdev,&udma_net_rx_callback);
+	if (ret)
+		return ret;
+	else
+		netif_start_queue(netdev);
+	return 0;
+}
+
+/**
+ * udma_close - Disables a network interface
+ * @netdev: network interface device structure
+ *
+ * Returns 0, this is not allowed to fail
+ *
+ **/
+
+static int udma_net_close(struct net_device *netdev)
+{
+	struct udma_adapter *adapter = netdev_priv(netdev);
+
+	netif_stop_queue(netdev);
+	udma_flush(adapter->udma_port);
+	return 0;
+}
+
+/* Netdevice get statistics request */
+
+/*static struct rtnl_link_stats64 *
+udma_net_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
+{
+	return 0;
+}*/
+
+
+static const struct net_device_ops udma_netdev_ops = {
+	.ndo_open				= udma_net_open,
+	.ndo_stop				= udma_net_close,
+	.ndo_start_xmit			= udma_net_xmit,
+	.ndo_change_mtu			= eth_change_mtu,
+	//.ndo_get_stats64	= udma_net_get_stats64,
+};
+
+
+/**
+ * udma_net_init - Driver Registration Routine
+ *
+ * udma_net_init is the first routine called when the driver is
+ * loaded.
+ **/
+ #define UDMA_NET "eth_udma"
+static int __init udma_net_init(void)
+{
+	struct udma_adapter *adapter = NULL;
+	struct net_device *netdev = NULL;
+	int i,err;
+
+	for (i = 0; i < UDMA_PORT_NUM_TOTAL; i++) {
+		netdev = alloc_etherdev(sizeof(struct udma_adapter));
+		if (!netdev){
+			return -ENOMEM;
+		}
+
+		udma_net_dev[i] = netdev;
+		adapter = netdev_priv(netdev);
+		adapter->udma_port = i;
+
+		netdev->netdev_ops = &udma_netdev_ops;
+		//netdev->features |= NETIF_F_HW_CSUM;
+		////NETIF_F_SG;
+		sprintf(netdev->name, "%s%d", UDMA_NET, i);
+		err = register_netdev(netdev);
+		if (err){
+			free_netdev(netdev);
+			return err;
+		}
+
+		/* Currently the udma mac address is generated base on the GBE Mac address */
+		udma_set_mac_addr(i, netdev, adapter);
+
+	}
+
+	printk(KERN_INFO "UDMA Network Device Driver init \n");
+	return 0;
+}
+
+/**
+ * udma_net_exit - Driver Exit Cleanup Routine
+ *
+ **/
+static void __exit udma_net_exit(void)
+{
+	int i;
+	struct udma_adapter *adapter;
+	for (i = 0; i < UDMA_PORT_NUM_TOTAL; i++) {
+		adapter = netdev_priv(udma_net_dev[i]);
+		unregister_netdev(udma_net_dev[i]);
+		free_netdev(udma_net_dev[i]);
+		udma_net_dev[i] = NULL;
+	}
+	printk(KERN_INFO "UDMA Network Device Driver exit \n");
+}
+module_init(udma_net_init);
+module_exit(udma_net_exit);
+
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("Intel(R) UDMA Network Device Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
+
+/* udma_net.c */
--- /dev/null
+++ b/drivers/net/udma/udma_skb.c
@@ -0,0 +1,127 @@
+/*******************************************************************************
+
+  Intel(R) UDMA Network Device Model sample code
+
+  Copyright(c) 2012 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+*******************************************************************************/
+
+/*
+ * UDMA buffer management and skb interaction layer
+*/
+
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/vmalloc.h>
+#include <linux/pagemap.h>
+#include <linux/delay.h>
+#include <linux/netdevice.h>
+#include <linux/tcp.h>
+#include <linux/ipv6.h>
+#include <linux/slab.h>
+#include <net/checksum.h>
+#include <net/ip6_checksum.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/if_vlan.h>
+#include <linux/cpu.h>
+#include <linux/smp.h>
+#include <linux/pm_qos.h>
+#include <linux/pm_runtime.h>
+#include <linux/aer.h>
+#include <linux/crc32.h>
+#include <asm/io.h>
+
+#include <linux/udma_api.h>
+#include <linux/udma_skb.h>
+
+
+
+//#define UDMA_LDML_DEBUG 1
+#ifdef UDMA_LDML_DEBUG
+
+
+
+#define udma_net_dbg(fmt, args...)  do \
+				     { \
+				           printk(KERN_INFO "\n%s " fmt "\n","udma_skb",##args); \
+				     } while(0)
+#else
+#define udma_net_dbg(fmt, arg...)  do { } while (0)
+#endif
+#define udma_net_info(fmt, args...)  do \
+				     { \
+				           printk(KERN_INFO "\n%s " fmt "\n","udma_skb",##args); \
+				     } while(0)
+
+#define udma_net_err(fmt, args...)  do \
+				     { \
+				           printk(KERN_ERR "\n%s " fmt "\n","udma_skb",##args); \
+				     } while(0)
+
+
+
+
+
+
+
+
+
+
+int udma_xmit_skb(unsigned char port,struct sk_buff *skb)
+{
+	udma_result_t ret = UDMA_OK;
+	udma_net_dbg("In %s function port %d skb 0x%x\n", __func__,port, skb);
+
+	ret = udma_send_packet(port, skb);
+	if (likely(UDMA_OK == ret))
+		return skb->len;
+	else if (ret == UDMA_FULL) {
+			udma_net_dbg("failure: udma tx buffer full\n");
+			dev_kfree_skb_any(skb);
+			return 0;
+	} else {
+		udma_net_dbg("udma_send_packet fail %d\n", ret);
+		dev_kfree_skb_any(skb);
+		return 0;
+	}
+	return 0;
+}
+
+int udma_register(uint8_t port, struct net_device *netdev,rx_callback_t rx_handler)
+{
+        int ret = 0;
+        WARN_ON(!rx_handler);
+
+        ret = udma_register_handler(port, netdev,rx_handler);
+        if(ret)
+                return ret;
+        return 0;
+}
+EXPORT_SYMBOL_GPL(udma_register);
+
+
+
+EXPORT_SYMBOL_GPL(udma_xmit_skb);
+
